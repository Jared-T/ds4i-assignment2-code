---
title: "Happy Legendary Title"
format: html
author: Jared Tavares
---

# Read in the daaaata

```{python}

# Loading in the necessary libraries
import zipfile
import os
import pandas as pd
import re
import matplotlib.pyplot as plt
from wordcloud import WordCloud, STOPWORDS
from itertools import cycle
import seaborn as sns
import numpy as np
from collections import Counter
from nltk.tokenize import word_tokenize

```

```{python}

# Unzip the file and get the list of filenames
with zipfile.ZipFile("data/speeches.zip", 'r') as zip_ref:
    zip_ref.extractall("data")

filenames = os.listdir("data")
filenames = [filename for filename in filenames if filename.endswith('.txt')]

# Read the content of each speech file and extract the date from the first line
speeches = []
dates = []
for filename in filenames:
    with open(os.path.join("data", filename), 'r', encoding='utf-8') as file:
        # Extract date from the first line
        date = file.readline().strip()
        dates.append(date)
        
        # Read the rest of the file
        speeches.append(file.read())

# Create DataFrame
sona = pd.DataFrame({'filename': filenames, 'speech': speeches, 'date': dates})

# Extract year and president for each speech
sona['year'] = sona['filename'].str[:4]
sona['president'] = sona['filename'].str.split('_').str[-1].str.split('.').str[0]

# Clean the sona dataset by removing unnecessary text
replace_reg = r'(http.*?(\s|.$))|(www.*?(\s|.$))|&amp;|&lt;|&gt;|\n'
sona['speech'] = sona['speech'].str.replace(replace_reg, ' ')

# Split speeches into sentences
# sona_sentences = sona['speech'].str.split(r'(?<!\w\.\w.)(?<![A-Z][a-z]\.)(?<=\.|\?)\s', expand=True).stack().reset_index(level=-1, drop=True)
# sona_sentences.name = 'sentence'

# # Remove newline characters from the sentences
# sona_sentences = sona_sentences.str.replace('\n', '').str.strip()

# # Merge with the president, date, and year columns to associate each sentence with the respective details
# df_sentences = sona[['president', 'date', 'year']].join(sona_sentences)

# Make a csv of the sentences
sona.to_csv('data/sona_speeches.csv', index=False)

```


```{python}

# Read in the sona speeches dataset
sona_speeches_df = pd.read_csv('data/sona_speeches.csv')

# Function to clean the speech text by removing special characters
def clean_text(text):
    # Remove special characters: keep only letters, numbers, and basic punctuation
    text = re.sub(r'[^a-zA-Z0-9.,?! ]', '', text)
    return text

# Apply the cleaning function to the speech column
sona_speeches_df['speech'] = sona_speeches_df['speech'].apply(clean_text)

```


```{python}

from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS
from collections import Counter
import matplotlib.pyplot as plt

# Function to count words in speeches excluding stopwords
def get_word_frequencies(speeches, stopwords):
    word_counts = Counter()
    for speech in speeches:
        words = speech.lower().split()
        # Remove stopwords from the count
        words = [word.strip('.,!?"\'-()') for word in words if word.strip('.,!?"\'-()') not in stopwords]
        word_counts.update(words)
    return word_counts

# Get the word frequencies excluding stopwords
word_frequencies = get_word_frequencies(sona_speeches_df['speech'], ENGLISH_STOP_WORDS)

# Get the top 10 most frequent words across all speeches
top_10_words = word_frequencies.most_common(10)

# Plotting the bar graph for the top 10 most frequent words across speeches
plt.figure(figsize=(10, 6))
plt.bar([word for word, count in top_10_words], [count for word, count in top_10_words])
plt.title('Top 10 Most Frequent Words Across SONA Speeches')
plt.xlabel('Words')
plt.ylabel('Frequency')
plt.xticks(rotation=45)
plt.show()


```



```{python}

# Function to get top N frequent words for each president
def get_top_words_by_president(speeches_df, n, stopwords):
    presidents = speeches_df['president'].unique()
    top_words_by_president = {}
    for president in presidents:
        president_speeches = speeches_df[speeches_df['president'] == president]['speech']
        word_frequencies = get_word_frequencies(president_speeches, stopwords)
        top_words_by_president[president] = word_frequencies.most_common(n)
    return top_words_by_president

# Get the top 10 most frequent words for each president
top_10_words_by_president = get_top_words_by_president(sona_speeches_df, 10, ENGLISH_STOP_WORDS)

# Plotting the bar graph for the top 10 most frequent words faceted by president
fig, axes = plt.subplots(nrows=len(top_10_words_by_president), ncols=1, figsize=(10, 6 * len(top_10_words_by_president)))
fig.suptitle('Top 10 Most Frequent Words by President (Excluding Stopwords)', y=1.02)

for i, (president, top_words) in enumerate(top_10_words_by_president.items()):
    axes[i].bar([word for word, count in top_words], [count for word, count in top_words])
    axes[i].set_title(president)
    axes[i].set_xlabel('Words')
    axes[i].set_ylabel('Frequency')
    axes[i].tick_params(axis='x', rotation=45)

plt.tight_layout()
plt.show()


```



```{python}

# We will first identify the overall top words to track over time.
overall_top_words = [word for word, count in top_10_words]

# Function to get yearly frequency of specific words
def get_yearly_word_trends(speeches_df, words):
    yearly_word_trends = {word: [] for word in words}
    years = sorted(speeches_df['year'].unique())
    
    for year in years:
        year_speeches = speeches_df[speeches_df['year'] == year]['speech']
        word_counts = get_word_frequencies(year_speeches, ENGLISH_STOP_WORDS)
        for word in words:
            yearly_word_trends[word].append(word_counts[word])
    
    return years, yearly_word_trends

# Get the yearly trends for the overall top words
years, yearly_word_trends = get_yearly_word_trends(sona_speeches_df, overall_top_words)

# Plotting the line graph for how the most frequent words trend over time across speeches
plt.figure(figsize=(14, 7))

for word in overall_top_words:
    plt.plot(years, yearly_word_trends[word], label=word, marker='o')

plt.title('Trends of Most Frequent Words Over Time in SONA Speeches (Excluding Stopwords)')
plt.xlabel('Year')
plt.ylabel('Frequency')
plt.legend()
plt.grid(True)
plt.xticks(years, rotation=45)
plt.tight_layout()
plt.show()


```


```{python}

# Function to get yearly frequency of specific words for each president
def get_presidential_yearly_word_trends(speeches_df, words):
    presidents = speeches_df['president'].unique()
    presidential_word_trends = {president: {word: [] for word in words} for president in presidents}
    presidential_years = {president: sorted(speeches_df[speeches_df['president'] == president]['year'].unique()) for president in presidents}
    
    for president in presidents:
        for year in presidential_years[president]:
            year_speeches = speeches_df[(speeches_df['year'] == year) & (speeches_df['president'] == president)]['speech']
            word_counts = get_word_frequencies(year_speeches, ENGLISH_STOP_WORDS)
            for word in words:
                presidential_word_trends[president][word].append(word_counts[word])
    
    return presidential_years, presidential_word_trends

# Get the presidential yearly trends for the overall top words
presidential_years, presidential_word_trends = get_presidential_yearly_word_trends(sona_speeches_df, overall_top_words)

# Plotting the line graphs for the most frequent words for each president over time
for president in presidential_years:
    plt.figure(figsize=(14, 7))

    for word in overall_top_words:
        plt.plot(presidential_years[president], presidential_word_trends[president][word], label=word, marker='o')

    plt.title(f'Trends of Most Frequent Words Over Time for President {president} (Excluding Stopwords)')
    plt.xlabel('Year')
    plt.ylabel('Frequency')
    plt.legend()
    plt.grid(True)
    plt.xticks(presidential_years[president], rotation=45)
    plt.tight_layout()
    plt.show()


```


```{python}

from sklearn.feature_extraction.text import CountVectorizer
from itertools import chain

# Define the list of negation words
negation_words = ['not', 'no', 'never', 'without', 'nor']

# Function to get top N frequent bigrams for the given speeches
def get_top_negation_bigrams(speeches, n, negation_words):
    vectorizer = CountVectorizer(ngram_range=(2, 2), stop_words='english')
    X = vectorizer.fit_transform(speeches)
    feature_names = vectorizer.get_feature_names_out()
    bigram_frequencies = zip(feature_names, X.toarray().sum(axis=0))

    # Filter bigrams to only keep those with negation words
    negation_bigrams = [(bigram, freq) for bigram, freq in bigram_frequencies if any(neg_word in bigram for neg_word in negation_words)]
    negation_bigrams = sorted(negation_bigrams, key=lambda x: x[1], reverse=True)[:n]
    
    return negation_bigrams

# Get the top 10 most frequent negation bigrams across all speeches
top_10_negation_bigrams = get_top_negation_bigrams(sona_speeches_df['speech'], 10, negation_words)

# Plotting the bar graph for the top 10 most frequent negation bigrams across speeches
plt.figure(figsize=(12, 6))
plt.bar([bigram for bigram, count in top_10_negation_bigrams], [count for bigram, count in top_10_negation_bigrams])
plt.title('Top 10 Most Frequent Negation Bigrams Across SONA Speeches')
plt.xlabel('Bigrams')
plt.ylabel('Frequency')
plt.xticks(rotation=45)
plt.show()


```


```{python}

# Function to get top N frequent negation bigrams for each president
def get_top_negation_bigrams_by_president(speeches_df, n, negation_words):
    presidents = speeches_df['president'].unique()
    top_negation_bigrams_by_president = {}
    for president in presidents:
        president_speeches = speeches_df[speeches_df['president'] == president]['speech']
        negation_bigrams = get_top_negation_bigrams(president_speeches, n, negation_words)
        top_negation_bigrams_by_president[president] = negation_bigrams
    return top_negation_bigrams_by_president

# Get the top 10 most frequent negation bigrams for each president
top_10_negation_bigrams_by_president = get_top_negation_bigrams_by_president(sona_speeches_df, 10, negation_words)

# Plotting the bar graph for the top 10 most frequent negation bigrams faceted by president
fig, axes = plt.subplots(nrows=len(top_10_negation_bigrams_by_president), ncols=1, figsize=(12, 6 * len(top_10_negation_bigrams_by_president)))
fig.suptitle('Top 10 Most Frequent Negation Bigrams by President', y=1.02)

for i, (president, negation_bigrams) in enumerate(top_10_negation_bigrams_by_president.items()):
    axes[i].bar([bigram for bigram, count in negation_bigrams], [count for bigram, count in negation_bigrams])
    axes[i].set_title(president)
    axes[i].set_xlabel('Bigrams')
    axes[i].set_ylabel('Frequency')
    axes[i].tick_params(axis='x', rotation=45)

plt.tight_layout()
plt.show()


```


```{python}

# We will first identify the overall top negation bigrams to track over time.
overall_top_negation_bigrams = [bigram for bigram, count in top_10_negation_bigrams]

# Function to get yearly frequency of specific bigrams
def get_yearly_bigram_trends(speeches_df, bigrams):
    yearly_bigram_trends = {bigram: [] for bigram in bigrams}
    years = sorted(speeches_df['year'].unique())
    
    for year in years:
        year_speeches = speeches_df[speeches_df['year'] == year]['speech']
        negation_bigrams = get_top_negation_bigrams(year_speeches, None, negation_words)  # None for n to get all
        negation_bigrams_dict = dict(negation_bigrams)
        
        for bigram in bigrams:
            yearly_bigram_trends[bigram].append(negation_bigrams_dict.get(bigram, 0))
    
    return years, yearly_bigram_trends

# Get the yearly trends for the overall top negation bigrams
years, yearly_negation_bigram_trends = get_yearly_bigram_trends(sona_speeches_df, overall_top_negation_bigrams)

# Plotting the line graph for how the most frequent negation bigrams trend over time across speeches
plt.figure(figsize=(14, 7))

for bigram in overall_top_negation_bigrams:
    plt.plot(years, yearly_negation_bigram_trends[bigram], label=bigram, marker='o')

plt.title('Trends of Most Frequent Negation Bigrams Over Time in SONA Speeches')
plt.xlabel('Year')
plt.ylabel('Frequency')
plt.legend()
plt.grid(True)
plt.xticks(years, rotation=45)
plt.tight_layout()
plt.show()


```