{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "601386c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# imports\n",
    "import os\n",
    "import sys\n",
    "import types\n",
    "import json\n",
    "\n",
    "# figure size/format\n",
    "fig_width = 7\n",
    "fig_height = 5\n",
    "fig_format = 'retina'\n",
    "fig_dpi = 96\n",
    "\n",
    "# matplotlib defaults / format\n",
    "try:\n",
    "  import matplotlib.pyplot as plt\n",
    "  plt.rcParams['figure.figsize'] = (fig_width, fig_height)\n",
    "  plt.rcParams['figure.dpi'] = fig_dpi\n",
    "  plt.rcParams['savefig.dpi'] = fig_dpi\n",
    "  from IPython.display import set_matplotlib_formats\n",
    "  set_matplotlib_formats(fig_format)\n",
    "except Exception:\n",
    "  pass\n",
    "\n",
    "# plotly use connected mode\n",
    "try:\n",
    "  import plotly.io as pio\n",
    "  pio.renderers.default = \"notebook_connected\"\n",
    "except Exception:\n",
    "  pass\n",
    "\n",
    "# enable pandas latex repr when targeting pdfs\n",
    "try:\n",
    "  import pandas as pd\n",
    "  if fig_format == 'pdf':\n",
    "    pd.set_option('display.latex.repr', True)\n",
    "except Exception:\n",
    "  pass\n",
    "\n",
    "\n",
    "\n",
    "# output kernel dependencies\n",
    "kernel_deps = dict()\n",
    "for module in list(sys.modules.values()):\n",
    "  # Some modules play games with sys.modules (e.g. email/__init__.py\n",
    "  # in the standard library), and occasionally this can cause strange\n",
    "  # failures in getattr.  Just ignore anything that's not an ordinary\n",
    "  # module.\n",
    "  if not isinstance(module, types.ModuleType):\n",
    "    continue\n",
    "  path = getattr(module, \"__file__\", None)\n",
    "  if not path:\n",
    "    continue\n",
    "  if path.endswith(\".pyc\") or path.endswith(\".pyo\"):\n",
    "    path = path[:-1]\n",
    "  if not os.path.exists(path):\n",
    "    continue\n",
    "  kernel_deps[path] = os.stat(path).st_mtime\n",
    "print(json.dumps(kernel_deps))\n",
    "\n",
    "# set run_path if requested\n",
    "if r'/Users/heiletjevanzyl/Desktop/DSFI/STA5073Z Assignment 2/ds4i-assignment2-code':\n",
    "  os.chdir(r'/Users/heiletjevanzyl/Desktop/DSFI/STA5073Z Assignment 2/ds4i-assignment2-code')\n",
    "\n",
    "# reset state\n",
    "%reset\n",
    "\n",
    "def ojs_define(**kwargs):\n",
    "  import json\n",
    "  try:\n",
    "    # IPython 7.14 preferred import\n",
    "    from IPython.display import display, HTML\n",
    "  except:\n",
    "    from IPython.core.display import display, HTML\n",
    "\n",
    "  # do some minor magic for convenience when handling pandas\n",
    "  # dataframes\n",
    "  def convert(v):\n",
    "    try:\n",
    "      import pandas as pd\n",
    "    except ModuleNotFoundError: # don't do the magic when pandas is not available\n",
    "      return v\n",
    "    if type(v) == pd.Series:\n",
    "      v = pd.DataFrame(v)\n",
    "    if type(v) == pd.DataFrame:\n",
    "      j = json.loads(v.T.to_json(orient='split'))\n",
    "      return dict((k,v) for (k,v) in zip(j[\"index\"], j[\"data\"]))\n",
    "    else:\n",
    "      return v\n",
    "  \n",
    "  v = dict(contents=list(dict(name=key, value=convert(value)) for (key, value) in kwargs.items()))\n",
    "  display(HTML('<script type=\"ojs-define\">' + json.dumps(v) + '</script>'), metadata=dict(ojs_define = True))\n",
    "globals()[\"ojs_define\"] = ojs_define\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d03d8fbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| eval: true\n",
    "\n",
    "# Run 1\n",
    "\n",
    "# Loading in the necessary libraries\n",
    "import zipfile\n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "from itertools import cycle\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import wordnet, stopwords, words\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from ast import literal_eval\n",
    "from collections import defaultdict\n",
    "import pyLDAvis.gensim_models\n",
    "import gensim\n",
    "from gensim.models import LsiModel, TfidfModel\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "from gensim import corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from plsa import Corpus, Pipeline, Visualize\n",
    "from plsa.pipeline import DEFAULT_PIPELINE\n",
    "from plsa.algorithms import PLSA\n",
    "from gensim.models import AuthorTopicModel\n",
    "from gensim.models import LdaModel\n",
    "import tqdm\n",
    "\n",
    "# Global params\n",
    "plt.rcParams['xtick.labelsize'] = 18\n",
    "plt.rcParams['ytick.labelsize'] = 18\n",
    "\n",
    "# Set the global label sizes for the plots\n",
    "plt.rcParams['axes.labelsize'] = 20\n",
    "\n",
    "# Set the global legend size\n",
    "plt.rcParams['legend.fontsize'] = 18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e34e0c97",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| eval: false\n",
    "\n",
    "# Unzip the file and get the list of filenames\n",
    "with zipfile.ZipFile(\"data/speeches.zip\", 'r') as zip_ref:\n",
    "    zip_ref.extractall(\"data\")\n",
    "\n",
    "filenames = os.listdir(\"data\")\n",
    "filenames = [filename for filename in filenames if filename.endswith('.txt')]\n",
    "\n",
    "# Read the content of each speech file and extract the date from the first line\n",
    "speeches = []\n",
    "dates = []\n",
    "for filename in filenames:\n",
    "    with open(os.path.join(\"data\", filename), 'r', encoding='utf-8') as file:\n",
    "        # Extract date from the first line\n",
    "        date = file.readline().strip()\n",
    "        dates.append(date)\n",
    "        \n",
    "        # Read the rest of the file\n",
    "        speeches.append(file.read())\n",
    "\n",
    "# Create DataFrame\n",
    "sona = pd.DataFrame({'filename': filenames, 'speech': speeches, 'date': dates})\n",
    "\n",
    "# Extract year and president for each speech\n",
    "sona['year'] = sona['filename'].str[:4]\n",
    "sona['president'] = sona['filename'].str.split('_').str[-1].str.split('.').str[0]\n",
    "\n",
    "# Clean the sona dataset by removing unnecessary text\n",
    "replace_reg = r'(http.*?(\\s|.$))|(www.*?(\\s|.$))|&amp;|&lt;|&gt;|\\n'\n",
    "sona['speech'] = sona['speech'].str.replace(replace_reg, ' ')\n",
    "\n",
    "# Split speeches into sentences\n",
    "sona_sentences = sona.copy()\n",
    "\n",
    "import itertools\n",
    "\n",
    "# Replace new lines with space and split into sentences based on regular expression\n",
    "sona_sentences['speech'] = sona_sentences['speech'].str.replace('\\n', ' ').str.split(r'(?<!\\w\\.\\w.)(?<![A-Z][a-z]\\.)(?<=\\.|\\?)\\s')\n",
    "\n",
    "# Flatten the list of sentence fragments to avoid nested lists\n",
    "sona_sentences['speech'] = sona_sentences['speech'].apply(lambda sentences: list(itertools.chain.from_iterable(sentence.split('.') for sentence in sentences)))\n",
    "\n",
    "# Remove empty strings from the list of sentences\n",
    "sona_sentences['speech'] = sona_sentences['speech'].apply(lambda sentences: [sentence.strip() for sentence in sentences if sentence.strip()])\n",
    "\n",
    "# Make a csv of the speeches\n",
    "sona.to_csv('data/sona_speeches.csv', index=False)\n",
    "\n",
    "# Make a csv of the sentences\n",
    "sona_sentences.to_csv('data/sona_sentences_untransformed.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2de0434d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| eval: false\n",
    "\n",
    "# Make sure to download the necessary NLTK corpus if you haven't already\n",
    "#nltk.download('wordnet')\n",
    "#nltk.download('averaged_perceptron_tagger')\n",
    "#nltk.download('words')\n",
    "\n",
    "# Read in the sona speeches dataset\n",
    "sona_speeches_df = pd.read_csv('data/sona_speeches.csv')\n",
    "sona_sentences_clean = pd.read_csv('data/sona_sentences_untransformed.csv')\n",
    "sona_sentences_clean['speech'] = sona_sentences_clean['speech'].apply(literal_eval)\n",
    "\n",
    "# Initialize the WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "english_words = set(words.words())\n",
    "additional_words = {\n",
    "    'honourable', 'member', 'chairperson',\n",
    "    'south', 'africa', 'african', 'africans', 'year',\n",
    "    'madame', 'madam', 'soes', 'ms', 'madams', 'madames', 'mw',\n",
    "    'compatriotsthe',\n",
    "    'also'\n",
    "}\n",
    "\n",
    "# Function to convert NLTK's part-of-speech tags to WordNet's part-of-speech tags\n",
    "def get_wordnet_pos(word):\n",
    "    \"\"\"Map NLTK part of speech tags to WordNet part of speech tags.\"\"\"\n",
    "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "    tag_dict = {\"J\": nltk.corpus.wordnet.ADJ,\n",
    "                \"N\": nltk.corpus.wordnet.NOUN,\n",
    "                \"V\": nltk.corpus.wordnet.VERB,\n",
    "                \"R\": nltk.corpus.wordnet.ADV}\n",
    "\n",
    "    return tag_dict.get(tag, nltk.corpus.wordnet.NOUN)\n",
    "\n",
    "# Clean the text, convert to lowercase, and lemmatize each word\n",
    "def clean_text(text):\n",
    "    # Remove special characters: keep only letters, numbers, and basic punctuation\n",
    "    text = re.sub(r'[.;]', ' ', text)  # Replaces periods with spaces\n",
    "    text = re.sub(r'[^a-zA-Z ]', '', text)\n",
    "    text = text.lower()  # Convert to lowercase\n",
    "    \n",
    "    # Tokenize the text\n",
    "    words = word_tokenize(text)\n",
    "    \n",
    "    # Remove stop words\n",
    "    words = [word for word in words if word not in stop_words]\n",
    "    \n",
    "    # Remove additional words\n",
    "    words = [word for word in words if word not in additional_words]\n",
    "\n",
    "    # Lemmatize each word with the correct POS tag\n",
    "    lemmatized_words = []\n",
    "    for word, tag in nltk.pos_tag(words):\n",
    "        wntag = get_wordnet_pos(tag)\n",
    "        lemmatized_word = lemmatizer.lemmatize(word, wntag)\n",
    "        # Only append the lemmatized word if it is in the set of English words\n",
    "        if lemmatized_word in english_words:\n",
    "            lemmatized_words.append(lemmatized_word)\n",
    "    \n",
    "    # Join the lemmatized words back into one string\n",
    "    text = ' '.join(words)\n",
    "    return text\n",
    "\n",
    "def clean_text_no_word_removals(text):\n",
    "    # Remove special characters: keep only letters, numbers, and basic punctuation\n",
    "    text = re.sub(r'[.;]', ' ', text)  # Replaces periods with spaces\n",
    "    text = re.sub(r'[^a-zA-Z ]', '', text)\n",
    "    text = text.lower()  # Convert to lowercase\n",
    "    return text\n",
    "\n",
    "# Apply the cleaning function to the speech column\n",
    "tempdf = sona_speeches_df.copy()\n",
    "sona_speeches_df['speech'] = tempdf['speech'].apply(clean_text)\n",
    "sona_speeches_df['speech_untrans'] = tempdf['speech'].apply(clean_text_no_word_removals)\n",
    "\n",
    "def clean_speeches(speeches):\n",
    "    # The input is expected to be a list of strings\n",
    "    return [clean_text(sentence) for sentence in speeches]\n",
    "\n",
    "# Apply the cleaning to the sentences too\n",
    "sona_sentences_clean['sentence'] = sona_sentences_clean['speech'].apply(lambda speeches: [clean_text(sentence) for sentence in speeches])\n",
    "\n",
    "# Apply the cleaning to sentences that need to keep their words\n",
    "sona_sentences_clean['sent_untrans'] = sona_sentences_clean['speech'].apply(lambda speeches: [clean_text_no_word_removals(sentence) for sentence in speeches])\n",
    "\n",
    "# Make a csv of the speeches\n",
    "sona_speeches_df.to_csv('data/sona_speeches_adapted.csv', index=False)\n",
    "\n",
    "# Remove the speech column from the sentences DataFrame\n",
    "sona_sentences_clean.drop(columns=['speech'], inplace=True)\n",
    "\n",
    "# Make a csv of the sentences\n",
    "sona_sentences_clean.to_csv('data/sona_sentences_clean.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "acc2c169",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| eval: true\n",
    "\n",
    "# Run 2\n",
    "\n",
    "sona_sentences_clean = pd.read_csv('data/sona_sentences_clean.csv')\n",
    "sona_sentences_clean['sentence'] = sona_sentences_clean['sentence'].apply(literal_eval)\n",
    "sona_sentences_clean['sent_untrans'] = sona_sentences_clean['sent_untrans'].apply(literal_eval)\n",
    "\n",
    "# Make the sentences into a single column\n",
    "sona_sentences_alltogether = sona_sentences_clean.explode('sentence')\n",
    "sona_sentences_all_untrans = sona_sentences_clean.explode('sent_untrans')\n",
    "\n",
    "# Drop the other columns\n",
    "sona_sentences_alltogether.drop(columns=['sent_untrans'], inplace=True)\n",
    "sona_sentences_all_untrans.drop(columns=['sentence'], inplace=True)\n",
    "\n",
    "# Make a csv of the sentences\n",
    "sona_sentences_all_untrans.to_csv('data/sona_sentiment_sentences.csv', index=False)\n",
    "\n",
    "# Speeches\n",
    "sona_speeches_clean = pd.read_csv('data/sona_speeches_adapted.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a657f122",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "max_features = 2000\n",
    "\n",
    "bow_vectorizer = CountVectorizer(max_features=max_features)\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=max_features)\n",
    "\n",
    "# Transformed on the words\n",
    "bow_matrix_words = bow_vectorizer.fit_transform(sona_speeches_clean['speech'])\n",
    "tfidf_matrix_words = tfidf_vectorizer.fit_transform(sona_speeches_clean['speech'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "575c0b89",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "\n",
    "cmap = plt.cm.cividis\n",
    "\n",
    "norm = plt.Normalize(0, 100)\n",
    "\n",
    "# Define a colour map based on cividis\n",
    "# Define a new colormap using a smaller slice of the cividis colormap, this time stopping well before the yellows\n",
    "cividis_modified = cmap(np.linspace(0, 0.4, cmap.N))  # Using only 40% of the colormap range\n",
    "\n",
    "# Create a new colormap from the data\n",
    "cividis_no_yellow_light = LinearSegmentedColormap.from_list('cividis_no_yellow_light', cividis_modified)\n",
    "\n",
    "# Let's pick three colors from the modified colormap\n",
    "colormap = [cividis_no_yellow_light(norm(0)), \n",
    "          cividis_no_yellow_light(norm(50)), \n",
    "          cividis_no_yellow_light(norm(100))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4808bffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Function to count words in speeches excluding stopwords\n",
    "def get_word_frequencies(speeches, stopwords):\n",
    "    word_counts = Counter()\n",
    "    for speech in speeches:\n",
    "        words = speech.lower().split()\n",
    "        # Remove stopwords from the count\n",
    "        words = [word.strip('.,!?\"\\'-()') for word in words if word.strip('.,!?\"\\'-()') not in stopwords]\n",
    "        word_counts.update(words)\n",
    "    return word_counts\n",
    "\n",
    "# Get the word frequencies excluding stopwords\n",
    "word_frequencies = get_word_frequencies(sona_speeches_clean['speech'], ENGLISH_STOP_WORDS)\n",
    "\n",
    "# Get the top 10 most frequent words across all speeches\n",
    "top_10_words = word_frequencies.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "74a0e2bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar([word for word, count in top_10_words], [count for word, count in top_10_words], color=colormap[2])\n",
    "plt.xlabel('Words')\n",
    "plt.ylabel('Frequency')\n",
    "plt.xticks()\n",
    "plt.yticks()\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "# Save the plot as a PNG file\n",
    "plt.savefig(f'saved_plots/overall_top_words.png', bbox_inches='tight')\n",
    "plt.close()  # Close the figure to avoid displaying it in the notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b6dfae4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get top N frequent words for each president\n",
    "def get_top_words_by_president(speeches_df, n, stopwords):\n",
    "    presidents = speeches_df['president'].unique()\n",
    "    top_words_by_president = {}\n",
    "    for president in presidents:\n",
    "        president_speeches = speeches_df[speeches_df['president'] == president]['speech']\n",
    "        word_frequencies = get_word_frequencies(president_speeches, stopwords)\n",
    "        top_words_by_president[president] = word_frequencies.most_common(n)\n",
    "    return top_words_by_president\n",
    "\n",
    "# Get the top 10 most frequent words for each president\n",
    "top_10_words_by_president = get_top_words_by_president(sona_speeches_clean, 10, ENGLISH_STOP_WORDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c286b494",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the word frequencies for each president\n",
    "for president, top_words in top_10_words_by_president.items():\n",
    "    \n",
    "    # Individual plot for each president\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.bar([word for word, count in top_words], [count for word, count in top_words], color=colormap[0])\n",
    "    plt.xlabel('Words')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.xticks(rotation=45)\n",
    "\n",
    "    # Save the plot as a PNG file\n",
    "    plt.savefig(f'saved_plots/{president}_top_words.png', bbox_inches='tight')\n",
    "    plt.close()  # Close the figure to avoid displaying it in the notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "63f83ea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import opinion_lexicon\n",
    "from nltk.tokenize import treebank\n",
    "from afinn import Afinn\n",
    "\n",
    "from dateutil import parser\n",
    "\n",
    "# Function to parse date strings based on the described rule\n",
    "def parse_date(date_str):\n",
    "    # Split the string by comma and take the last part\n",
    "    date_part = date_str.split(',')[-1].strip()\n",
    "    # Parse the date part into a datetime object\n",
    "    return parser.parse(date_part)\n",
    "\n",
    "# Define a function to get Bing lexicon sentiment scores\n",
    "def get_bing_sentiment(text):\n",
    "    tokenizer = treebank.TreebankWordTokenizer()\n",
    "    tokens = tokenizer.tokenize(text.lower())\n",
    "    pos_score = sum(1 for word in tokens if word in positive_words)\n",
    "    neg_score = sum(1 for word in tokens if word in negative_words)\n",
    "    compound_score = pos_score - neg_score\n",
    "    return compound_score\n",
    "\n",
    "\n",
    "# Load the AFINN lexicon\n",
    "afinn = Afinn()\n",
    "\n",
    "# Define a function to get AFINN sentiment scores\n",
    "def get_afinn_sentiment(text):\n",
    "    return afinn.score(text)\n",
    "\n",
    "# Load positive and negative words\n",
    "positive_words = set(opinion_lexicon.positive())\n",
    "negative_words = set(opinion_lexicon.negative())\n",
    "\n",
    "# Apply Bing sentiment analysis\n",
    "sona_speeches_clean['bing_sentiment'] = sona_speeches_clean['speech_untrans'].apply(get_bing_sentiment)\n",
    "sona_sentences_all_untrans['bing_sentiment'] = sona_sentences_all_untrans['sent_untrans'].apply(get_bing_sentiment)\n",
    "\n",
    "# Apply AFINN sentiment analysis\n",
    "sona_speeches_clean['afinn_sentiment'] = sona_speeches_clean['speech_untrans'].apply(lambda text: get_afinn_sentiment(text))\n",
    "sona_sentences_all_untrans['afinn_sentiment'] = sona_sentences_all_untrans['sent_untrans'].apply(lambda text: get_afinn_sentiment(text))\n",
    "\n",
    "# Convert the date strings to datetime objects\n",
    "sona_speeches_clean['date'] = sona_speeches_clean['date'].apply(parse_date)\n",
    "sona_sentences_all_untrans['date'] = sona_sentences_all_untrans['date'].apply(parse_date)\n",
    "\n",
    "# Sort the DataFrames by date in ascending order\n",
    "sona_speeches_clean.sort_values('date', ascending=True, inplace=True)\n",
    "#sona_sentences_all_untrans.sort_values('date', ascending=True, inplace=True)\n",
    "\n",
    "# Create a new variable which is the date as a string\n",
    "sona_speeches_clean['date_str'] = sona_speeches_clean['date'].dt.strftime('%Y-%m-%d')\n",
    "sona_sentences_all_untrans['date_str'] = sona_sentences_all_untrans['date'].dt.strftime('%Y-%m-%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "30f58dfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# For plotting sentiment scores of speeches by each president\n",
    "def plot_speeches_by_president(df, lexicon):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "\n",
    "    presidents = df['president'].unique()\n",
    "\n",
    "    lexicon_lab = lexicon\n",
    "\n",
    "    if lexicon == 'afinn':\n",
    "        lexicon_lab = 'AFINN'\n",
    "\n",
    "    colors = ['lightsteelblue', colormap[1], 'midnightblue', 'lightgray', 'darkgray',  'dimgray']\n",
    "\n",
    "    for idx, president in enumerate(presidents):\n",
    "        president_df = df[df['president'] == president]\n",
    "        plt.bar(president_df['date_str'], president_df[f'{lexicon}_sentiment'], label=president, color=colors[idx])\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel(f'Sentiment Score')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.yticks()\n",
    "    plt.legend(loc =\"upper left\")\n",
    "    plt.savefig(f'sentiment_plots/speech_{lexicon}_all.png', bbox_inches='tight')\n",
    "    plt.close()  # Close the figure to avoid displaying it in the notebook\n",
    "\n",
    "\n",
    "# For plotting sentiment scores of sentences by each president\n",
    "def plot_sentences_by_president(df, lexicon):\n",
    "    #plt.figure(figsize=(10, 6))\n",
    "\n",
    "    presidents = df['president'].unique()\n",
    "\n",
    "    colors = [colormap[1],  'dimgray', 'midnightblue', 'darkgray', 'lightsteelblue', 'lightgray']\n",
    "\n",
    "    lexicon_lab = lexicon\n",
    "\n",
    "    if lexicon == 'afinn':\n",
    "        lexicon_lab = 'AFINN'\n",
    "    \n",
    "    # Create a copy of the DataFrame\n",
    "    df = df.copy()\n",
    "\n",
    "    # Add a column for the sentence number\n",
    "    df['sentence_num'] = df.groupby('date_str').cumcount() + 1\n",
    "    \n",
    "    i = 0\n",
    "\n",
    "    for idx, president in enumerate(presidents):\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        president_df = df[df['president'] == president]\n",
    "        plt.bar(president_df['sentence_num'], president_df[f'{lexicon}_sentiment'], label=president, color=colors[i])\n",
    "        plt.xlabel('Sentence')\n",
    "        plt.ylabel(f'Sentiment Score')\n",
    "        plt.xticks()\n",
    "        plt.savefig(f'sentiment_plots/sent_{lexicon}_{president}.png', bbox_inches='tight')\n",
    "        plt.close()  # Close the figure to avoid displaying it in the notebook\n",
    "\n",
    "        i += 1\n",
    "\n",
    "    \n",
    "# Assuming 'date' is a column in datetime format and 'president' is the name of each president\n",
    "plot_speeches_by_president(sona_speeches_clean, 'bing')\n",
    "plot_speeches_by_president(sona_speeches_clean, 'afinn')\n",
    "\n",
    "plot_sentences_by_president(sona_sentences_all_untrans, 'bing')\n",
    "plot_sentences_by_president(sona_sentences_all_untrans, 'afinn')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bfa59a1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate word sentiments across all speeches of a president\n",
    "def calculate_word_sentiments(president_speeches, lexicon):\n",
    "    # Combine all speeches into one large text\n",
    "    all_speeches = ' '.join(president_speeches)\n",
    "    # Tokenize the text into words and filter out stopwords and non-alphabetic tokens\n",
    "    words = [word for word in word_tokenize(all_speeches.lower()) if word.isalpha() and word not in stopwords.words('english')]\n",
    "    # Get sentiment score for each word\n",
    "    word_sentiments = defaultdict(int)\n",
    "    for word in words:\n",
    "        # Get the sentiment score for the word\n",
    "        if lexicon == 'bing':\n",
    "            sentiment = get_bing_sentiment(word)\n",
    "        elif lexicon == 'afinn':\n",
    "            sentiment = get_afinn_sentiment(word)\n",
    "\n",
    "        word_sentiments[word] += sentiment\n",
    "    return word_sentiments\n",
    "\n",
    "# Function to plot the top positive and negative words\n",
    "def plot_top_words(word_sentiments, president, lexicon):\n",
    "    # Sort words by sentiment score\n",
    "    sorted_words = sorted(word_sentiments.items(), key=lambda kv: kv[1])\n",
    "    # Select the top 10 positive and negative words\n",
    "    top_positive_words = sorted_words[-10:]\n",
    "    top_negative_words = sorted_words[:10]\n",
    "\n",
    "    # Words and their sentiment scores for plotting\n",
    "    words_positive, scores_positive = zip(*top_positive_words)\n",
    "    words_negative, scores_negative = zip(*top_negative_words)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "    # Plot the top negative words\n",
    "    ax.barh(range(10), scores_negative, color=colormap[2], label='Negative')\n",
    "    # Plot the top positive words\n",
    "    ax.barh(range(10, 20), scores_positive, color=colormap[0], label='Positive')\n",
    "\n",
    "    # Add the word labels\n",
    "    ax.set_yticks(range(20))\n",
    "    ax.set_yticklabels(words_negative + words_positive)\n",
    "    \n",
    "    # Set the labels and title\n",
    "    ax.set_xlabel(f'Contribution to Sentiment Score')\n",
    "    ax.legend(loc = \"lower right\")\n",
    "\n",
    "    # Adjust the view so negative words are at the bottom and positive at the top\n",
    "    ax.set_ylim(-1, 20)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    # Save the plot as a PNG file\n",
    "    plt.savefig(f'sentiment_plots/word_contr_{lexicon}_{president}.png', bbox_inches='tight')\n",
    "    plt.close()  # Close the figure to avoid displaying it in the notebook\n",
    "\n",
    "# Aggregate the speeches by president and calculate the top words\n",
    "presidents_speeches = sona_speeches_clean.groupby('president')['speech_untrans'].apply(list)\n",
    "for president, speeches in presidents_speeches.items():\n",
    "    word_sentiments_bing = calculate_word_sentiments(speeches, 'bing')\n",
    "    word_sentiments_afinn = calculate_word_sentiments(speeches, 'afinn')\n",
    "    plot_top_words(word_sentiments_bing, president, 'bing')\n",
    "    plot_top_words(word_sentiments_bing, president, 'AFINN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "62470411",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate word sentiments across all speeches\n",
    "def calculate_word_sentiments(speeches, lexicon):\n",
    "    # Combine all speeches into one large text\n",
    "    all_speeches = ' '.join(speeches)\n",
    "    # Tokenize the text into words and filter out stopwords and non-alphabetic tokens\n",
    "    words = [word for word in word_tokenize(all_speeches.lower()) if word.isalpha() and word not in stopwords.words('english')]\n",
    "    # Get sentiment score for each word\n",
    "    word_sentiments = defaultdict(int)\n",
    "    for word in words:\n",
    "        # Get the sentiment score for the word\n",
    "        if lexicon == 'bing':\n",
    "            sentiment = get_bing_sentiment(word)\n",
    "        elif lexicon == 'afinn':\n",
    "            sentiment = get_afinn_sentiment(word)\n",
    "\n",
    "        word_sentiments[word] += sentiment\n",
    "    return word_sentiments\n",
    "\n",
    "# Function to plot the top positive and negative words\n",
    "def plot_top_words(word_sentiments, lexicon):\n",
    "    # Sort words by sentiment score\n",
    "    sorted_words = sorted(word_sentiments.items(), key=lambda kv: kv[1])\n",
    "    # Select the top 10 positive and negative words\n",
    "    top_positive_words = sorted_words[-10:]\n",
    "    top_negative_words = sorted_words[:10]\n",
    "\n",
    "    # Words and their sentiment scores for plotting\n",
    "    words_positive, scores_positive = zip(*top_positive_words)\n",
    "    words_negative, scores_negative = zip(*top_negative_words)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "    # Plot the top negative words\n",
    "    ax.barh(range(10), scores_negative, color=colormap[2], label='Negative')\n",
    "    # Plot the top positive words\n",
    "    ax.barh(range(10, 20), scores_positive, color=colormap[0], label='Positive')\n",
    "\n",
    "    # Add the word labels\n",
    "    ax.set_yticks(range(20))\n",
    "    ax.set_yticklabels(words_negative + words_positive)\n",
    "    # ax.yticks(fontsize=16)\n",
    "\n",
    "    # Set the labels and title\n",
    "    ax.set_xlabel(f'Contribution to Sentiment Score')\n",
    "    ax.legend()\n",
    "    # ax.xticks(fontsize=16)\n",
    "\n",
    "    # Adjust the view so negative words are at the bottom and positive at the top\n",
    "    ax.set_ylim(-1, 20)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'sentiment_plots/word_contr_{lexicon}_all.png', bbox_inches='tight')\n",
    "    plt.close()  # Close the figure to avoid displaying it in the notebook\n",
    "\n",
    "# Calculate the word sentiments across all speeches for each lexicon\n",
    "all_speeches = sona_speeches_clean['speech_untrans'].tolist()\n",
    "word_sentiments_bing = calculate_word_sentiments(all_speeches, 'bing')\n",
    "word_sentiments_afinn = calculate_word_sentiments(all_speeches, 'afinn')\n",
    "\n",
    "# Plot the top words for each lexicon\n",
    "plot_top_words(word_sentiments_bing, 'bing')\n",
    "plot_top_words(word_sentiments_afinn, 'AFINN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b0b7c085",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| eval: false\n",
    "\n",
    "# Run 2.5\n",
    "\n",
    "texts = sona_speeches_clean['speech']\n",
    "sentences = sona_sentences_alltogether['sentence']\n",
    "\n",
    "# Further process tokens using gensim's simple_preprocess\n",
    "tokenized_texts = [simple_preprocess(doc, deacc=True) for doc in texts]  # deacc=True removes punctuations\n",
    "tokenized_sentences = [simple_preprocess(doc, deacc=True) for doc in sentences]  # deacc=True removes punctuations\n",
    "\n",
    "# Create a Gensim dictionary from the tokenized documents\n",
    "dictionary = corpora.Dictionary(tokenized_texts)\n",
    "dict_sentences = corpora.Dictionary(tokenized_sentences)\n",
    "\n",
    "#dictionary.filter_extremes(no_below=2, no_above=0.7)\n",
    "#dict_sentences.filter_extremes(no_below=2, no_above=0.7)\n",
    "\n",
    "# Create a BOW corpus\n",
    "bow_corpus = [dictionary.doc2bow(text) for text in tokenized_texts]\n",
    "bow_corpus_sentences = [dict_sentences.doc2bow(text) for text in tokenized_sentences]\n",
    "\n",
    "# Create a TF-IDF corpus\n",
    "tfidf = TfidfModel(bow_corpus)\n",
    "tfidf_corpus = tfidf[bow_corpus]\n",
    "\n",
    "tfidf_sentences = TfidfModel(bow_corpus_sentences)\n",
    "tfidf_corpus_sentences = tfidf_sentences[bow_corpus_sentences]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1c6a7f7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the function to compute coherence values\n",
    "def compute_coherence_values(dictionary, corpus, texts, start, limit, step, coherence='u_mass'):\n",
    "    coherence_values = []\n",
    "    model_list = []\n",
    "    for num_topics in range(start, limit, step):\n",
    "        model = LsiModel(corpus=corpus, num_topics=num_topics, id2word=dictionary)\n",
    "        model_list.append(model)\n",
    "        coherencemodel = CoherenceModel(model=model, texts=tokenized_texts, dictionary=dictionary, coherence=coherence)\n",
    "        coherence_values.append(coherencemodel.get_coherence())\n",
    "    return model_list, coherence_values\n",
    "\n",
    "# Set parameters\n",
    "start, limit, step = 2, 20, 1\n",
    "\n",
    "# Compute coherence values for BOW\n",
    "bow_model_list, bow_coherence_values = compute_coherence_values(dictionary=dictionary, corpus=bow_corpus, texts=texts, start=start, limit=limit, step=step)\n",
    "\n",
    "bow_model_list_sentences, bow_coherence_values_sentences = compute_coherence_values(dictionary=dict_sentences, corpus=bow_corpus_sentences, texts=sentences, start=start, limit=limit, step=step)\n",
    "\n",
    "# Compute coherence values for TF-IDF\n",
    "tfidf_model_list, tfidf_coherence_values = compute_coherence_values(dictionary=dictionary, corpus=tfidf_corpus, texts=tokenized_texts, start=start, limit=limit, step=step)\n",
    "\n",
    "tfidf_model_list_sentences, tfidf_coherence_values_sentences = compute_coherence_values(dictionary=dict_sentences, corpus=tfidf_corpus_sentences, texts=sentences, start=start, limit=limit, step=step)\n",
    "\n",
    "# Save the to csv\n",
    "coherence_df = pd.DataFrame({'bow_coherence_values': bow_coherence_values, 'tfidf_coherence_values': tfidf_coherence_values, 'bow_coherence_values_sentences': bow_coherence_values_sentences, 'tfidf_coherence_values_sentences': tfidf_coherence_values_sentences})\n",
    "\n",
    "coherence_df.to_csv('lsa_plots/coherence_values.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ffde158f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| eval: false\n",
    "\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "\n",
    "# Read in the coherence values\n",
    "coherence_df = pd.read_csv('lsa_plots/coherence_values.csv')\n",
    "\n",
    "# Extract the coherence values\n",
    "bow_coherence_values = coherence_df['bow_coherence_values']\n",
    "tfidf_coherence_values = coherence_df['tfidf_coherence_values']\n",
    "bow_coherence_values_sentences = coherence_df['bow_coherence_values_sentences']\n",
    "tfidf_coherence_values_sentences = coherence_df['tfidf_coherence_values_sentences']\n",
    "\n",
    "# Plotting the coherence values\n",
    "x = range(start, limit, step)\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(x, bow_coherence_values, label='BoW', color='midnightblue')\n",
    "plt.plot(x, tfidf_coherence_values, label='tf-idf', color='darkgray')\n",
    "plt.xlabel(\"Number of Topics\")\n",
    "plt.ylabel(\"Coherence score\")\n",
    "plt.legend(loc='lower left')\n",
    "plt.gca().xaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "plt.savefig(f'lsa_plots/words_coherence_plots.png', bbox_inches='tight')\n",
    "plt.close()\n",
    "\n",
    "\n",
    "# Plotting the coherence values\n",
    "x = range(start, limit, step)\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(x, bow_coherence_values_sentences, label='BoW', color='midnightblue')\n",
    "plt.plot(x, tfidf_coherence_values_sentences, label='tf-idf', color='darkgray')\n",
    "plt.xlabel(\"Number of Topics\")\n",
    "plt.ylabel(\"Coherence score\")\n",
    "plt.legend(loc='lower right')\n",
    "plt.gca().xaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "plt.savefig(f'lsa_plots/sentence_coherence_plots.png', bbox_inches='tight')\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "108b9263",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run 3\n",
    "\n",
    "# TODO: Set the number of topics based on the plots\n",
    "lsa_bow_words = LsiModel(corpus=bow_corpus, num_topics=3, id2word=dictionary)\n",
    "lsa_bow_sentences = LsiModel(corpus=bow_corpus_sentences, num_topics=3, id2word=dict_sentences)\n",
    "\n",
    "lsa_tfidf_words = LsiModel(corpus=tfidf_corpus, num_topics=3, id2word=dictionary)\n",
    "lsa_tfidf_sentences = LsiModel(corpus=tfidf_corpus_sentences, num_topics=4, id2word=dict_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "dd8a0e4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run 4\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Function to plot the top words for each topic in a single LSA model\n",
    "def plot_top_words_for_each_topic(model, fiton, lexicon, num_words=10):\n",
    "\n",
    "    colors = ['lightsteelblue', 'midnightblue', 'lightgray', 'dimgray']\n",
    "    \n",
    "    j = 0\n",
    "\n",
    "    for i in range(model.num_topics):\n",
    "        # Extract the top words for this topic\n",
    "        top_words = model.show_topic(i, num_words)\n",
    "        # Separate the words and their corresponding weights\n",
    "        words, weights = zip(*top_words)\n",
    "        weights = [abs(weight) for weight in weights]  # Use absolute values for weights\n",
    "\n",
    "        # Create a bar chart for the top words in this topic\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.barh(words, weights, color = colors[j])\n",
    "        # ax.set_yticklabels(words, fontsize=20)\n",
    "        j += 1\n",
    "\n",
    "        if j == 4:\n",
    "            j = 0\n",
    "\n",
    "        plt.xlabel('Weight')\n",
    "        plt.gca().invert_yaxis()  # Highest weights on top\n",
    "        plt.savefig(f'lsa_plots/{fiton}_{lexicon}_topic_{i + 1}.png', bbox_inches='tight')\n",
    "        plt.close()\n",
    "\n",
    "\n",
    "# Apply the plotting function to each of your LSA models\n",
    "plot_top_words_for_each_topic(lsa_bow_words, 'words', 'bow')\n",
    "plot_top_words_for_each_topic(lsa_bow_sentences, 'sentences', 'bow')\n",
    "plot_top_words_for_each_topic(lsa_tfidf_words, 'words', 'tfidf')\n",
    "plot_top_words_for_each_topic(lsa_tfidf_sentences, 'sentences', 'tfidf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7c64ac99",
   "metadata": {},
   "outputs": [],
   "source": [
    "sona_speeches_clean['speech'].to_csv('data/sona_speeches_only.csv', index=False)\n",
    "sona_sentences_alltogether['sentence'].to_csv('data/sona_sentences_only.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "012bb902",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def calculate_umass_coherence(plsa_result, corpus, top_n=10, tf_idf=False):\n",
    "    # Extract the top N words for each topic\n",
    "    top_words_by_topic = [\n",
    "        [word for word, _ in plsa_result.word_given_topic[i][:top_n]]\n",
    "        for i in range(plsa_result.n_topics)\n",
    "    ]\n",
    "    \n",
    "    # Get document-word matrix (document frequency matrix)\n",
    "    doc_word_matrix = corpus.get_doc_word(tf_idf=tf_idf)\n",
    "    \n",
    "    # Calculate document frequencies for single words\n",
    "    word_doc_freq = np.sum(doc_word_matrix > 0, axis=0)\n",
    "    \n",
    "    # Calculate coherence for each topic\n",
    "    topic_coherences = []\n",
    "    for top_words in top_words_by_topic:\n",
    "        pair_scores = []\n",
    "        for i, word in enumerate(top_words):\n",
    "            for j in range(i + 1, len(top_words)):\n",
    "                # Get indices in the vocabulary\n",
    "                word_i_index = corpus.index[word]\n",
    "                word_j_index = corpus.index[top_words[j]]\n",
    "                \n",
    "                # Count the documents where both words appear\n",
    "                both_docs = np.sum((doc_word_matrix[:, word_i_index] > 0) & (doc_word_matrix[:, word_j_index] > 0))\n",
    "                \n",
    "                # Calculate score for this word pair\n",
    "                score = math.log((both_docs + 1.0) / word_doc_freq[word_i_index])  # Add 1 to avoid log(0)\n",
    "                pair_scores.append(score)\n",
    "                \n",
    "        # Average over all pairs to get the topic coherence\n",
    "        topic_coherence = sum(pair_scores) / len(pair_scores)\n",
    "        topic_coherences.append(topic_coherence)\n",
    "        \n",
    "    # Average over all topics to get the overall coherence\n",
    "    overall_coherence = sum(topic_coherences) / len(topic_coherences)\n",
    "    return overall_coherence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7dc76751",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline(*DEFAULT_PIPELINE)\n",
    "\n",
    "corpus = Corpus.from_csv(\"data/sona_speeches_only.csv\", pipeline)\n",
    "corpus_sent = Corpus.from_csv(\"data/sona_sentences_only.csv\", pipeline)\n",
    "\n",
    "topic_numbers = range(2, 12, 1)\n",
    "\n",
    "# Loop over the topic number and calculate the coherence values\n",
    "bow_coherence_values = []\n",
    "bow_coherence_values_sent = []\n",
    "tfidf_coherence_values = []\n",
    "tfidf_coherence_values_sent = []\n",
    "\n",
    "for n_topics in topic_numbers:\n",
    "    # Initialize the models\n",
    "    tfidf_plsa = PLSA(corpus, n_topics, tf_idf=True)\n",
    "    bow_plsa = PLSA(corpus, n_topics, tf_idf=False)\n",
    "\n",
    "    tfidf_plsa_sent = PLSA(corpus_sent, n_topics, tf_idf=True)\n",
    "    bow_plsa_sent = PLSA(corpus_sent, n_topics, tf_idf=False)\n",
    "\n",
    "    # Fit the models\n",
    "    tfidf_result = tfidf_plsa.fit()\n",
    "    bow_result = bow_plsa.fit()\n",
    "    tfidf_result_sent = tfidf_plsa_sent.fit()\n",
    "    bow_result_sent = bow_plsa_sent.fit()\n",
    "\n",
    "    # Calculate the coherence values\n",
    "    bow_coherence_values.append(calculate_umass_coherence(bow_result, corpus, tf_idf=False))\n",
    "    tfidf_coherence_values.append(calculate_umass_coherence(tfidf_result, corpus, tf_idf=True))\n",
    "    bow_coherence_values_sent.append(calculate_umass_coherence(bow_result_sent, corpus_sent, tf_idf=False))\n",
    "    tfidf_coherence_values_sent.append(calculate_umass_coherence(tfidf_result_sent, corpus_sent, tf_idf=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "48a82c7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the coherence value results\n",
    "pd.DataFrame({\n",
    "    'topic_number': topic_numbers,\n",
    "    'bow_coherence': bow_coherence_values,\n",
    "    'tfidf_coherence': tfidf_coherence_values,\n",
    "    'bow_coherence_sent': bow_coherence_values_sent,\n",
    "    'tfidf_coherence_sent': tfidf_coherence_values_sent\n",
    "}).to_csv('data/saved_plsa_coherence_values.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "eb022447",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| eval: true\n",
    "\n",
    "# Load the coherence value results\n",
    "coherence_values = pd.read_csv('data/saved_plsa_coherence_values.csv')\n",
    "\n",
    "topic_numbers = coherence_values['topic_number']\n",
    "bow_coherence_values = coherence_values['bow_coherence']\n",
    "tfidf_coherence_values = coherence_values['tfidf_coherence']\n",
    "bow_coherence_values_sent = coherence_values['bow_coherence_sent']\n",
    "tfidf_coherence_values_sent = coherence_values['tfidf_coherence_sent']\n",
    "\n",
    "# Plot the speech coherence values - updated\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(topic_numbers, bow_coherence_values, label='BoW', color='midnightblue')\n",
    "plt.plot(topic_numbers, tfidf_coherence_values, label='tf-idf', color='darkgray')\n",
    "plt.xlabel(\"Number of Topics\")\n",
    "plt.ylabel(\"Coherence score\")\n",
    "plt.legend(loc='lower right')\n",
    "plt.savefig(f'plsa_plots/words_coherence.png', bbox_inches='tight')\n",
    "plt.close()\n",
    "\n",
    "\n",
    "# Plot the sentence coherence values - updated\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(topic_numbers, bow_coherence_values_sent, label='BoW', color='midnightblue')\n",
    "plt.plot(topic_numbers, tfidf_coherence_values_sent, label='tf-idf', color='darkgray')\n",
    "plt.xlabel(\"Number of Topics\")\n",
    "plt.ylabel(\"Coherence score\")\n",
    "plt.legend(loc='lower right')\n",
    "plt.savefig(f'plsa_plots/sentences_coherence.png', bbox_inches='tight')\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1cd377eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run 5\n",
    "\n",
    "# TODO: Change to the official number of topics\n",
    "\n",
    "# Fit the models with the calibrated number of topics\n",
    "tfidf_plsa = PLSA(corpus, 4, tf_idf=True)\n",
    "bow_plsa = PLSA(corpus, 4, tf_idf=False)\n",
    "tfidf_plsa_sent = PLSA(corpus_sent, 8, tf_idf=True)\n",
    "bow_plsa_sent = PLSA(corpus_sent, 3, tf_idf=False)\n",
    "\n",
    "# Fit the models\n",
    "tfidf_result = tfidf_plsa.fit()\n",
    "bow_result = bow_plsa.fit()\n",
    "tfidf_result_sent = tfidf_plsa_sent.fit()\n",
    "bow_result_sent = bow_plsa_sent.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "311e419a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run 6\n",
    "\n",
    "# Function to plot the top words for a given topic\n",
    "def plot_top_words_for_topic(word_given_topic, topic_num, corptype, color, top_n=10):\n",
    "    # Extract the top words for this topic\n",
    "    top_words_data = word_given_topic[topic_num][:top_n]\n",
    "    words, probabilities = zip(*top_words_data)\n",
    "\n",
    "    # Create a bar plot\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.barh(words, probabilities, color=color)\n",
    "    plt.xlabel('Probability')\n",
    "    plt.gca().invert_yaxis() \n",
    "    plt.savefig(f'plsa_plots/{corptype}_topic_{topic_num + 1}.png', bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def plot_word_contribution(result, corptype):\n",
    "    # Number of topics in your model\n",
    "    n_topics = len(result.word_given_topic)\n",
    "\n",
    "    colours = ['lightsteelblue', 'midnightblue', 'lightgray', 'dimgray']\n",
    "    \n",
    "    i = 0\n",
    "\n",
    "    # Plot the top words for each topic\n",
    "    for topic_num in range(n_topics):\n",
    "        plot_top_words_for_topic(result.word_given_topic, topic_num, corptype, colours[i])\n",
    "        i+=1\n",
    "\n",
    "        if i == 4:\n",
    "            i = 0\n",
    "\n",
    "# Plot the word contribution for each topic for each model\n",
    "plot_word_contribution(bow_result, 'words-BoW')\n",
    "plot_word_contribution(tfidf_result, 'words-tf-idf')\n",
    "plot_word_contribution(bow_result_sent, 'sentences-BoW')\n",
    "plot_word_contribution(tfidf_result_sent, 'sentences-tf-idf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1b80e2e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| eval: true\n",
    "\n",
    "texts = sona_speeches_clean['speech']\n",
    "sentences = sona_sentences_alltogether['sentence']\n",
    "\n",
    "# Further process tokens using gensim's simple_preprocess\n",
    "tokenized_texts = [simple_preprocess(doc, deacc=True) for doc in texts]  # deacc=True removes punctuations\n",
    "tokenized_sentences = [simple_preprocess(doc, deacc=True) for doc in sentences]  # deacc=True removes punctuations\n",
    "\n",
    "# Create a Gensim dictionary from the tokenized documents\n",
    "dictionary = corpora.Dictionary(tokenized_texts)\n",
    "dictionary.filter_extremes(no_below=3, no_above=0.7)\n",
    "\n",
    "dict_sentences = corpora.Dictionary(tokenized_sentences)\n",
    "dict_sentences.filter_extremes(no_below=3, no_above=0.7)\n",
    "\n",
    "# Create a BOW corpus\n",
    "bow_corpus = [dictionary.doc2bow(text) for text in tokenized_texts]\n",
    "bow_corpus_sentences = [dict_sentences.doc2bow(text) for text in tokenized_sentences]\n",
    "\n",
    "# Create a TF-IDF corpus\n",
    "tfidf = TfidfModel(bow_corpus)\n",
    "tfidf_corpus = tfidf[bow_corpus]\n",
    "\n",
    "tfidf_sentences = TfidfModel(bow_corpus_sentences)\n",
    "tfidf_corpus_sentences = tfidf_sentences[bow_corpus_sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0e74ff52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the function to compute coherence values\n",
    "def compute_coherence_values(corpus, dictionary, k, a, b, texts):\n",
    "    lda_model = LdaModel(corpus=corpus,\n",
    "                         id2word=dictionary,\n",
    "                         num_topics=k, \n",
    "                         random_state=100,\n",
    "                         eval_every=None,\n",
    "                         alpha=a,\n",
    "                         eta=b)\n",
    "    \n",
    "    coherence_model_lda = CoherenceModel(model=lda_model, texts=texts, dictionary=dictionary, coherence='u_mass')\n",
    "    \n",
    "    return coherence_model_lda.get_coherence()\n",
    "\n",
    "# Define the parameter space for grid search\n",
    "grid = {}\n",
    "grid['Validation_Set'] = {}\n",
    "# Topics range\n",
    "min_topics = 2\n",
    "max_topics = 10\n",
    "step_size = 1\n",
    "topics_range = range(min_topics, max_topics, step_size)\n",
    "# Alpha parameter\n",
    "alpha = list(np.arange(0.1, 1, 0.1))\n",
    "# Beta parameter\n",
    "beta = list(np.arange(0.1, 1, 0.2))\n",
    "\n",
    "# Validation sets\n",
    "num_of_docs = len(bow_corpus)\n",
    "corpus_sets = [tfidf_corpus, \n",
    "               bow_corpus]\n",
    "corpus_title = ['TF-IDF Corpus', 'BoW Corpus']\n",
    "model_results = {'Validation_Set': [],\n",
    "                 'Topics': [],\n",
    "                 'Alpha': [],\n",
    "                 'Beta': [],\n",
    "                 'Coherence': []\n",
    "                }\n",
    "model_results_sentences = {'Validation_Set': [],\n",
    "                 'Topics': [],\n",
    "                 'Alpha': [],\n",
    "                 'Beta': [],\n",
    "                 'Coherence': []\n",
    "                }\n",
    "\n",
    "# Can take a long time to run\n",
    "# If you want to only test a few models, reduce the number of steps in topics_range\n",
    "# and/or limit the number of values in alpha and beta lists.\n",
    "if 1 == 1:\n",
    "    pbar = tqdm.tqdm(total=(max_topics-min_topics)*len(alpha)*len(beta)*len(corpus_sets))\n",
    "    \n",
    "    # iterate through validation corpuses\n",
    "    for i in range(len(corpus_sets)):\n",
    "        # iterate through number of topics\n",
    "        for k in topics_range:\n",
    "            # iterate through alpha values\n",
    "            for a in alpha:\n",
    "                # iterare through beta values\n",
    "                for b in beta:\n",
    "                    # get the coherence score for the given parameters\n",
    "                    cv = compute_coherence_values(corpus=corpus_sets[i], dictionary=dictionary, k=k, a=a, b=b, texts=tokenized_texts)\n",
    "                    # Save the model results\n",
    "                    model_results['Validation_Set'].append(corpus_title[i])\n",
    "                    model_results['Topics'].append(k)\n",
    "                    model_results['Alpha'].append(a)\n",
    "                    model_results['Beta'].append(b)\n",
    "                    model_results['Coherence'].append(cv)\n",
    "                    \n",
    "                    pbar.update(1)\n",
    "    pbar.close()\n",
    "\n",
    "\n",
    "# Get the results for the sentences\n",
    "if 1 == 1:\n",
    "    pbar = tqdm.tqdm(total=(max_topics-min_topics)*len(alpha)*len(beta)*len(corpus_sets))\n",
    "    \n",
    "    # iterate through validation corpuses\n",
    "    for i in range(len(corpus_sets)):\n",
    "        # iterate through number of topics\n",
    "        for k in topics_range:\n",
    "            # iterate through alpha values\n",
    "            for a in alpha:\n",
    "                # iterare through beta values\n",
    "                for b in beta:\n",
    "                    # get the coherence score for the given parameters\n",
    "                    cv = compute_coherence_values(corpus=corpus_sets[i], dictionary=dict_sentences, k=k, a=a, b=b, texts=tokenized_sentences)\n",
    "                    # Save the model results\n",
    "                    model_results_sentences['Validation_Set'].append(corpus_title[i])\n",
    "                    model_results_sentences['Topics'].append(k)\n",
    "                    model_results_sentences['Alpha'].append(a)\n",
    "                    model_results_sentences['Beta'].append(b)\n",
    "                    model_results_sentences['Coherence'].append(cv)\n",
    "                    \n",
    "                    pbar.update(1)\n",
    "    pbar.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "aa5af135",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Save the results to a csv\n",
    "# model_results_df = pd.DataFrame(model_results)\n",
    "# model_results_sentences_df = pd.DataFrame(model_results_sentences)\n",
    "\n",
    "# model_results_df.to_csv('data/sona_speeches_lda.csv', index=False)\n",
    "# model_results_sentences_df.to_csv('data/sona_sentences_lda.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "35b014cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| eval: true\n",
    "\n",
    "# Save the results to a csv\n",
    "model_results_df = pd.read_csv('data/sona_speeches_lda.csv')\n",
    "sorted_speeches_df = model_results_df.sort_values(by='Coherence', ascending=False)\n",
    "\n",
    "# Concatenate the head and tail of the DataFrame\n",
    "combined_speeches_df = pd.concat([sorted_speeches_df.head(5), sorted_speeches_df.tail(5)])\n",
    "\n",
    "combined_speeches_df['Validation_Set'] = combined_speeches_df['Validation_Set'].replace(['TF-IDF Corpus', 'BoW Corpus'], ['tf-idf', 'BoW'])\n",
    "\n",
    "# Change the validation set column name from \"Validation_Set\" to \"Corpus\"\n",
    "combined_speeches_df = combined_speeches_df.rename(columns={'Validation_Set': 'Corpus'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "24aa8bd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| eval: true\n",
    "\n",
    "# Save the results to a csv\n",
    "model_results_sentences_df = pd.read_csv('data/sona_sentences_lda.csv')\n",
    "sorted_sentences_df = model_results_sentences_df.sort_values(by='Coherence', ascending=False)\n",
    "\n",
    "# Concatenate the head and tail of the DataFrame\n",
    "combined_sentences_df = pd.concat([sorted_sentences_df.head(5), sorted_sentences_df.tail(5)])\n",
    "combined_sentences_df['Validation_Set'] = combined_sentences_df['Validation_Set'].replace(['TF-IDF Corpus', 'BoW Corpus'], ['tf-idf', 'BoW'])\n",
    "\n",
    "# Change the validation set column name from \"Validation_Set\" to \"Corpus\"\n",
    "combined_sentences_df = combined_sentences_df.rename(columns={'Validation_Set': 'Corpus'})\n",
    "\n",
    "num_cols = combined_sentences_df.select_dtypes(include=['number']).columns\n",
    "\n",
    "# Remove the first value from the num_cols\n",
    "num_cols = num_cols[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "bed7f334",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use pivot_table to handle duplicate (Alpha, Beta) pairs by averaging their coherence values\n",
    "pivot_table = model_results_df.pivot_table(index='Alpha', columns='Beta', values='Coherence', aggfunc=np.mean)\n",
    "pivot_table_sentences = model_results_sentences_df.pivot_table(index='Alpha', columns='Beta', values='Coherence', aggfunc=np.mean)\n",
    "\n",
    "# Create the meshgrid for Alpha and Beta values\n",
    "Alpha, Beta = np.meshgrid(pivot_table.columns, pivot_table.index)\n",
    "Alpha_sentences, Beta_sentences = np.meshgrid(pivot_table_sentences.columns, pivot_table_sentences.index)\n",
    "\n",
    "# Create the contour plot using the values of the pivot_table\n",
    "# We need to use the values attribute to get the coherence scores as a 2D array\n",
    "plt.figure(figsize=(8, 6))\n",
    "cp = plt.contourf(Alpha, Beta, pivot_table.values, cmap='seismic', levels=100)\n",
    "plt.colorbar(cp)\n",
    "plt.xlabel('Beta')\n",
    "plt.ylabel('Alpha')\n",
    "plt.savefig(f'lda_plots/words_contour_plot.png', bbox_inches='tight')\n",
    "plt.close()\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "cp = plt.contourf(Alpha_sentences, Beta_sentences, pivot_table_sentences.values, cmap='seismic', levels=100)\n",
    "plt.colorbar(cp)\n",
    "plt.xlabel('Beta')\n",
    "plt.ylabel('Alpha')\n",
    "plt.savefig(f'lda_plots/sentences_contour_plot.png', bbox_inches='tight')\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "word-lda",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_c192f th {\n",
       "  text-align: center;\n",
       "}\n",
       "#T_c192f td {\n",
       "  text-align: center;\n",
       "}\n",
       "#T_c192f  {\n",
       "  margin: auto;\n",
       "  border: 1px solid black;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_c192f\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th id=\"T_c192f_level0_col0\" class=\"col_heading level0 col0\" >Corpus</th>\n",
       "      <th id=\"T_c192f_level0_col1\" class=\"col_heading level0 col1\" >Topics</th>\n",
       "      <th id=\"T_c192f_level0_col2\" class=\"col_heading level0 col2\" >Alpha</th>\n",
       "      <th id=\"T_c192f_level0_col3\" class=\"col_heading level0 col3\" >Beta</th>\n",
       "      <th id=\"T_c192f_level0_col4\" class=\"col_heading level0 col4\" >Coherence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td id=\"T_c192f_row0_col0\" class=\"data row0 col0\" >BoW</td>\n",
       "      <td id=\"T_c192f_row0_col1\" class=\"data row0 col1\" >9</td>\n",
       "      <td id=\"T_c192f_row0_col2\" class=\"data row0 col2\" >0.60</td>\n",
       "      <td id=\"T_c192f_row0_col3\" class=\"data row0 col3\" >0.50</td>\n",
       "      <td id=\"T_c192f_row0_col4\" class=\"data row0 col4\" >-0.42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_c192f_row1_col0\" class=\"data row1 col0\" >BoW</td>\n",
       "      <td id=\"T_c192f_row1_col1\" class=\"data row1 col1\" >9</td>\n",
       "      <td id=\"T_c192f_row1_col2\" class=\"data row1 col2\" >0.60</td>\n",
       "      <td id=\"T_c192f_row1_col3\" class=\"data row1 col3\" >0.10</td>\n",
       "      <td id=\"T_c192f_row1_col4\" class=\"data row1 col4\" >-0.42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_c192f_row2_col0\" class=\"data row2 col0\" >BoW</td>\n",
       "      <td id=\"T_c192f_row2_col1\" class=\"data row2 col1\" >9</td>\n",
       "      <td id=\"T_c192f_row2_col2\" class=\"data row2 col2\" >0.60</td>\n",
       "      <td id=\"T_c192f_row2_col3\" class=\"data row2 col3\" >0.70</td>\n",
       "      <td id=\"T_c192f_row2_col4\" class=\"data row2 col4\" >-0.42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_c192f_row3_col0\" class=\"data row3 col0\" >BoW</td>\n",
       "      <td id=\"T_c192f_row3_col1\" class=\"data row3 col1\" >9</td>\n",
       "      <td id=\"T_c192f_row3_col2\" class=\"data row3 col2\" >0.60</td>\n",
       "      <td id=\"T_c192f_row3_col3\" class=\"data row3 col3\" >0.90</td>\n",
       "      <td id=\"T_c192f_row3_col4\" class=\"data row3 col4\" >-0.42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_c192f_row4_col0\" class=\"data row4 col0\" >BoW</td>\n",
       "      <td id=\"T_c192f_row4_col1\" class=\"data row4 col1\" >9</td>\n",
       "      <td id=\"T_c192f_row4_col2\" class=\"data row4 col2\" >0.60</td>\n",
       "      <td id=\"T_c192f_row4_col3\" class=\"data row4 col3\" >0.30</td>\n",
       "      <td id=\"T_c192f_row4_col4\" class=\"data row4 col4\" >-0.42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_c192f_row5_col0\" class=\"data row5 col0\" >tf-idf</td>\n",
       "      <td id=\"T_c192f_row5_col1\" class=\"data row5 col1\" >7</td>\n",
       "      <td id=\"T_c192f_row5_col2\" class=\"data row5 col2\" >0.20</td>\n",
       "      <td id=\"T_c192f_row5_col3\" class=\"data row5 col3\" >0.90</td>\n",
       "      <td id=\"T_c192f_row5_col4\" class=\"data row5 col4\" >-4.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_c192f_row6_col0\" class=\"data row6 col0\" >tf-idf</td>\n",
       "      <td id=\"T_c192f_row6_col1\" class=\"data row6 col1\" >7</td>\n",
       "      <td id=\"T_c192f_row6_col2\" class=\"data row6 col2\" >0.20</td>\n",
       "      <td id=\"T_c192f_row6_col3\" class=\"data row6 col3\" >0.10</td>\n",
       "      <td id=\"T_c192f_row6_col4\" class=\"data row6 col4\" >-4.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_c192f_row7_col0\" class=\"data row7 col0\" >tf-idf</td>\n",
       "      <td id=\"T_c192f_row7_col1\" class=\"data row7 col1\" >7</td>\n",
       "      <td id=\"T_c192f_row7_col2\" class=\"data row7 col2\" >0.20</td>\n",
       "      <td id=\"T_c192f_row7_col3\" class=\"data row7 col3\" >0.30</td>\n",
       "      <td id=\"T_c192f_row7_col4\" class=\"data row7 col4\" >-4.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_c192f_row8_col0\" class=\"data row8 col0\" >tf-idf</td>\n",
       "      <td id=\"T_c192f_row8_col1\" class=\"data row8 col1\" >7</td>\n",
       "      <td id=\"T_c192f_row8_col2\" class=\"data row8 col2\" >0.20</td>\n",
       "      <td id=\"T_c192f_row8_col3\" class=\"data row8 col3\" >0.50</td>\n",
       "      <td id=\"T_c192f_row8_col4\" class=\"data row8 col4\" >-4.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_c192f_row9_col0\" class=\"data row9 col0\" >tf-idf</td>\n",
       "      <td id=\"T_c192f_row9_col1\" class=\"data row9 col1\" >7</td>\n",
       "      <td id=\"T_c192f_row9_col2\" class=\"data row9 col2\" >0.20</td>\n",
       "      <td id=\"T_c192f_row9_col3\" class=\"data row9 col3\" >0.70</td>\n",
       "      <td id=\"T_c192f_row9_col4\" class=\"data row9 col4\" >-4.25</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x13f5cd9d0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| eval: true\n",
    "#| output: true\n",
    "#| label: word-lda\n",
    "#| tbl-cap: 'Table 1: Coherence values obtained from a grid search of hyperparameter combinations to tune to optimal values for implementation of LDA on SONA speeches tokenized by words.'\n",
    "\n",
    "def style_df(df):\n",
    "    # Select the numeric columns except the first one\n",
    "    numeric_cols = df.select_dtypes(include=['number']).columns[1:]\n",
    "    format_dict = {col: \"{:.2f}\" for col in numeric_cols}\n",
    "    styles = [\n",
    "        dict(selector=\"th\", props=[(\"text-align\", \"center\")]),\n",
    "        dict(selector=\"td\", props=[(\"text-align\", \"center\")]),\n",
    "        dict(selector=\"\", props=[(\"margin\", \"auto\"), (\"border\", \"1px solid black\")])\n",
    "    ]\n",
    "    return df.style.set_table_styles(styles).format(format_dict).hide()\n",
    "\n",
    "\n",
    "# Save the results to a csv\n",
    "style_df(combined_speeches_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "sent-lda",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_15f81 th {\n",
       "  text-align: center;\n",
       "}\n",
       "#T_15f81 td {\n",
       "  text-align: center;\n",
       "}\n",
       "#T_15f81  {\n",
       "  margin: auto;\n",
       "  border: 1px solid black;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_15f81\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th id=\"T_15f81_level0_col0\" class=\"col_heading level0 col0\" >Corpus</th>\n",
       "      <th id=\"T_15f81_level0_col1\" class=\"col_heading level0 col1\" >Topics</th>\n",
       "      <th id=\"T_15f81_level0_col2\" class=\"col_heading level0 col2\" >Alpha</th>\n",
       "      <th id=\"T_15f81_level0_col3\" class=\"col_heading level0 col3\" >Beta</th>\n",
       "      <th id=\"T_15f81_level0_col4\" class=\"col_heading level0 col4\" >Coherence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td id=\"T_15f81_row0_col0\" class=\"data row0 col0\" >BoW</td>\n",
       "      <td id=\"T_15f81_row0_col1\" class=\"data row0 col1\" >2</td>\n",
       "      <td id=\"T_15f81_row0_col2\" class=\"data row0 col2\" >0.90</td>\n",
       "      <td id=\"T_15f81_row0_col3\" class=\"data row0 col3\" >0.90</td>\n",
       "      <td id=\"T_15f81_row0_col4\" class=\"data row0 col4\" >-18.98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_15f81_row1_col0\" class=\"data row1 col0\" >BoW</td>\n",
       "      <td id=\"T_15f81_row1_col1\" class=\"data row1 col1\" >2</td>\n",
       "      <td id=\"T_15f81_row1_col2\" class=\"data row1 col2\" >0.80</td>\n",
       "      <td id=\"T_15f81_row1_col3\" class=\"data row1 col3\" >0.50</td>\n",
       "      <td id=\"T_15f81_row1_col4\" class=\"data row1 col4\" >-18.98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_15f81_row2_col0\" class=\"data row2 col0\" >BoW</td>\n",
       "      <td id=\"T_15f81_row2_col1\" class=\"data row2 col1\" >2</td>\n",
       "      <td id=\"T_15f81_row2_col2\" class=\"data row2 col2\" >0.70</td>\n",
       "      <td id=\"T_15f81_row2_col3\" class=\"data row2 col3\" >0.10</td>\n",
       "      <td id=\"T_15f81_row2_col4\" class=\"data row2 col4\" >-18.98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_15f81_row3_col0\" class=\"data row3 col0\" >BoW</td>\n",
       "      <td id=\"T_15f81_row3_col1\" class=\"data row3 col1\" >2</td>\n",
       "      <td id=\"T_15f81_row3_col2\" class=\"data row3 col2\" >0.70</td>\n",
       "      <td id=\"T_15f81_row3_col3\" class=\"data row3 col3\" >0.30</td>\n",
       "      <td id=\"T_15f81_row3_col4\" class=\"data row3 col4\" >-18.98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_15f81_row4_col0\" class=\"data row4 col0\" >BoW</td>\n",
       "      <td id=\"T_15f81_row4_col1\" class=\"data row4 col1\" >2</td>\n",
       "      <td id=\"T_15f81_row4_col2\" class=\"data row4 col2\" >0.70</td>\n",
       "      <td id=\"T_15f81_row4_col3\" class=\"data row4 col3\" >0.50</td>\n",
       "      <td id=\"T_15f81_row4_col4\" class=\"data row4 col4\" >-18.98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_15f81_row5_col0\" class=\"data row5 col0\" >tf-idf</td>\n",
       "      <td id=\"T_15f81_row5_col1\" class=\"data row5 col1\" >2</td>\n",
       "      <td id=\"T_15f81_row5_col2\" class=\"data row5 col2\" >0.40</td>\n",
       "      <td id=\"T_15f81_row5_col3\" class=\"data row5 col3\" >0.10</td>\n",
       "      <td id=\"T_15f81_row5_col4\" class=\"data row5 col4\" >-20.56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_15f81_row6_col0\" class=\"data row6 col0\" >tf-idf</td>\n",
       "      <td id=\"T_15f81_row6_col1\" class=\"data row6 col1\" >2</td>\n",
       "      <td id=\"T_15f81_row6_col2\" class=\"data row6 col2\" >0.40</td>\n",
       "      <td id=\"T_15f81_row6_col3\" class=\"data row6 col3\" >0.30</td>\n",
       "      <td id=\"T_15f81_row6_col4\" class=\"data row6 col4\" >-20.56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_15f81_row7_col0\" class=\"data row7 col0\" >tf-idf</td>\n",
       "      <td id=\"T_15f81_row7_col1\" class=\"data row7 col1\" >2</td>\n",
       "      <td id=\"T_15f81_row7_col2\" class=\"data row7 col2\" >0.40</td>\n",
       "      <td id=\"T_15f81_row7_col3\" class=\"data row7 col3\" >0.50</td>\n",
       "      <td id=\"T_15f81_row7_col4\" class=\"data row7 col4\" >-20.56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_15f81_row8_col0\" class=\"data row8 col0\" >tf-idf</td>\n",
       "      <td id=\"T_15f81_row8_col1\" class=\"data row8 col1\" >2</td>\n",
       "      <td id=\"T_15f81_row8_col2\" class=\"data row8 col2\" >0.40</td>\n",
       "      <td id=\"T_15f81_row8_col3\" class=\"data row8 col3\" >0.70</td>\n",
       "      <td id=\"T_15f81_row8_col4\" class=\"data row8 col4\" >-20.56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_15f81_row9_col0\" class=\"data row9 col0\" >tf-idf</td>\n",
       "      <td id=\"T_15f81_row9_col1\" class=\"data row9 col1\" >2</td>\n",
       "      <td id=\"T_15f81_row9_col2\" class=\"data row9 col2\" >0.40</td>\n",
       "      <td id=\"T_15f81_row9_col3\" class=\"data row9 col3\" >0.90</td>\n",
       "      <td id=\"T_15f81_row9_col4\" class=\"data row9 col4\" >-20.56</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x148f9c810>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| eval: true\n",
    "#| output: true\n",
    "#| label: sent-lda\n",
    "#| tbl-cap: 'Table 2: Coherence values obtained from a grid search of hyperparameter combinations to tune to optimal values for implementation of LDA on SONA speeches tokenized by sentences.'\n",
    "\n",
    "# Save the results to a csv\n",
    "style_df(combined_sentences_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "760d2a2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| eval: true\n",
    "\n",
    "# TODO: Set the number of topics based on the plots\n",
    "\n",
    "# Train the best models for each corpus\n",
    "lda_model_speeches = LdaModel(corpus=bow_corpus,\n",
    "                         id2word=dictionary,\n",
    "                         num_topics=9, \n",
    "                         random_state=100,\n",
    "                         eval_every=None,\n",
    "                         alpha=0.6,\n",
    "                         eta=0.5)\n",
    "\n",
    "lda_model_sentences = LdaModel(corpus=bow_corpus,\n",
    "                         id2word=dict_sentences,\n",
    "                         num_topics=2, \n",
    "                         random_state=100,\n",
    "                         eval_every=None,\n",
    "                         alpha=0.9,\n",
    "                         eta=0.9)\n",
    "\n",
    "# Prepare the visualization data\n",
    "vis_data_speeches = pyLDAvis.gensim_models.prepare(lda_model_speeches, bow_corpus, dictionary)\n",
    "vis_data_sentences = pyLDAvis.gensim_models.prepare(lda_model_sentences, bow_corpus_sentences, dict_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4b688304",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<link rel=\"stylesheet\" type=\"text/css\" href=\"https://cdn.jsdelivr.net/gh/bmabey/pyLDAvis@3.4.0/pyLDAvis/js/ldavis.v1.0.0.css\">\n",
       "\n",
       "\n",
       "<div id=\"ldavis_el1699455308817122844973370\" style=\"background-color:white;\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "\n",
       "var ldavis_el1699455308817122844973370_data = {\"mdsDat\": {\"x\": [-0.0030292824979288578, -0.0015002341353801325, -0.0014275955871507436, 4.673383992686359e-05, 0.0014071035566489173, 0.0013160748273492418, 0.000874189107792757, 0.0005119344869540468, 0.001801076401787907], \"y\": [-0.0002835120350127985, 0.0027885490937290375, -0.0009054908117027465, -0.0022919495404668793, -4.832580765245138e-05, 3.110039351126509e-05, -0.00018998352577630375, -0.0005522708835650193, 0.001451883116935892], \"topics\": [1, 2, 3, 4, 5, 6, 7, 8, 9], \"cluster\": [1, 1, 1, 1, 1, 1, 1, 1, 1], \"Freq\": [31.839423786940024, 21.035895191676147, 16.374120356366767, 9.706852822208258, 9.366280072986868, 4.1334111090977395, 3.597367745795299, 3.5722544220206793, 0.3743944929082167]}, \"tinfo\": {\"Term\": [\"plan\", \"shall\", \"necessary\", \"number\", \"key\", \"mining\", \"relations\", \"fact\", \"households\", \"possible\", \"crisis\", \"second\", \"elections\", \"businesses\", \"per\", \"eskom\", \"say\", \"around\", \"project\", \"question\", \"parties\", \"reform\", \"objectives\", \"potential\", \"fellow\", \"decade\", \"child\", \"law\", \"rand\", \"students\", \"alliance\", \"covid\", \"establish\", \"improvements\", \"weapons\", \"election\", \"dr\", \"proteas\", \"oil\", \"reading\", \"mining\", \"municipal\", \"stakeholders\", \"kwazulunatal\", \"secured\", \"childhood\", \"madiba\", \"extension\", \"rise\", \"hardship\", \"located\", \"broadband\", \"vaccine\", \"na\", \"promotes\", \"investors\", \"mogoeng\", \"disabilities\", \"story\", \"body\", \"eskom\", \"position\", \"municipalities\", \"funds\", \"technology\", \"elements\", \"necessary\", \"second\", \"number\", \"reform\", \"structures\", \"key\", \"northern\", \"industries\", \"prevention\", \"supply\", \"brought\", \"plan\", \"total\", \"increased\", \"per\", \"transport\", \"possible\", \"approach\", \"potential\", \"welcome\", \"cost\", \"objectives\", \"law\", \"crisis\", \"rest\", \"fellow\", \"relations\", \"shall\", \"households\", \"businesses\", \"rand\", \"fact\", \"project\", \"compatriots\", \"speech\", \"solutions\", \"mandate\", \"serious\", \"adequate\", \"seriously\", \"developmental\", \"gauteng\", \"visit\", \"engagements\", \"decade\", \"recession\", \"currently\", \"centre\", \"obligations\", \"expectancy\", \"question\", \"valued\", \"comes\", \"cut\", \"acts\", \"functioning\", \"welllocated\", \"anc\", \"sudan\", \"ready\", \"intensify\", \"pilot\", \"discuss\", \"debt\", \"legal\", \"host\", \"liberation\", \"child\", \"anniversary\", \"thousand\", \"want\", \"foundation\", \"shall\", \"aware\", \"living\", \"defence\", \"matters\", \"drive\", \"crisis\", \"households\", \"complete\", \"presidential\", \"former\", \"number\", \"elections\", \"around\", \"law\", \"agricultural\", \"second\", \"key\", \"possible\", \"relations\", \"indeed\", \"necessary\", \"objectives\", \"mining\", \"plan\", \"businesses\", \"rand\", \"eskom\", \"reform\", \"fellow\", \"leave\", \"consultations\", \"vigour\", \"republic\", \"financing\", \"authorities\", \"born\", \"road\", \"trends\", \"sugar\", \"livestock\", \"results\", \"belongs\", \"allocations\", \"obstacles\", \"priority\", \"george\", \"groupings\", \"diversity\", \"aims\", \"consequence\", \"formal\", \"interactions\", \"ensured\", \"head\", \"look\", \"devastation\", \"commitments\", \"occupy\", \"frames\", \"undertaken\", \"broad\", \"au\", \"committee\", \"plan\", \"shall\", \"proud\", \"sanitation\", \"chief\", \"objective\", \"reduction\", \"immediate\", \"terms\", \"positive\", \"reconstruction\", \"rates\", \"ministers\", \"indeed\", \"week\", \"rand\", \"objectives\", \"say\", \"could\", \"relations\", \"spirit\", \"elections\", \"households\", \"implemented\", \"crisis\", \"fellow\", \"respond\", \"implementing\", \"child\", \"businesses\", \"necessary\", \"key\", \"number\", \"possible\", \"reform\", \"law\", \"mining\", \"eskom\", \"supply\", \"potential\", \"second\", \"operations\", \"upliftment\", \"vaccines\", \"consultations\", \"millions\", \"patriotism\", \"dealt\", \"welfare\", \"spending\", \"received\", \"disability\", \"shows\", \"neither\", \"hydrogen\", \"institution\", \"strengthened\", \"weeks\", \"streets\", \"coming\", \"councils\", \"morality\", \"bold\", \"presidents\", \"healthier\", \"contain\", \"confidence\", \"fulfil\", \"syndicates\", \"competitiveness\", \"ranks\", \"involvement\", \"vital\", \"inequality\", \"benefit\", \"negotiations\", \"group\", \"town\", \"ordinary\", \"shall\", \"conference\", \"transition\", \"structure\", \"governance\", \"around\", \"urban\", \"available\", \"pandemic\", \"say\", \"meeting\", \"cent\", \"difficult\", \"businesses\", \"agreed\", \"additional\", \"criminal\", \"plan\", \"approach\", \"particularly\", \"number\", \"roads\", \"fact\", \"manufacturing\", \"restructuring\", \"possible\", \"mining\", \"key\", \"law\", \"necessary\", \"implementing\", \"increased\", \"potential\", \"elections\", \"crimes\", \"project\", \"indeed\", \"eskom\", \"reform\", \"fellow\", \"households\", \"party\", \"tons\", \"struggled\", \"movement\", \"certainty\", \"marks\", \"conflict\", \"say\", \"amount\", \"example\", \"confident\", \"nutrition\", \"councillors\", \"food\", \"deeds\", \"competitiveness\", \"ideals\", \"wife\", \"eastern\", \"changing\", \"tide\", \"hundred\", \"slow\", \"maputo\", \"guest\", \"turned\", \"attitudes\", \"congratulate\", \"house\", \"specific\", \"allocated\", \"addressed\", \"problem\", \"former\", \"speak\", \"changes\", \"anc\", \"procurement\", \"related\", \"rates\", \"starting\", \"started\", \"urban\", \"cup\", \"continued\", \"clear\", \"fact\", \"students\", \"project\", \"believe\", \"mining\", \"coming\", \"rand\", \"parties\", \"plan\", \"shall\", \"necessary\", \"key\", \"number\", \"actions\", \"per\", \"investments\", \"increased\", \"elections\", \"indeed\", \"objectives\", \"crisis\", \"relations\", \"reform\", \"question\", \"possible\", \"calendar\", \"surpassed\", \"asgisa\", \"moral\", \"evolution\", \"attend\", \"field\", \"occasion\", \"historical\", \"path\", \"practice\", \"refused\", \"machel\", \"low\", \"pretoria\", \"effectiveness\", \"inflows\", \"somewhat\", \"impetus\", \"languages\", \"applies\", \"expectations\", \"smme\", \"terrorism\", \"nd\", \"technologies\", \"setas\", \"shortage\", \"medicines\", \"favourable\", \"left\", \"municipality\", \"current\", \"project\", \"child\", \"stations\", \"nuclear\", \"alleviation\", \"courts\", \"shall\", \"partnerships\", \"plan\", \"capital\", \"learning\", \"per\", \"hand\", \"cabinet\", \"agree\", \"technical\", \"businesses\", \"systems\", \"households\", \"shared\", \"agency\", \"fact\", \"basis\", \"general\", \"started\", \"farmers\", \"parties\", \"crimes\", \"necessary\", \"present\", \"objectives\", \"reform\", \"bill\", \"number\", \"transport\", \"crisis\", \"relations\", \"mining\", \"increased\", \"context\", \"indeed\", \"elections\", \"second\", \"key\", \"possible\", \"law\", \"principle\", \"low\", \"licence\", \"intervene\", \"nutrition\", \"worst\", \"voters\", \"recently\", \"carry\", \"room\", \"opposition\", \"recognised\", \"faster\", \"arise\", \"battle\", \"vision\", \"miss\", \"integral\", \"preserve\", \"commitments\", \"deserves\", \"benefits\", \"nd\", \"nuclear\", \"thorough\", \"urgent\", \"evolved\", \"demand\", \"raising\", \"mind\", \"inclusive\", \"firmly\", \"north\", \"ability\", \"centres\", \"achieved\", \"rather\", \"municipality\", \"fact\", \"media\", \"concerns\", \"decisive\", \"redistribution\", \"technical\", \"direct\", \"say\", \"line\", \"immediate\", \"learning\", \"restructuring\", \"extend\", \"conference\", \"presidential\", \"number\", \"funds\", \"cup\", \"plan\", \"crisis\", \"improvement\", \"question\", \"proud\", \"objectives\", \"matters\", \"structures\", \"necessary\", \"elections\", \"eskom\", \"law\", \"indeed\", \"context\", \"key\", \"pleased\", \"businesses\", \"second\", \"relations\", \"shall\", \"municipalities\", \"reform\", \"households\", \"possible\", \"percent\", \"hundred\", \"related\", \"compensation\", \"nationally\", \"confront\", \"farms\", \"finalised\", \"earnings\", \"grows\", \"orders\", \"broaden\", \"zimbabwe\", \"relationship\", \"faith\", \"challenging\", \"leaks\", \"someone\", \"rich\", \"exchange\", \"redistribution\", \"scaling\", \"van\", \"testing\", \"aimed\", \"offering\", \"field\", \"unions\", \"petroleum\", \"outcomes\", \"affected\", \"developing\", \"joint\", \"largest\", \"historic\", \"context\", \"turn\", \"capture\", \"producers\", \"numbers\", \"necessary\", \"technology\", \"recovery\", \"improvement\", \"around\", \"republic\", \"procurement\", \"cent\", \"farmers\", \"project\", \"mining\", \"governance\", \"funding\", \"plan\", \"could\", \"municipalities\", \"shall\", \"possible\", \"potential\", \"approach\", \"rand\", \"matters\", \"cost\", \"urgent\", \"find\", \"bill\", \"manufacturing\", \"fact\", \"second\", \"supply\", \"law\", \"actions\", \"per\", \"businesses\", \"number\", \"indeed\", \"say\", \"relations\", \"households\", \"key\", \"crisis\", \"refurbishment\", \"attracting\", \"preferential\", \"telescope\", \"eyes\", \"wetlands\", \"twenty\", \"intensified\", \"catalyst\", \"acid\", \"victories\", \"hundred\", \"meerkat\", \"utilise\", \"parameters\", \"prospects\", \"focuses\", \"controls\", \"transferred\", \"municipality\", \"plumbers\", \"frene\", \"maths\", \"robotics\", \"mere\", \"contractors\", \"relationship\", \"sefako\", \"minimise\", \"tone\", \"colleges\", \"concerns\", \"brics\", \"prevent\", \"decades\", \"active\", \"pain\", \"changing\", \"students\", \"improvement\", \"plays\", \"huge\", \"pursuit\", \"decisive\", \"abuse\", \"bank\", \"shared\", \"teams\", \"words\", \"finalised\", \"civil\", \"relations\", \"plan\", \"early\", \"white\", \"parties\", \"informed\", \"participate\", \"houses\", \"started\", \"elections\", \"key\", \"conference\", \"manufacturing\", \"around\", \"led\", \"bill\", \"fellow\", \"criminal\", \"priority\", \"serve\", \"necessary\", \"shall\", \"households\", \"possible\", \"fact\", \"second\", \"mining\", \"potential\", \"number\", \"decade\", \"per\", \"eskom\", \"crisis\", \"question\", \"rand\", \"child\", \"reform\", \"businesses\"], \"Freq\": [127.0, 134.0, 94.0, 99.0, 82.0, 88.0, 66.0, 59.0, 68.0, 71.0, 69.0, 65.0, 57.0, 64.0, 55.0, 66.0, 58.0, 53.0, 55.0, 59.0, 50.0, 68.0, 66.0, 55.0, 58.0, 56.0, 54.0, 65.0, 60.0, 44.0, 7.801435021046376, 10.553129257521553, 21.119787553980956, 10.715335077882283, 8.085886681832303, 10.595130616393071, 9.306875313628488, 3.8804226784834372, 3.6172450187701926, 6.270306570218927, 36.634375471527015, 15.10565187901407, 14.90288543340572, 18.221502237446487, 4.085458483330703, 8.197328548437836, 9.083441370971236, 8.932958068533381, 10.196727013129987, 3.2707865364854354, 3.6630927492330922, 8.247819435282706, 6.167403648546262, 5.9547538110342275, 3.9231176578743807, 8.95496542747438, 5.7537220535409315, 8.134263847388079, 6.9236278138844085, 6.113707282224178, 26.792459171020184, 12.033422940600268, 24.02848201811213, 18.291042598005923, 16.769908090424586, 9.753163441211735, 36.68099247630819, 25.28856150270015, 37.97919739989128, 26.267730535632275, 17.889854323601316, 31.355256444467006, 14.055381520945557, 16.819811068287144, 13.55281206154954, 22.175382716800875, 15.402513310077413, 45.53425780109365, 15.623075974550687, 21.47814100409371, 20.65182830656271, 18.853688815487, 25.47384555260918, 19.782146877795782, 20.311230319059604, 17.850167920029303, 17.622415852125883, 22.90870594230116, 22.840818247893043, 23.366335516820484, 18.094346796620474, 20.526678073496125, 22.27127810983255, 33.20737759565171, 21.53340852405657, 20.656150507434404, 19.689976606496053, 19.449940603253374, 18.21915060151724, 18.365646557078637, 5.481188031811767, 8.242795811942017, 8.750942400874555, 11.649667504365178, 2.1030906708721147, 2.5867135441401996, 6.478174793810725, 9.093998080501757, 5.463938512114653, 3.42887654333868, 16.531110591370744, 3.221011550895877, 10.627309663568816, 11.83262927690027, 5.319767290372052, 2.7767314028868597, 17.196641544904722, 2.99117200055919, 3.31031032983833, 4.236485807409426, 3.0422433423224295, 5.081638222959566, 1.805503302067666, 5.34417909015102, 6.3038599960067225, 9.348043438070295, 12.226481641561223, 3.097269942943786, 3.4597660009105566, 9.440167209574758, 5.925282719210553, 11.001519359823504, 8.738541110995525, 14.517346299749683, 10.341377198249226, 8.333765755911323, 13.41657867493206, 11.032639717538064, 32.280502354494615, 6.711665999610923, 10.036327143565046, 10.218209138173421, 13.186572512976381, 9.89914553168098, 16.387267207440058, 16.16135396929637, 10.163660157526508, 12.5821863406666, 12.053974327381429, 20.32883227342172, 13.494530024559246, 12.702126510992654, 14.680948811925361, 11.180700563258574, 14.320931732895705, 16.699142616487688, 14.648762813027515, 13.913166119987016, 12.701337546073631, 17.012979666980737, 13.713597039813468, 15.668326290872631, 18.008020607223354, 12.916380301698236, 12.544400845014126, 12.965032240800243, 12.73939096667301, 12.025863749660852, 2.724764579362315, 4.477281770933754, 1.422257091084434, 11.958295146811215, 4.0141845089854, 5.54489361080477, 2.7324109749609824, 9.62226090100598, 3.0630522817387544, 2.676771624345715, 1.8964845287334073, 7.4397545413361925, 3.8380858044696344, 2.361027492614721, 2.6549844403404395, 10.900082203486594, 1.3531611584050605, 1.3676376965447405, 2.9252701803929013, 1.7232101567361833, 2.3886223741096546, 2.79512504273749, 1.7156544122554582, 2.678905855579114, 3.5968571536844602, 6.551165035533297, 1.9425631993764707, 5.54414429809577, 1.978440999276546, 1.6865223917172703, 5.312502920024046, 5.007561467180296, 4.6371987269019606, 5.965081454272449, 26.788532988905036, 28.00619234077641, 8.616193834288998, 8.936650252168276, 9.146637016944778, 7.990573265732879, 6.1777453090001995, 6.4244305806910935, 10.095346553764804, 6.964622688487404, 10.099769070732613, 9.618862270075505, 8.92473146414105, 11.120914614732257, 9.299381530702634, 11.688949191614412, 12.680950139198234, 11.338158708893138, 8.398346098001706, 12.445242560691652, 9.210168254770018, 10.93192537364462, 12.228745683260167, 7.67069691020945, 12.259039924489658, 10.584386803281431, 8.611573247248462, 9.273103186397465, 9.936740633039898, 10.99808182592587, 14.187543423670512, 12.780618161403567, 14.290339695437195, 11.461623435550612, 11.135997283974131, 10.616542274400691, 11.236876782391496, 10.10215010356457, 9.680940994034067, 9.55813021301354, 9.711237331243163, 3.096211921371871, 0.9446631606033697, 1.6402884644339484, 2.6726539473374404, 5.551252573536281, 2.481897666797762, 1.2715069620296497, 1.767172552137689, 3.8941121159915335, 4.332047425702875, 2.8496786981131006, 1.717487898243319, 1.4746538475259992, 1.461551038862362, 2.7183443273833627, 2.2580257388527496, 3.516633434462352, 1.2860128997498206, 5.268520049530533, 1.433102962234863, 1.0929906294635954, 1.6590084488180414, 2.178021755411563, 0.7738418275919873, 1.5035916370066365, 5.478087329910042, 1.9686444270838739, 1.2613410326365457, 2.99239372840543, 1.1070207551553521, 2.575854195350261, 3.881015519986991, 4.765770552548022, 4.5163732934466045, 4.301155599620483, 4.5011286705600755, 4.124905256800974, 2.8746116522248295, 16.003101584761534, 5.761763556832272, 3.885735587734175, 2.9917354971743486, 6.498402894549968, 6.4895274897673065, 5.189428861026832, 4.668564182023096, 3.753120787965023, 6.850309649979533, 4.15432095508247, 4.134369621665812, 4.794928736110044, 7.1822756335129, 4.6779157871253645, 5.524518880185959, 5.521857377029798, 11.73173195790459, 5.914032103647448, 4.783907789509034, 9.351349140713916, 4.84549139405097, 6.189958038349063, 4.955658924612174, 5.217867618665689, 6.688711553001875, 7.588552475009566, 7.269201917033858, 6.155112476018816, 7.324618836158413, 5.275111059564081, 5.639592237001279, 5.542883500469837, 5.629949568820425, 5.379569467050228, 5.48564627223054, 5.441400956690041, 5.709118726195764, 5.5254016990433135, 5.323576634427597, 5.289990999361751, 3.777153510012373, 1.4336317237629836, 1.086758611881076, 4.696708799813636, 2.399133710609939, 2.11661044788513, 3.9465554569964008, 8.058193691913779, 1.8060870701861123, 3.777035115024177, 4.336157395051784, 1.7961042745421574, 1.1831268119915117, 4.890712026408205, 1.7100598926018031, 3.0688736270456665, 1.1324102955922568, 1.1584567433941577, 6.525479750269809, 2.2417374650302073, 1.6010318962440566, 2.4401262518014595, 1.5480192664091676, 0.9334620550350103, 1.4846933840328906, 2.1140887845036285, 0.9402877509051207, 3.994960423651434, 4.713302082209917, 4.185097079745423, 4.224056524215594, 2.9339369107267177, 4.024597236937147, 6.173617038970808, 4.217364060173972, 3.8147009069144637, 2.4610428995388527, 5.299181030101528, 3.2599910132697256, 5.664086972555814, 3.9253801386898606, 5.606580578517055, 5.001682899508955, 5.002032089790597, 4.318135579709321, 5.280230038902336, 6.564122076012102, 5.1520067680153225, 6.116120236424764, 3.7782202638427265, 8.514012598063529, 4.50361736449525, 6.230166681611284, 5.472858384963095, 10.35861550618176, 10.521624123803203, 8.28909389231073, 7.417604270109719, 8.104550575140095, 5.174593534421647, 5.630247586790364, 5.194097834856813, 5.492075540085117, 5.44426074996618, 5.354676807479986, 5.596819436275052, 5.573618814334991, 5.504607424785054, 5.541715711161303, 5.339706285757359, 5.487429660801476, 0.6297160460708069, 0.38591269998916966, 1.0019387491998428, 0.9134778099877366, 0.5163089065249539, 1.1691255863529468, 0.6671929192539175, 1.650316809995901, 0.8063823031075114, 1.6846096261392207, 1.072174776338713, 0.4579397964152588, 0.5420182011844615, 1.2462039333536579, 0.591642546616879, 1.1531678011988267, 0.39338742650232994, 0.42701317995110794, 0.658719252136121, 0.49377288849484535, 0.6095833320456818, 0.7287493932777567, 0.39245915380262314, 0.3480802055381082, 0.34101791239428786, 0.9415467579107646, 0.3815837038420457, 0.5562297582675932, 0.42093693060194604, 0.42159546156065647, 0.6432641118836981, 1.927676134477462, 2.6722766992801894, 3.1092036901299434, 3.029089235648007, 1.5165178958880876, 1.163617524155164, 0.9627318516300747, 1.436394314432936, 6.786500880957276, 1.8893400691998845, 6.35165007810148, 2.2243169173881294, 1.9046329080590487, 2.8752806893784975, 1.6094941074796374, 1.9569656497242824, 1.304333625760176, 1.8860473482688451, 3.1443562936051137, 1.6778962743771577, 3.237128669461528, 1.7622402211569281, 2.3253956325200646, 2.84842040403159, 1.885851052207235, 2.172260630418829, 2.312605436171456, 2.1627295571578307, 2.4041651150650387, 2.474229317012513, 3.7723523681872972, 2.037599680304605, 2.756632947632554, 2.7929458671808005, 2.1845995045557034, 3.4655302482428043, 2.2626261308459585, 2.688842552490389, 2.614581530163691, 2.934443052508055, 2.3341810717518876, 2.2184913952524323, 2.2687814144921634, 2.2318399242668474, 2.2704967563566925, 2.3828669923803334, 2.3057096772200665, 2.248589639691089, 1.188960123772064, 1.1573895019493092, 0.4435447650233149, 0.4754052035404085, 0.7052952595638309, 0.7161486213548441, 0.417023112646563, 0.6120588977981847, 1.3244565370868353, 0.3411365538979802, 0.44788038823437415, 0.3801117476293241, 1.3573904198010938, 0.40957493872288714, 0.5845497389252904, 1.9185895096646521, 0.3346903341348516, 0.32931735710743204, 0.3216248967784386, 1.2764582876710493, 0.2965169569977758, 1.3787116037170464, 0.2907672470479237, 1.0299436035309861, 0.38529460260284076, 2.148776145731288, 0.26304154589020273, 1.5337028089494842, 0.6157766675043714, 0.7850320345357155, 1.8081675647486652, 0.8473861970534415, 1.4149008733161164, 1.182682239445399, 1.8495388834754285, 1.9651770684061867, 1.1928463728157541, 1.626170624041285, 2.807850276161561, 0.9799259044817974, 1.3212704529428347, 1.1867727128449013, 1.077581639863719, 1.6780041239722239, 1.331118924992087, 2.59061887803499, 1.9108130974689868, 1.408619420243662, 1.6335861324289058, 2.040748368756578, 1.576911686395229, 1.9608517485095835, 2.155469731381988, 3.591261812306865, 1.9635108094850904, 1.8139865774393285, 4.209731674113405, 2.633702158653119, 1.724453537715464, 2.3074140335562072, 1.7347910430586788, 2.460266869335877, 2.096350809603489, 1.8951722056412432, 2.9086420181256014, 2.1622026883572016, 2.337982989020494, 2.314290891571311, 2.0958035251685785, 1.9913426520498343, 2.5082959711346153, 1.951689281497696, 2.1936184726610346, 2.208076267699217, 2.203196986359453, 2.9926841395297523, 2.099829761143874, 2.157714587604275, 2.126313078283599, 2.1295429083437263, 1.3498710924058208, 1.059246094440907, 1.4807879159380297, 0.4491508470648345, 0.422110435101693, 1.410002952785083, 0.5580045710657858, 1.4655456646838723, 0.4041996781435777, 0.37396927412532266, 0.47122977952416456, 0.39358127063104853, 0.9959829724527213, 0.5775558937334285, 0.33494956589701674, 0.5544022655570567, 0.2661748987613139, 0.35899912631553155, 0.5261156821215748, 0.9612522729745305, 1.140244039262272, 0.44170979956011186, 0.45513400769800794, 0.4189186711304414, 1.5275234044103392, 0.3098854691722774, 0.5388921691731227, 1.1627746960196437, 0.3699809766914162, 0.8833316892163406, 1.376650575427983, 1.3511832248853664, 1.3355342331396403, 1.0279332497173292, 1.2817223975148357, 2.4768945795689064, 1.367700798490265, 1.535009578617998, 0.8703845837882705, 1.2547159949098285, 4.045132730048679, 1.9427482877165996, 1.763425573221132, 1.806392024591977, 2.352946416414865, 2.200028681891987, 1.9089298392565979, 1.5223206013602375, 1.9341685554588164, 2.3588703321745825, 3.4223012352649684, 2.2157749575495522, 1.6300797384565324, 4.2622256089435915, 1.783666685693756, 2.3812458605319944, 4.011605790372685, 2.579392726175468, 2.1781922733092345, 2.110082331938947, 2.2516938696947926, 2.056318738159966, 1.9175754182044158, 1.7908886729210847, 1.739842969863588, 1.893986527402511, 1.7946768009899725, 2.1186424016434557, 2.192617728382319, 2.0723565226015395, 2.182888651295354, 1.8878681581362846, 2.009033269781829, 2.0931334396508867, 2.4576405958609517, 1.98036092305551, 1.9896185561325739, 2.033669077228417, 2.0433796349338103, 2.066339324030939, 1.9994774975248855, 0.03922943280904853, 0.040217641123271516, 0.05373870229102203, 0.06806706837689218, 0.05057684064840592, 0.02927279396060625, 0.045538889538951924, 0.07534331112336753, 0.0331610428240486, 0.04160907506805665, 0.02872970337075795, 0.10080050093318946, 0.03622276001760651, 0.06173217266852821, 0.02795065899211502, 0.03215481785429278, 0.03233524200436366, 0.05082186716365259, 0.03198898375062264, 0.18277829045038796, 0.02762574216325704, 0.041167569660651646, 0.043490759732323525, 0.0290037961578734, 0.0817357522990793, 0.027079755130200056, 0.05845732877375934, 0.031267820311550225, 0.047276114472596435, 0.026700842249922203, 0.1256811577659604, 0.14394310339936833, 0.08259411189609459, 0.08244307151838899, 0.09232338744087411, 0.12396331234375353, 0.08118343921839383, 0.08692188328648265, 0.21758170044181846, 0.19312970522278256, 0.05711049765815783, 0.10161046267150155, 0.10597855063437041, 0.11992737782483281, 0.15031560194955432, 0.195079613555058, 0.15804718234204657, 0.10690210566352666, 0.1260731091336089, 0.12691193346422597, 0.1847758598005917, 0.27905329506134663, 0.4804354731893037, 0.17338076698100438, 0.17890810317179398, 0.2164949621386091, 0.12909136470259797, 0.15678480036794018, 0.1621422318590714, 0.2001375042947501, 0.23302561949859968, 0.30835859088881684, 0.19098184431679896, 0.18109832942704718, 0.21313144574668408, 0.1648484503363249, 0.19320759468420834, 0.22395243776283513, 0.19403276050290028, 0.19090868740994374, 0.17683113473516224, 0.30184292956758385, 0.37242841787333414, 0.23890052887443186, 0.24132503455265375, 0.21637680402930579, 0.2248699063673791, 0.2648694336354144, 0.20536658171723737, 0.27833478544717893, 0.2068936512750722, 0.20186607284693328, 0.2167034341627298, 0.21461205240167178, 0.2020584901289665, 0.20225083552409487, 0.19605438800393354, 0.2024263354211822, 0.19682995185826574], \"Total\": [127.0, 134.0, 94.0, 99.0, 82.0, 88.0, 66.0, 59.0, 68.0, 71.0, 69.0, 65.0, 57.0, 64.0, 55.0, 66.0, 58.0, 53.0, 55.0, 59.0, 50.0, 68.0, 66.0, 55.0, 58.0, 56.0, 54.0, 65.0, 60.0, 44.0, 15.739354417303128, 24.446297977062475, 49.11882175238792, 25.315446058410163, 19.13713363741694, 25.104337984609643, 22.115575286082453, 9.278254042560933, 8.662661390780672, 15.075351203566386, 88.1055827816475, 36.35400012659444, 35.9396521248239, 44.11720264666432, 9.911408722029828, 19.924271391548626, 22.08553163036019, 21.84759954400225, 24.951298853509407, 8.02626715676754, 8.993642266547779, 20.300921904184857, 15.184935258240419, 14.683499533822065, 9.680697797166273, 22.2035437864082, 14.286418711676834, 20.204253857747062, 17.230615143747485, 15.248520513222587, 66.8292022628738, 30.039932839184356, 61.01071811175492, 46.20584750260474, 42.39922231936565, 24.39422244433754, 94.52319834135776, 65.2241305339424, 99.84703652646202, 68.09036792071575, 45.64878754229764, 82.78768428793653, 35.65386066370213, 43.12680301535554, 34.395069046618495, 58.32023194450267, 39.45963870150985, 127.72520169565617, 40.080519401053856, 57.13694116299477, 55.61516513305633, 50.13719844014811, 71.01634336128258, 53.19249846503828, 55.590742046707966, 47.5700203239834, 47.114056677887916, 66.13620963261342, 65.92588789359112, 69.0904787466005, 49.069052070607256, 58.722806297749464, 66.02793925502493, 134.18201722822053, 68.00749616540928, 64.18767793381203, 60.11917887045288, 59.82356353987909, 55.76859869425486, 46.21791634730999, 17.14353783751499, 26.175728689779312, 28.520112883747135, 38.69914436161101, 6.995278983173359, 8.6492325078733, 21.732456594137787, 30.694103983882304, 18.457912713747177, 11.60309791748, 56.431497209737685, 11.069776480937577, 36.5931043078737, 40.798079587539036, 18.381967374942615, 9.611812952849753, 59.592546743484014, 10.366861048826728, 11.507753684033984, 14.750560631329531, 10.739831353806053, 17.945885661879938, 6.390015068510358, 18.93761676607077, 22.405165164757896, 33.30885958504968, 43.604469232852374, 11.099464519422632, 12.455951505099826, 34.18038917851709, 21.365215138297216, 40.02525997776999, 31.826808660186984, 54.328844797401786, 38.25329580453615, 30.826045507131855, 51.44158231656412, 41.70755275513612, 134.18201722822053, 24.433767342090956, 38.2243431627942, 39.19364298017176, 52.637273374871604, 38.13693520335622, 69.0904787466005, 68.00749616540928, 39.77487440470141, 51.653601851804815, 49.100853604967874, 99.84703652646202, 57.86960072716641, 53.65825749005788, 65.92588789359112, 45.29500977058601, 65.2241305339424, 82.78768428793653, 71.01634336128258, 66.02793925502493, 56.56822042087326, 94.52319834135776, 66.13620963261342, 88.1055827816475, 127.72520169565617, 64.18767793381203, 60.11917887045288, 66.8292022628738, 68.09036792071575, 58.722806297749464, 10.502037564965498, 18.412759765803894, 5.937222135066304, 50.3901183748706, 17.056296579029983, 23.568071816729, 11.655128544523546, 41.052973167877425, 13.096491355716708, 11.444948851189114, 8.170229682850316, 32.05421182372943, 16.54515470874135, 10.290622351036253, 11.593954998331604, 47.650570480692814, 5.973897579966691, 6.039941239923312, 12.956376410831805, 7.6773290127752905, 10.650890214910048, 12.521099753585675, 7.689362749121728, 12.023416365387108, 16.212603074991154, 29.56209044452436, 8.768252601684766, 25.089410655365864, 8.956774629216392, 7.640881011019146, 24.08631299765481, 22.75167143261146, 21.109064664962894, 27.224852616936943, 127.72520169565617, 134.18201722822053, 40.05225261350334, 41.58929745353268, 42.66048519597097, 37.21698498266369, 28.52161779234398, 29.746344843927343, 48.88376584170986, 33.03871712403171, 49.66587925337994, 47.28843651315918, 44.07523628567319, 56.56822042087326, 46.26406648664899, 60.11917887045288, 66.13620963261342, 58.45177988849471, 41.27530414354865, 66.02793925502493, 46.49203217970178, 57.86960072716641, 68.00749616540928, 37.55819855553223, 69.0904787466005, 58.722806297749464, 44.2686113832868, 49.124824315847, 54.328844797401786, 64.18767793381203, 94.52319834135776, 82.78768428793653, 99.84703652646202, 71.01634336128258, 68.09036792071575, 65.92588789359112, 88.1055827816475, 66.8292022628738, 58.32023194450267, 55.590742046707966, 65.2241305339424, 20.174166460694543, 6.457966236158514, 11.2728660951654, 18.412759765803894, 38.433481155247364, 17.27214060686528, 8.94282279884332, 12.522258835081661, 27.612294329056905, 31.095396882454576, 20.609319994716508, 12.501193333493688, 10.734384221985312, 10.639526382571667, 19.798496856177128, 16.512779606025916, 25.74031549391659, 9.467950804220077, 38.81045484540282, 10.59135885700209, 8.102618397296192, 12.307239680477212, 16.168482613120542, 5.749868342291828, 11.174573550523938, 40.71399492107195, 14.799514829439172, 9.486332465680647, 22.57662986825491, 8.358204811869113, 19.5504336325184, 29.666727993200816, 36.863531811921256, 35.07724508635196, 33.472579815290636, 35.19047214440945, 32.11793755235294, 21.988179316633335, 134.18201722822053, 45.82469119542037, 30.2018150743673, 22.93890761794022, 53.417285674988634, 53.65825749005788, 42.064618198314456, 37.5762977333736, 29.583620486000154, 58.45177988849471, 33.2479411181032, 33.09550672454106, 39.507984035068276, 64.18767793381203, 38.51698946716582, 47.780515079703605, 48.0533983598775, 127.72520169565617, 53.19249846503828, 40.220882925085846, 99.84703652646202, 41.67723717134413, 59.82356353987909, 43.145646161269184, 47.023816348602, 71.01634336128258, 88.1055827816475, 82.78768428793653, 65.92588789359112, 94.52319834135776, 49.124824315847, 57.13694116299477, 55.590742046707966, 57.86960072716641, 52.80224378711735, 55.76859869425486, 56.56822042087326, 66.8292022628738, 68.09036792071575, 58.722806297749464, 68.00749616540928, 24.88810330121067, 9.776063583348394, 7.430590398673763, 32.25905408400449, 16.84599285124798, 15.068846181058094, 28.362910155723796, 58.45177988849471, 13.119700589035817, 27.48014104238802, 31.583036946027466, 13.110559920070115, 8.655065882807222, 35.90361198457339, 12.563268782241549, 22.57662986825491, 8.352886213792846, 8.55191019132174, 48.31076761757848, 16.609362923673505, 11.999592377775908, 18.290236905005333, 11.652746000344617, 7.050090807647863, 11.24166660647251, 16.015621338718145, 7.12419060462125, 30.34252670731537, 35.810819818751554, 31.823503016574648, 32.13446446961031, 22.502422335034634, 31.169566492587727, 49.100853604967874, 33.315049706833534, 30.02446959051099, 18.93761676607077, 42.89172440794613, 25.606684146738715, 47.28843651315918, 31.66755506909496, 47.71783549282437, 42.064618198314456, 42.07841237631488, 35.65656177640228, 45.69261430009503, 59.82356353987909, 44.705829678163724, 55.76859869425486, 30.931274544131615, 88.1055827816475, 38.81045484540282, 60.11917887045288, 50.7687436666587, 127.72520169565617, 134.18201722822053, 94.52319834135776, 82.78768428793653, 99.84703652646202, 47.82931671888712, 55.61516513305633, 48.349396146361244, 57.13694116299477, 57.86960072716641, 56.56822042087326, 66.13620963261342, 69.0904787466005, 66.02793925502493, 68.09036792071575, 59.592546743484014, 71.01634336128258, 9.395684197532404, 5.934120369361524, 15.703635458484339, 14.363748864033058, 8.139604204218339, 18.437779202669695, 10.584787836472982, 26.30948537880337, 12.909743337030216, 27.358525622445455, 17.429848013863374, 7.4681987751528185, 8.916416907583617, 20.526748195842288, 9.779990331111431, 19.141912735738106, 6.531933118355685, 7.09931547967709, 10.954724172724944, 8.222631202698086, 10.151590902056302, 12.21006672309287, 6.576302800575942, 5.83748773928652, 5.729045271641188, 16.018104133624085, 6.506333337676986, 9.495023792080383, 7.194189119247405, 7.225529194008614, 11.025333459893032, 33.68466854625842, 47.33656065635578, 55.76859869425486, 54.328844797401786, 26.63024371366451, 20.32040425122741, 16.788992512212577, 25.63052238792434, 134.18201722822053, 34.55370797042278, 127.72520169565617, 41.64575906686716, 35.300612670408164, 55.61516513305633, 29.497570045446253, 36.77544790595891, 23.451049036717475, 35.71170223516122, 64.18767793381203, 31.416704402442672, 68.00749616540928, 33.308670494773715, 46.462757679485954, 59.82356353987909, 36.441205011787666, 43.95816505971023, 47.71783549282437, 44.06233458433368, 50.7687436666587, 52.80224378711735, 94.52319834135776, 41.9194896289819, 66.13620963261342, 68.09036792071575, 47.31149043346891, 99.84703652646202, 50.13719844014811, 69.0904787466005, 66.02793925502493, 88.1055827816475, 57.13694116299477, 51.82681819104382, 56.56822042087326, 57.86960072716641, 65.2241305339424, 82.78768428793653, 71.01634336128258, 65.92588789359112, 19.8859330825875, 20.526748195842288, 8.027157080192104, 8.760448550167315, 13.110559920070115, 13.497524564575773, 7.915446653812921, 11.635798478435088, 25.358102039832538, 6.55501383653798, 8.61895445875445, 7.327129634749188, 26.212426088707804, 7.922537054116241, 11.318837546284342, 37.30444263857035, 6.5149999799581915, 6.43387692501756, 6.297257779433896, 25.089410655365864, 5.832415199587288, 27.125038886844596, 5.729045271641188, 20.32040425122741, 7.6181191097315555, 42.51644014571342, 5.209220665679292, 30.465511206805008, 12.250166763763973, 15.622384143839504, 36.13507565637371, 16.91006330140678, 28.434391477870744, 23.831722638416625, 37.61634724910004, 40.35512000058844, 24.250233312061642, 33.68466854625842, 59.82356353987909, 19.87503096308717, 27.356166794788592, 24.417102969537634, 22.034342327948554, 35.71170223516122, 27.764426911202527, 58.45177988849471, 41.76452654734865, 29.746344843927343, 35.300612670408164, 47.023816348602, 35.00063790491165, 45.82469119542037, 51.653601851804815, 99.84703652646202, 46.20584750260474, 42.07841237631488, 127.72520169565617, 69.0904787466005, 39.43121039487013, 59.592546743484014, 40.05225261350334, 66.13620963261342, 52.637273374871604, 45.64878754229764, 94.52319834135776, 57.86960072716641, 66.8292022628738, 65.92588789359112, 56.56822042087326, 51.82681819104382, 82.78768428793653, 50.1521721565677, 64.18767793381203, 65.2241305339424, 66.02793925502493, 134.18201722822053, 61.01071811175492, 68.09036792071575, 68.00749616540928, 71.01634336128258, 23.06883686770843, 18.290236905005333, 25.606684146738715, 7.945935714506274, 7.489882488783445, 25.444101609407095, 10.071069262796017, 26.474302660529638, 7.439356731384263, 6.952943445719894, 8.838891287016047, 7.430566334738879, 18.823897076043014, 10.947257668524742, 6.355190222605883, 10.519885035344547, 5.060474175346914, 6.892610157051272, 10.108659971545622, 18.486965943843643, 22.034342327948554, 8.600242738604338, 8.865360775513789, 8.201647353754929, 29.9249807293617, 6.079901373410514, 10.584787836472982, 22.96936417794484, 7.313022157166575, 17.462944518258308, 27.34167445712876, 26.902747963040625, 26.617255348789865, 20.519564014018776, 26.032923048370403, 51.82681819104382, 27.93308381788812, 31.81568166265918, 17.523936308558984, 26.040109799427384, 94.52319834135776, 42.39922231936565, 38.2639188574141, 39.43121039487013, 53.65825749005788, 50.3901183748706, 42.89172440794613, 33.09550672454106, 44.06233458433368, 55.76859869425486, 88.1055827816475, 53.417285674988634, 36.583880000642274, 127.72520169565617, 41.27530414354865, 61.01071811175492, 134.18201722822053, 71.01634336128258, 55.590742046707966, 53.19249846503828, 60.11917887045288, 52.637273374871604, 47.114056677887916, 42.51644014571342, 40.62285318399583, 47.31149043346891, 43.145646161269184, 59.82356353987909, 65.2241305339424, 58.32023194450267, 65.92588789359112, 47.82931671888712, 55.61516513305633, 64.18767793381203, 99.84703652646202, 56.56822042087326, 58.45177988849471, 66.02793925502493, 68.00749616540928, 82.78768428793653, 69.0904787466005, 6.310025865574632, 6.77700442755357, 9.256133573563062, 11.754730940870678, 8.734718192659152, 5.079849306062448, 7.942810942090153, 13.21735856700756, 5.868647949750721, 7.538865881248895, 5.205897826674961, 18.290236905005333, 6.582916211643458, 11.224579475267472, 5.102466895714825, 5.880113670334659, 5.925159065754873, 9.327591430699176, 5.8786703424670765, 33.68466854625842, 5.094147911938972, 7.592924739744635, 8.072529718306697, 5.393220969058001, 15.244083690266688, 5.067154272475476, 10.947257668524742, 5.872490439921404, 8.88205245140126, 5.028422194372076, 23.7270676585454, 27.356166794788592, 15.610722619574542, 15.625592783343516, 17.539531124168317, 23.821642982393378, 15.404904235004839, 16.609362923673505, 44.705829678163724, 39.43121039487013, 10.798194258201136, 20.028916665799677, 21.063207845198242, 24.417102969537634, 31.36925763435209, 41.993776484989425, 33.308670494773715, 21.625609951417115, 26.10117823628512, 26.474302660529638, 40.87330346831778, 66.02793925502493, 127.72520169565617, 38.515009744992796, 40.17874367591015, 50.7687436666587, 27.025698945599736, 34.325803527063876, 35.93897110716588, 47.71783549282437, 57.86960072716641, 82.78768428793653, 45.82469119542037, 43.145646161269184, 53.65825749005788, 38.13984873045201, 47.31149043346891, 58.722806297749464, 48.0533983598775, 47.650570480692814, 42.845827042120526, 94.52319834135776, 134.18201722822053, 68.00749616540928, 71.01634336128258, 59.82356353987909, 65.2241305339424, 88.1055827816475, 55.590742046707966, 99.84703652646202, 56.431497209737685, 55.61516513305633, 66.8292022628738, 69.0904787466005, 59.592546743484014, 60.11917887045288, 54.328844797401786, 68.09036792071575, 64.18767793381203], \"Category\": [\"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\"], \"logprob\": [30.0, 29.0, 28.0, 27.0, 26.0, 25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, -7.7159, -7.4138, -6.72, -7.3985, -7.6801, -7.4098, -7.5395, -8.4143, -8.4845, -7.9344, -6.1692, -7.0552, -7.0687, -6.8676, -8.3628, -7.6664, -7.5638, -7.5805, -7.4482, -8.5852, -8.4719, -7.6603, -7.9509, -7.986, -8.4033, -7.578, -8.0204, -7.6741, -7.8353, -7.9597, -6.4821, -7.2825, -6.591, -6.8638, -6.9506, -7.4926, -6.168, -6.5399, -6.1332, -6.5019, -6.886, -6.3248, -7.1272, -6.9477, -7.1636, -6.6712, -7.0357, -5.9518, -7.0215, -6.7032, -6.7424, -6.8335, -6.5326, -6.7854, -6.759, -6.8882, -6.9011, -6.6387, -6.6417, -6.6189, -6.8746, -6.7485, -6.6669, -6.2675, -6.7006, -6.7422, -6.7901, -6.8024, -6.8678, -6.4453, -7.6544, -7.2464, -7.1866, -6.9005, -8.6123, -8.4054, -7.4873, -7.1481, -7.6576, -8.1235, -6.5505, -8.1861, -6.9923, -6.8849, -7.6843, -8.3345, -6.511, -8.2601, -8.1587, -7.912, -8.2432, -7.7301, -8.7649, -7.6797, -7.5146, -7.1206, -6.8521, -8.2252, -8.1145, -7.1108, -7.5765, -6.9577, -7.188, -6.6804, -7.0196, -7.2354, -6.7593, -6.9549, -5.8813, -7.4519, -7.0495, -7.0316, -6.7765, -7.0633, -6.5592, -6.5731, -7.0369, -6.8235, -6.8664, -6.3437, -6.7535, -6.814, -6.6692, -6.9416, -6.694, -6.5404, -6.6714, -6.7229, -6.814, -6.5218, -6.7374, -6.6041, -6.4649, -6.7973, -6.8265, -6.7935, -6.811, -6.8687, -8.1028, -7.6062, -8.753, -6.6238, -7.7154, -7.3923, -8.1, -6.8411, -7.9858, -8.1206, -8.4652, -7.0984, -7.7602, -8.2461, -8.1288, -6.7164, -8.8028, -8.7921, -8.0318, -8.561, -8.2345, -8.0773, -8.5654, -8.1198, -7.8252, -7.2256, -8.4412, -7.3925, -8.4229, -8.5826, -7.4352, -7.4943, -7.5711, -7.3193, -5.8172, -5.7728, -6.9516, -6.9151, -6.8918, -7.027, -7.2843, -7.2451, -6.7931, -7.1644, -6.7927, -6.8415, -6.9164, -6.6964, -6.8753, -6.6466, -6.5651, -6.677, -6.9772, -6.5839, -6.8849, -6.7135, -6.6014, -7.0678, -6.599, -6.7458, -6.9521, -6.8781, -6.809, -6.7075, -6.4529, -6.5573, -6.4456, -6.6662, -6.695, -6.7428, -6.686, -6.7925, -6.8351, -6.8478, -6.8319, -7.4522, -8.6393, -8.0875, -7.5993, -6.8683, -7.6733, -8.3421, -8.013, -7.2229, -7.1163, -7.5351, -8.0415, -8.1939, -8.2029, -7.5823, -7.7679, -7.3248, -8.3308, -6.9206, -8.2225, -8.4934, -8.0761, -7.8039, -8.8387, -8.1745, -6.8816, -7.905, -8.3502, -7.4863, -8.4807, -7.6362, -7.2263, -7.0209, -7.0746, -7.1235, -7.078, -7.1653, -7.5264, -5.8096, -6.8311, -7.225, -7.4865, -6.7108, -6.7122, -6.9357, -7.0415, -7.2598, -6.6581, -7.1582, -7.163, -7.0148, -6.6107, -7.0395, -6.8732, -6.8736, -6.1201, -6.805, -7.0171, -6.3468, -7.0043, -6.7594, -6.9818, -6.9303, -6.6819, -6.5557, -6.5987, -6.7651, -6.5911, -6.9194, -6.8525, -6.8698, -6.8542, -6.8997, -6.8802, -6.8883, -6.8403, -6.873, -6.9102, -6.9165, -7.2177, -8.1864, -8.4634, -6.9998, -7.6715, -7.7968, -7.1738, -6.4599, -7.9555, -7.2177, -7.0796, -7.961, -8.3785, -6.9593, -8.0101, -7.4253, -8.4223, -8.3995, -6.6709, -7.7394, -8.076, -7.6546, -8.1097, -8.6155, -8.1514, -7.798, -8.6082, -7.1616, -6.9962, -7.1151, -7.1058, -7.4703, -7.1542, -6.7263, -7.1074, -7.2078, -7.646, -6.8791, -7.3649, -6.8125, -7.1792, -6.8227, -6.9369, -6.9368, -7.0838, -6.8827, -6.665, -6.9072, -6.7357, -7.2174, -6.4049, -7.0418, -6.7172, -6.8468, -6.2088, -6.1932, -6.4317, -6.5428, -6.4542, -6.9029, -6.8185, -6.8991, -6.8433, -6.8521, -6.8687, -6.8244, -6.8286, -6.841, -6.8343, -6.8715, -6.8442, -8.1911, -8.6808, -7.7267, -7.8191, -8.3897, -7.5724, -8.1333, -7.2277, -7.9438, -7.2071, -7.6589, -8.5096, -8.3411, -7.5085, -8.2535, -7.5861, -8.6616, -8.5796, -8.1461, -8.4343, -8.2236, -8.045, -8.6639, -8.7839, -8.8044, -7.7889, -8.692, -8.3152, -8.5939, -8.5923, -8.1698, -7.0723, -6.7457, -6.5943, -6.6204, -7.3122, -7.5771, -7.7666, -7.3665, -5.8137, -7.0924, -5.8799, -6.9292, -7.0843, -6.6725, -7.2527, -7.0572, -7.4629, -7.0941, -6.583, -7.2111, -6.5539, -7.162, -6.8847, -6.6819, -7.0942, -6.9529, -6.8902, -6.9573, -6.8514, -6.8227, -6.4009, -7.0168, -6.7146, -6.7015, -6.9472, -6.4858, -6.9121, -6.7395, -6.7675, -6.6521, -6.881, -6.9318, -6.9094, -6.9258, -6.9086, -6.8603, -6.8932, -6.9183, -7.4166, -7.4436, -8.4027, -8.3333, -7.9389, -7.9236, -8.4643, -8.0806, -7.3087, -8.6652, -8.3929, -8.557, -7.2842, -8.4824, -8.1266, -6.9381, -8.6843, -8.7005, -8.7241, -7.3456, -8.8054, -7.2686, -8.825, -7.5602, -8.5435, -6.8248, -8.9252, -7.162, -8.0746, -7.8318, -6.9974, -7.7553, -7.2427, -7.4219, -6.9748, -6.9141, -7.4134, -7.1035, -6.5573, -7.61, -7.3111, -7.4185, -7.515, -7.0721, -7.3037, -6.6378, -6.9422, -7.2471, -7.0989, -6.8764, -7.1343, -6.9163, -6.8217, -6.3112, -6.915, -6.9942, -6.1523, -6.6213, -7.0448, -6.7536, -7.0388, -6.6895, -6.8495, -6.9504, -6.522, -6.8186, -6.7404, -6.7506, -6.8498, -6.9009, -6.6701, -6.921, -6.8042, -6.7976, -6.7998, -6.4935, -6.8479, -6.8207, -6.8353, -6.8338, -7.2827, -7.5252, -7.1901, -8.3831, -8.4452, -7.2391, -8.1661, -7.2005, -8.4886, -8.5663, -8.3351, -8.5152, -7.5867, -8.1317, -8.6765, -8.1726, -8.9063, -8.6072, -8.2249, -7.6222, -7.4515, -8.3998, -8.3699, -8.4528, -7.1591, -8.7543, -8.201, -7.4319, -8.577, -7.7068, -7.2631, -7.2817, -7.2934, -7.5552, -7.3345, -6.6757, -7.2696, -7.1542, -7.7215, -7.3558, -6.1852, -6.9186, -7.0155, -6.9914, -6.727, -6.7942, -6.9362, -7.1625, -6.923, -6.7245, -6.3524, -6.7871, -7.0941, -6.1329, -7.004, -6.7151, -6.1935, -6.6352, -6.8042, -6.836, -6.771, -6.8618, -6.9317, -7.0, -7.0289, -6.944, -6.9979, -6.8319, -6.7976, -6.854, -6.8021, -6.9473, -6.8851, -6.8441, -6.6835, -6.8994, -6.8948, -6.8729, -6.8681, -6.8569, -6.8898, -8.5654, -8.5405, -8.2507, -8.0143, -8.3113, -8.8582, -8.4163, -7.9128, -8.7335, -8.5065, -8.8769, -7.6217, -8.6451, -8.112, -8.9044, -8.7643, -8.7587, -8.3065, -8.7694, -7.0266, -8.9161, -8.5172, -8.4623, -8.8674, -7.8313, -8.936, -8.1665, -8.7922, -8.3788, -8.9501, -7.4011, -7.2654, -7.8209, -7.8227, -7.7095, -7.4148, -7.8381, -7.7698, -6.8523, -6.9715, -8.1898, -7.6137, -7.5716, -7.4479, -7.2221, -6.9614, -7.1719, -7.5629, -7.398, -7.3913, -7.0157, -6.6034, -6.0601, -7.0793, -7.048, -6.8573, -7.3743, -7.18, -7.1464, -6.9358, -6.7837, -6.5036, -6.9826, -7.0358, -6.8729, -7.1298, -6.9711, -6.8234, -6.9668, -6.983, -7.0596, -6.5249, -6.3148, -6.7588, -6.7487, -6.8578, -6.8193, -6.6556, -6.91, -6.606, -6.9026, -6.9272, -6.8563, -6.866, -6.9263, -6.9253, -6.9564, -6.9245, -6.9525], \"loglift\": [30.0, 29.0, 28.0, 27.0, 26.0, 25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, 0.4426, 0.3044, 0.3004, 0.2847, 0.283, 0.2818, 0.2789, 0.2727, 0.2712, 0.2672, 0.2669, 0.2662, 0.2642, 0.2602, 0.2582, 0.2563, 0.256, 0.2501, 0.2496, 0.2468, 0.2463, 0.2437, 0.2434, 0.2419, 0.2412, 0.2364, 0.235, 0.2347, 0.2327, 0.2305, 0.2304, 0.2296, 0.2127, 0.2178, 0.2169, 0.2277, 0.1979, 0.197, 0.1779, 0.192, 0.2077, 0.1736, 0.2136, 0.2029, 0.2131, 0.1775, 0.2037, 0.113, 0.2023, 0.166, 0.1538, 0.1664, 0.1192, 0.1553, 0.1376, 0.1643, 0.1611, 0.0843, 0.0845, 0.0603, 0.1468, 0.0934, 0.0577, -0.252, -0.0055, 0.0107, 0.0282, 0.0209, 0.0257, 0.6361, 0.4186, 0.4034, 0.3775, 0.3584, 0.3571, 0.3519, 0.3486, 0.3425, 0.3416, 0.3399, 0.3312, 0.3244, 0.3225, 0.3212, 0.319, 0.3172, 0.3161, 0.316, 0.313, 0.3114, 0.2976, 0.2972, 0.295, 0.2938, 0.2908, 0.2883, 0.2874, 0.2826, 0.2779, 0.2723, 0.2764, 0.2675, 0.2664, 0.2392, 0.2509, 0.2509, 0.215, 0.2291, 0.1342, 0.2668, 0.2217, 0.2146, 0.1747, 0.2102, 0.12, 0.1219, 0.1945, 0.1467, 0.1545, -0.0327, 0.103, 0.1181, 0.057, 0.1599, 0.0428, -0.042, -0.0196, 0.0017, 0.0652, -0.1559, -0.0144, -0.168, -0.4001, -0.0444, -0.0081, -0.0809, -0.1172, -0.0268, 0.4603, 0.3954, 0.3805, 0.3711, 0.3628, 0.3625, 0.3589, 0.3587, 0.3565, 0.3565, 0.349, 0.3489, 0.3483, 0.3373, 0.3354, 0.3343, 0.3245, 0.3242, 0.3213, 0.3154, 0.3145, 0.3099, 0.3094, 0.308, 0.3037, 0.3026, 0.3023, 0.2998, 0.2994, 0.2986, 0.2979, 0.2958, 0.2939, 0.2913, 0.2476, 0.2427, 0.2729, 0.2718, 0.2696, 0.271, 0.2798, 0.2769, 0.2321, 0.2526, 0.2167, 0.2169, 0.2124, 0.1828, 0.2051, 0.1718, 0.1579, 0.1694, 0.2172, 0.1407, 0.1905, 0.143, 0.0936, 0.221, 0.0803, 0.096, 0.1723, 0.1422, 0.1107, 0.0454, -0.087, -0.0589, -0.1346, -0.0144, -0.0012, -0.0166, -0.2499, -0.0799, 0.0137, 0.0488, -0.0951, 0.4581, 0.4101, 0.4048, 0.4024, 0.3974, 0.3923, 0.3817, 0.3742, 0.3735, 0.3613, 0.3538, 0.3474, 0.3473, 0.3473, 0.3468, 0.3427, 0.3418, 0.336, 0.3354, 0.3321, 0.3291, 0.3284, 0.3277, 0.3268, 0.3266, 0.3265, 0.3151, 0.3147, 0.3115, 0.3108, 0.3055, 0.2984, 0.2866, 0.2825, 0.2805, 0.2759, 0.28, 0.2978, 0.2059, 0.2588, 0.2817, 0.2954, 0.2258, 0.2199, 0.2398, 0.2468, 0.2677, 0.1884, 0.2525, 0.2523, 0.2234, 0.1421, 0.2241, 0.1749, 0.1687, -0.0552, 0.1357, 0.2032, -0.0358, 0.1804, 0.0639, 0.1683, 0.1338, -0.0302, -0.1196, -0.1003, -0.0389, -0.2253, 0.101, 0.0167, 0.0268, 0.0022, 0.0484, 0.0133, -0.0091, -0.1277, -0.1791, -0.0683, -0.2215, 0.4826, 0.4483, 0.4456, 0.4411, 0.419, 0.4052, 0.3958, 0.3865, 0.3851, 0.3835, 0.3824, 0.3803, 0.3781, 0.3746, 0.3738, 0.3724, 0.3698, 0.369, 0.3661, 0.3653, 0.3538, 0.3537, 0.3495, 0.3462, 0.3436, 0.3431, 0.343, 0.3405, 0.3402, 0.3394, 0.3389, 0.3308, 0.321, 0.2945, 0.3013, 0.3049, 0.3275, 0.2769, 0.3069, 0.2459, 0.2802, 0.2267, 0.2386, 0.2384, 0.2569, 0.2101, 0.1583, 0.2073, 0.1578, 0.2655, 0.0312, 0.2142, 0.1011, 0.1406, -0.144, -0.1777, -0.0659, -0.0444, -0.1432, 0.1442, 0.0778, 0.1371, 0.0259, 0.0044, 0.0106, -0.1015, -0.1493, -0.1164, -0.1405, -0.0443, -0.1924, 0.4833, 0.4532, 0.4341, 0.4309, 0.4283, 0.4279, 0.422, 0.4171, 0.4129, 0.3986, 0.3976, 0.3944, 0.3857, 0.3844, 0.3809, 0.3767, 0.3764, 0.3751, 0.3748, 0.3735, 0.3735, 0.3674, 0.3673, 0.3664, 0.3647, 0.3521, 0.3499, 0.3487, 0.3475, 0.3447, 0.3447, 0.3253, 0.3117, 0.2992, 0.2993, 0.3204, 0.326, 0.3274, 0.3044, 0.2018, 0.2798, 0.1849, 0.2563, 0.2665, 0.2238, 0.2777, 0.2526, 0.2968, 0.2451, 0.1699, 0.2563, 0.1411, 0.2468, 0.1913, 0.1414, 0.2247, 0.1786, 0.1591, 0.1718, 0.136, 0.1254, -0.0351, 0.1621, 0.0084, -0.0077, 0.1107, -0.1747, 0.0878, -0.0602, -0.0429, -0.216, -0.0117, 0.035, -0.0301, -0.0693, -0.1718, -0.3619, -0.2415, -0.1922, 0.508, 0.4494, 0.4292, 0.4111, 0.4024, 0.3886, 0.3815, 0.38, 0.3729, 0.3693, 0.3678, 0.3661, 0.3643, 0.3626, 0.3616, 0.3574, 0.3563, 0.3527, 0.3505, 0.3466, 0.3459, 0.3457, 0.3442, 0.3428, 0.3407, 0.34, 0.3391, 0.3361, 0.3346, 0.3342, 0.33, 0.3315, 0.3244, 0.3217, 0.3125, 0.3028, 0.3129, 0.2942, 0.266, 0.3152, 0.2946, 0.3009, 0.3071, 0.2671, 0.2872, 0.2087, 0.2404, 0.2749, 0.2518, 0.1876, 0.2251, 0.1735, 0.1484, -0.0002, 0.1666, 0.181, -0.0875, 0.0579, 0.1953, 0.0736, 0.1857, 0.0335, 0.1017, 0.1433, -0.1562, 0.0379, -0.0279, -0.0245, 0.0295, 0.0659, -0.1717, 0.0786, -0.0513, -0.0607, -0.0752, -0.4781, -0.0442, -0.1268, -0.1403, -0.182, 0.4935, 0.4832, 0.4817, 0.4589, 0.4559, 0.4391, 0.4389, 0.438, 0.4193, 0.4092, 0.4004, 0.3939, 0.3928, 0.3899, 0.3889, 0.3888, 0.3869, 0.3771, 0.3763, 0.3754, 0.3706, 0.3631, 0.3627, 0.3576, 0.3569, 0.3554, 0.3543, 0.3486, 0.348, 0.3478, 0.3432, 0.3407, 0.3397, 0.3381, 0.3208, 0.2911, 0.3153, 0.3006, 0.3296, 0.2992, 0.1806, 0.2489, 0.2547, 0.2487, 0.205, 0.2006, 0.2198, 0.2528, 0.206, 0.1689, 0.0838, 0.1494, 0.221, -0.0681, 0.1904, 0.0885, -0.178, 0.0166, 0.0925, 0.1048, 0.0473, 0.0895, 0.1305, 0.1648, 0.1814, 0.1139, 0.1522, -0.0087, -0.0608, -0.0053, -0.0759, 0.0998, 0.0112, -0.0912, -0.3725, -0.0202, -0.0483, -0.1483, -0.173, -0.3585, -0.2106, 0.5071, 0.4606, 0.4387, 0.4361, 0.436, 0.4312, 0.4262, 0.4204, 0.4116, 0.3881, 0.388, 0.3866, 0.3851, 0.3846, 0.3806, 0.3788, 0.3768, 0.3752, 0.3739, 0.3711, 0.3705, 0.3703, 0.3639, 0.3621, 0.3592, 0.3559, 0.3551, 0.3522, 0.3518, 0.3494, 0.347, 0.3403, 0.3458, 0.3431, 0.3407, 0.3293, 0.3419, 0.3349, 0.2623, 0.2687, 0.3455, 0.3038, 0.2956, 0.2715, 0.2468, 0.2157, 0.2369, 0.2779, 0.2547, 0.2472, 0.1885, 0.1212, 0.0047, 0.1843, 0.1734, 0.1301, 0.2436, 0.1988, 0.1865, 0.1136, 0.0728, -0.0052, 0.1072, 0.1143, 0.0591, 0.1436, 0.0869, 0.0185, 0.0756, 0.0678, 0.0974, -0.1591, -0.2993, -0.0637, -0.0969, -0.0345, -0.0824, -0.2194, -0.0134, -0.295, -0.021, -0.031, -0.1438, -0.1867, -0.0991, -0.107, -0.0368, -0.2306, -0.1996]}, \"token.table\": {\"Topic\": [1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 6, 8, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 7, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 8, 1, 2, 3, 4, 5, 6, 8, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 8, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 7, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 8, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 8, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 8, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 8, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 8, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 8, 1, 2, 3, 4, 5, 1, 2, 3, 5, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 7, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 1, 2, 3, 4, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 8, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 8, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 7, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 8, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 8, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 1, 2, 3, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 7, 8], \"Freq\": [0.3356870219320209, 0.25176526644901565, 0.16784351096601044, 0.08392175548300522, 0.08392175548300522, 0.04196087774150261, 0.04196087774150261, 0.04196087774150261, 0.35066178894696043, 0.19127006669834207, 0.1593917222486184, 0.09563503334917103, 0.09563503334917103, 0.03187834444972368, 0.03187834444972368, 0.03187834444972368, 0.3221400407138038, 0.24780003131831063, 0.12390001565915532, 0.07434000939549319, 0.07434000939549319, 0.04956000626366213, 0.04956000626366213, 0.04956000626366213, 0.26529189290587013, 0.26529189290587013, 0.13264594645293507, 0.13264594645293507, 0.13264594645293507, 0.33452286374983536, 0.2090767898436471, 0.14635375289055297, 0.08363071593745884, 0.10453839492182355, 0.04181535796872942, 0.04181535796872942, 0.04181535796872942, 0.3358290612411921, 0.20989316327574506, 0.12593589796544705, 0.08395726531029803, 0.08395726531029803, 0.04197863265514901, 0.04197863265514901, 0.04197863265514901, 0.27933399521556174, 0.27933399521556174, 0.09311133173852058, 0.09311133173852058, 0.09311133173852058, 0.2930064687801362, 0.20929033484295442, 0.18836130135865897, 0.12557420090577265, 0.08371613393718176, 0.04185806696859088, 0.04185806696859088, 0.04185806696859088, 0.3110776207013726, 0.17775864040078437, 0.17775864040078437, 0.08887932020039219, 0.13331898030058828, 0.04443966010019609, 0.04443966010019609, 0.04443966010019609, 0.285907110325529, 0.285907110325529, 0.1429535551627645, 0.1429535551627645, 0.1429535551627645, 0.3291678428148881, 0.2194452285432587, 0.14629681902883915, 0.10972261427162935, 0.07314840951441957, 0.03657420475720979, 0.03657420475720979, 0.03657420475720979, 0.36588443839840723, 0.21522614023435718, 0.17218091218748574, 0.06456784207030715, 0.06456784207030715, 0.043045228046871435, 0.043045228046871435, 0.021522614023435718, 0.29849410954026195, 0.21321007824304425, 0.21321007824304425, 0.08528403129721769, 0.08528403129721769, 0.042642015648608846, 0.042642015648608846, 0.042642015648608846, 0.2596256908532427, 0.23366312176791845, 0.18173798359726992, 0.12981284542662136, 0.1038502763412971, 0.05192513817064855, 0.05192513817064855, 0.025962569085324273, 0.3532397957531779, 0.2428523595803098, 0.11038743617286809, 0.08830994893829447, 0.08830994893829447, 0.044154974469147235, 0.044154974469147235, 0.022077487234573617, 0.36758586745578264, 0.16708448520717395, 0.16708448520717395, 0.06683379408286957, 0.10025069112430436, 0.03341689704143479, 0.03341689704143479, 0.06683379408286957, 0.2605072671331324, 0.1302536335665662, 0.2605072671331324, 0.1302536335665662, 0.1302536335665662, 0.23825134218687258, 0.23825134218687258, 0.17868850664015443, 0.11912567109343629, 0.11912567109343629, 0.059562835546718144, 0.059562835546718144, 0.5082800595178901, 0.12707001487947253, 0.12707001487947253, 0.06353500743973627, 0.12707001487947253, 0.06353500743973627, 0.06353500743973627, 0.28007312860344497, 0.21783465558045717, 0.1555961825574694, 0.09335770953448165, 0.12447694604597553, 0.031119236511493883, 0.031119236511493883, 0.031119236511493883, 0.19435170505490393, 0.19435170505490393, 0.19435170505490393, 0.09717585252745196, 0.09717585252745196, 0.3048850065483061, 0.15244250327415304, 0.15244250327415304, 0.07622125163707652, 0.15244250327415304, 0.07622125163707652, 0.07622125163707652, 0.21121981975929122, 0.26402477469911406, 0.15841486481946843, 0.10560990987964561, 0.10560990987964561, 0.052804954939822804, 0.052804954939822804, 0.31369846042329813, 0.26141538368608175, 0.15684923021164907, 0.07842461510582453, 0.07842461510582453, 0.052283076737216355, 0.026141538368608178, 0.026141538368608178, 0.1970134552599909, 0.2955201828899864, 0.1970134552599909, 0.09850672762999545, 0.09850672762999545, 0.09850672762999545, 0.37599286698565887, 0.20679607684211238, 0.11279786009569766, 0.11279786009569766, 0.07519857339713178, 0.03759928669856589, 0.03759928669856589, 0.03759928669856589, 0.25244438572374717, 0.25244438572374717, 0.12622219286187358, 0.12622219286187358, 0.12622219286187358, 0.2981833691294313, 0.24227398741766293, 0.14909168456471564, 0.11181876342353674, 0.09318230285294728, 0.03727292114117891, 0.03727292114117891, 0.03727292114117891, 0.3820771321304665, 0.19103856606523326, 0.12735904404348886, 0.06367952202174443, 0.06367952202174443, 0.06367952202174443, 0.06367952202174443, 0.06367952202174443, 0.21694586728866028, 0.2711823341108253, 0.1627094004664952, 0.10847293364433014, 0.05423646682216507, 0.05423646682216507, 0.05423646682216507, 0.05423646682216507, 0.28073364554601604, 0.28073364554601604, 0.14036682277300802, 0.14036682277300802, 0.14036682277300802, 0.29511564015931735, 0.14755782007965867, 0.14755782007965867, 0.14755782007965867, 0.14755782007965867, 0.28423807948055985, 0.1894920529870399, 0.23686506623379985, 0.09474602649351994, 0.09474602649351994, 0.04737301324675997, 0.04737301324675997, 0.04737301324675997, 0.2970119937869201, 0.21215142413351434, 0.2545817089602172, 0.08486056965340574, 0.08486056965340574, 0.04243028482670287, 0.04243028482670287, 0.04243028482670287, 0.3193502479980121, 0.18628764466550707, 0.15967512399900605, 0.13306260333250505, 0.07983756199950302, 0.05322504133300202, 0.02661252066650101, 0.02661252066650101, 0.2864887719521424, 0.2864887719521424, 0.1637078696869385, 0.08185393484346926, 0.08185393484346926, 0.04092696742173463, 0.04092696742173463, 0.04092696742173463, 0.3571957860318115, 0.19050441921696615, 0.19050441921696615, 0.09525220960848307, 0.0714391572063623, 0.04762610480424154, 0.02381305240212077, 0.02381305240212077, 0.329297562913146, 0.1920902450326685, 0.164648781456573, 0.109765854304382, 0.109765854304382, 0.054882927152191, 0.0274414635760955, 0.0274414635760955, 0.2650448853720686, 0.17669659024804574, 0.17669659024804574, 0.08834829512402287, 0.08834829512402287, 0.08834829512402287, 0.2909676414128725, 0.2263081655433453, 0.16164868967381807, 0.09698921380429085, 0.12931895173905447, 0.03232973793476362, 0.03232973793476362, 0.03232973793476362, 0.3022032787253621, 0.1813219672352173, 0.2417626229802897, 0.06044065574507242, 0.12088131149014485, 0.06044065574507242, 0.06044065574507242, 0.06044065574507242, 0.2850851021903899, 0.19955957153327294, 0.17105106131423395, 0.14254255109519495, 0.08552553065711697, 0.05701702043807798, 0.02850851021903899, 0.02850851021903899, 0.22119783956917927, 0.22119783956917927, 0.18433153297431606, 0.11059891978458963, 0.11059891978458963, 0.03686630659486321, 0.03686630659486321, 0.03686630659486321, 0.3170477163701602, 0.23250165867145084, 0.16909211539741878, 0.08454605769870939, 0.08454605769870939, 0.042273028849354695, 0.021136514424677347, 0.042273028849354695, 0.39348079669743474, 0.19674039834871737, 0.1311602655658116, 0.0655801327829058, 0.0655801327829058, 0.243758964470226, 0.16250597631348399, 0.16250597631348399, 0.16250597631348399, 0.08125298815674199, 0.08125298815674199, 0.34319655804049454, 0.17159827902024727, 0.2573974185303709, 0.08579913951012363, 0.08579913951012363, 0.25623413454187793, 0.25623413454187793, 0.19217560090640842, 0.06405853363546948, 0.12811706727093897, 0.06405853363546948, 0.06405853363546948, 0.2637168885710879, 0.2197640738092399, 0.2197640738092399, 0.08790562952369596, 0.08790562952369596, 0.04395281476184798, 0.04395281476184798, 0.04395281476184798, 0.39407077362091963, 0.14777654010784486, 0.14777654010784486, 0.09851769340522991, 0.049258846702614954, 0.049258846702614954, 0.049258846702614954, 0.049258846702614954, 0.2691584880481769, 0.2691584880481769, 0.13457924402408844, 0.13457924402408844, 0.13457924402408844, 0.38013525956146305, 0.1520541038245852, 0.17739645446201607, 0.10136940254972347, 0.0760270519122926, 0.050684701274861736, 0.025342350637430868, 0.025342350637430868, 0.3271655974477598, 0.2025310841343275, 0.1713724558059694, 0.10905519914925327, 0.0778965708208952, 0.04673794249253711, 0.031158628328358078, 0.031158628328358078, 0.27192054942666377, 0.19034438459866465, 0.16315232965599827, 0.10876821977066552, 0.10876821977066552, 0.05438410988533276, 0.05438410988533276, 0.02719205494266638, 0.31929553366511537, 0.21286368911007691, 0.21286368911007691, 0.10643184455503846, 0.10643184455503846, 0.10643184455503846, 0.3361686835272076, 0.19209639058697575, 0.19209639058697575, 0.0720361464701159, 0.09604819529348788, 0.04802409764674394, 0.02401204882337197, 0.02401204882337197, 0.3143104116400752, 0.18858624698404514, 0.18858624698404514, 0.09429312349202257, 0.06286208232801505, 0.03143104116400752, 0.03143104116400752, 0.06286208232801505, 0.31548102407008183, 0.19717564004380114, 0.15774051203504091, 0.11830538402628069, 0.07887025601752046, 0.03943512800876023, 0.03943512800876023, 0.03943512800876023, 0.34079399840042424, 0.17039699920021212, 0.17039699920021212, 0.17039699920021212, 0.17039699920021212, 0.3021558208258155, 0.1812934924954893, 0.15107791041290775, 0.12086232833032619, 0.12086232833032619, 0.030215582082581547, 0.030215582082581547, 0.060431164165163094, 0.2941314915142516, 0.2941314915142516, 0.1470657457571258, 0.09804383050475055, 0.0735328728785629, 0.024510957626187636, 0.024510957626187636, 0.024510957626187636, 0.26584186746732164, 0.2126734939738573, 0.15950512048039298, 0.10633674698692865, 0.10633674698692865, 0.05316837349346432, 0.05316837349346432, 0.05316837349346432, 0.2968064894809445, 0.23744519158475558, 0.11872259579237779, 0.11872259579237779, 0.11872259579237779, 0.059361297896188894, 0.059361297896188894, 0.2851742191022665, 0.19011614606817764, 0.19011614606817764, 0.09505807303408882, 0.09505807303408882, 0.09505807303408882, 0.09505807303408882, 0.33306167057686925, 0.16653083528843463, 0.13322466823074772, 0.09991850117306078, 0.13322466823074772, 0.03330616705768693, 0.03330616705768693, 0.03330616705768693, 0.30103502602579935, 0.1806210156154796, 0.1806210156154796, 0.06020700520515987, 0.12041401041031974, 0.06020700520515987, 0.06020700520515987, 0.06020700520515987, 0.28129075290342287, 0.2344089607528524, 0.21096806467756715, 0.1172044803764262, 0.07032268822585572, 0.02344089607528524, 0.02344089607528524, 0.02344089607528524, 0.27609642825899655, 0.27609642825899655, 0.1840642855059977, 0.07362571420239908, 0.07362571420239908, 0.05521928565179931, 0.03681285710119954, 0.01840642855059977, 0.40152032878820343, 0.20076016439410171, 0.15057012329557629, 0.05019004109852543, 0.10038008219705086, 0.05019004109852543, 0.05019004109852543, 0.3669877090220267, 0.1957267781450809, 0.1712609308769458, 0.07339754180440534, 0.09786338907254045, 0.024465847268135114, 0.024465847268135114, 0.024465847268135114, 0.3282806254307231, 0.1969683752584339, 0.15319762520100413, 0.0875415001148595, 0.10942687514357438, 0.04377075005742975, 0.04377075005742975, 0.021885375028714876, 0.29502170688500234, 0.25287574875857344, 0.21072979063214453, 0.08429191625285781, 0.08429191625285781, 0.042145958126428906, 0.042145958126428906, 0.042145958126428906, 0.2606937967539435, 0.2606937967539435, 0.173795864502629, 0.0868979322513145, 0.0868979322513145, 0.0868979322513145, 0.2834287833991457, 0.1803637712540018, 0.1803637712540018, 0.12883126518142984, 0.12883126518142984, 0.05153250607257194, 0.05153250607257194, 0.02576625303628597, 0.2790021693276766, 0.1594298110443866, 0.23914471656657993, 0.0797149055221933, 0.11957235828328996, 0.03985745276109665, 0.03985745276109665, 0.03985745276109665, 0.29384915733292505, 0.18365572333307817, 0.2203868679996938, 0.07346228933323126, 0.1101934339998469, 0.03673114466661563, 0.03673114466661563, 0.03673114466661563, 0.25963957158571543, 0.3894593573785732, 0.12981978579285772, 0.06490989289642886, 0.06490989289642886, 0.021636630965476286, 0.021636630965476286, 0.021636630965476286, 0.25170100436991405, 0.25170100436991405, 0.12585050218495702, 0.12585050218495702, 0.12585050218495702, 0.3100551340411852, 0.1328807717319365, 0.1328807717319365, 0.1328807717319365, 0.1328807717319365, 0.044293590577312164, 0.044293590577312164, 0.044293590577312164, 0.3016979985380317, 0.2514149987816931, 0.15084899926901585, 0.10056599951267724, 0.07542449963450792, 0.05028299975633862, 0.05028299975633862, 0.05028299975633862, 0.40210311928992637, 0.18277414513178472, 0.10966448707907082, 0.10966448707907082, 0.10966448707907082, 0.03655482902635694, 0.03655482902635694, 0.03655482902635694, 0.30551215152321926, 0.17457837229898243, 0.19640066883635524, 0.13093377922423682, 0.08728918614949122, 0.021822296537372804, 0.04364459307474561, 0.04364459307474561, 0.3193005261508179, 0.17193105254274807, 0.17193105254274807, 0.12280789467339148, 0.09824631573871319, 0.049123157869356594, 0.049123157869356594, 0.049123157869356594, 0.25330053008110875, 0.1899753975608316, 0.1899753975608316, 0.12665026504055438, 0.12665026504055438, 0.031662566260138594, 0.031662566260138594, 0.031662566260138594, 0.31731581669815884, 0.21154387779877257, 0.1410292518658484, 0.10577193889938628, 0.1410292518658484, 0.0352573129664621, 0.0352573129664621, 0.0352573129664621, 0.314414716731137, 0.19650919795696065, 0.1572073583655685, 0.07860367918278426, 0.07860367918278426, 0.03930183959139213, 0.03930183959139213, 0.03930183959139213, 0.2966133996293118, 0.23069931082279807, 0.0988711332097706, 0.0988711332097706, 0.13182817761302748, 0.03295704440325687, 0.03295704440325687, 0.03295704440325687, 0.2816665968259008, 0.18777773121726718, 0.18777773121726718, 0.09388886560863359, 0.09388886560863359, 0.21724065544095048, 0.21724065544095048, 0.21724065544095048, 0.16293049158071288, 0.10862032772047524, 0.05431016386023762, 0.05431016386023762, 0.05431016386023762, 0.26846662080087524, 0.1789777472005835, 0.1789777472005835, 0.1789777472005835, 0.08948887360029174, 0.08948887360029174, 0.3087204763568711, 0.21224532749534888, 0.17365526795074, 0.07718011908921778, 0.09647514886152221, 0.03859005954460889, 0.03859005954460889, 0.03859005954460889, 0.3084986171403622, 0.19631730181659415, 0.1402266441547101, 0.11218131532376809, 0.11218131532376809, 0.02804532883094202, 0.02804532883094202, 0.02804532883094202, 0.19734942854058124, 0.19734942854058124, 0.19734942854058124, 0.19734942854058124, 0.32162643725220785, 0.21441762483480523, 0.21441762483480523, 0.10720881241740261, 0.10720881241740261, 0.3820515843724397, 0.19102579218621984, 0.16980070416552875, 0.06367526406207327, 0.08490035208276438, 0.021225088020691094, 0.021225088020691094, 0.04245017604138219, 0.29073074684721867, 0.1938204978981458, 0.1938204978981458, 0.0969102489490729, 0.0969102489490729, 0.04845512447453645, 0.024227562237268224, 0.04845512447453645, 0.23107854140924358, 0.23107854140924358, 0.11553927070462179, 0.11553927070462179, 0.11553927070462179, 0.2832497737546358, 0.18883318250309053, 0.09441659125154526, 0.09441659125154526, 0.09441659125154526, 0.2731118739623507, 0.23409589196772915, 0.15606392797848612, 0.11704794598386457, 0.07803196398924306, 0.03901598199462153, 0.03901598199462153, 0.03901598199462153, 0.4499658807366704, 0.1636239566315165, 0.12271796747363738, 0.12271796747363738, 0.08181197831575825, 0.04090598915787912, 0.04090598915787912, 0.04090598915787912, 0.3408946042628519, 0.20832448038285395, 0.15150871300571198, 0.09469294562856997, 0.09469294562856997, 0.037877178251427994, 0.037877178251427994, 0.018938589125713997, 0.35377310617419855, 0.1664814617290346, 0.1664814617290346, 0.12486109629677596, 0.0832407308645173, 0.04162036543225865, 0.04162036543225865, 0.04162036543225865, 0.33289681034569013, 0.23158038980569748, 0.1736852923542731, 0.05789509745142437, 0.08684264617713655, 0.043421323088568275, 0.043421323088568275, 0.028947548725712185, 0.2138863966518358, 0.2376515518353731, 0.19012124146829848, 0.09506062073414924, 0.11882577591768655, 0.04753031036707462, 0.04753031036707462, 0.04753031036707462, 0.27462916231651735, 0.23237852196013006, 0.16900256142554915, 0.1056266008909682, 0.1056266008909682, 0.06337596053458093, 0.04225064035638729, 0.04225064035638729, 0.30060308377917916, 0.30060308377917916, 0.1639653184250068, 0.0819826592125034, 0.0819826592125034, 0.027327553070834468, 0.027327553070834468, 0.027327553070834468, 0.20338210017781522, 0.2711761335704203, 0.13558806678521015, 0.13558806678521015, 0.06779403339260508, 0.06779403339260508, 0.06779403339260508, 0.22364303139928965, 0.22364303139928965, 0.22364303139928965, 0.11182151569964482, 0.11182151569964482, 0.32182196471050345, 0.2633088802176846, 0.11702616898563761, 0.08776962673922821, 0.08776962673922821, 0.029256542246409403, 0.029256542246409403, 0.029256542246409403, 0.31897080336358613, 0.30125020317672024, 0.15948540168179307, 0.053161800560597695, 0.07088240074746359, 0.035441200373731795, 0.035441200373731795, 0.035441200373731795, 0.28507033424116623, 0.22805626739293297, 0.17104220054469974, 0.11402813369646649, 0.11402813369646649, 0.05701406684823324, 0.05701406684823324, 0.05701406684823324, 0.28668429701644305, 0.20477449786888788, 0.1638195982951103, 0.08190979914755515, 0.08190979914755515, 0.04095489957377758, 0.04095489957377758, 0.04095489957377758, 0.23879135693097361, 0.23879135693097361, 0.1591942379539824, 0.0795971189769912, 0.1591942379539824, 0.0795971189769912, 0.0795971189769912, 0.2551434171367802, 0.2551434171367802, 0.17860039199574612, 0.10205736685471208, 0.10205736685471208, 0.02551434171367802, 0.05102868342735604, 0.05102868342735604, 0.3610640217172183, 0.1969440118457554, 0.16412000987146286, 0.0984720059228777, 0.06564800394858514, 0.03282400197429257, 0.06564800394858514, 0.03282400197429257, 0.34291111513143363, 0.17145555756571682, 0.17145555756571682, 0.17145555756571682, 0.17145555756571682, 0.3421434276908911, 0.2280956184605941, 0.2280956184605941, 0.11404780923029705, 0.11404780923029705, 0.2973673920222021, 0.22302554401665156, 0.11151277200832578, 0.11151277200832578, 0.11151277200832578, 0.03717092400277526, 0.03717092400277526, 0.03717092400277526, 0.2760847571009744, 0.2760847571009744, 0.1380423785504872, 0.09202825236699146, 0.09202825236699146, 0.04601412618349573, 0.04601412618349573, 0.04601412618349573, 0.35435875410836576, 0.20249071663335189, 0.1518680374750139, 0.12655669789584492, 0.07593401873750695, 0.05062267915833797, 0.025311339579168986, 0.025311339579168986, 0.3241557993897809, 0.21610386625985392, 0.14406924417323594, 0.10805193312992696, 0.07203462208661797, 0.036017311043308985, 0.036017311043308985, 0.036017311043308985, 0.39595622072094006, 0.19797811036047003, 0.1484835827703525, 0.09898905518023501, 0.09898905518023501, 0.04949452759011751, 0.04949452759011751, 0.04949452759011751, 0.242608683900382, 0.1940869471203056, 0.1940869471203056, 0.1455652103402292, 0.1455652103402292, 0.0485217367800764, 0.0485217367800764, 0.0485217367800764, 0.24084872189585144, 0.24084872189585144, 0.1605658145972343, 0.08028290729861715, 0.08028290729861715, 0.3087282951007749, 0.23154622132558117, 0.23154622132558117, 0.07718207377519372, 0.07718207377519372, 0.07718207377519372, 0.40695301314019117, 0.18086800584008497, 0.13565100438006372, 0.09043400292004249, 0.09043400292004249, 0.04521700146002124, 0.04521700146002124, 0.04521700146002124, 0.3146555940065144, 0.26221299500542866, 0.1573277970032572, 0.0786638985016286, 0.0786638985016286, 0.026221299500542868, 0.026221299500542868, 0.026221299500542868, 0.3634946503374595, 0.18174732516872974, 0.1557834215731969, 0.10385561438213127, 0.07789171078659846, 0.051927807191065635, 0.025963903595532817, 0.025963903595532817, 0.2688404484708524, 0.1344202242354262, 0.1344202242354262, 0.1344202242354262, 0.1344202242354262, 0.2690911496771537, 0.2276925112652839, 0.1655945536474792, 0.1034965960296745, 0.14489523444154428, 0.0413986384118698, 0.0206993192059349, 0.0206993192059349, 0.31344829969880444, 0.15672414984940222, 0.15672414984940222, 0.10448276656626815, 0.10448276656626815, 0.052241383283134074, 0.052241383283134074, 0.052241383283134074, 0.4381712836539889, 0.19916876529726768, 0.11950125917836062, 0.07966750611890708, 0.07966750611890708, 0.03983375305945354, 0.03983375305945354, 0.03983375305945354, 0.27648367707657145, 0.2246429876247143, 0.19008252799014286, 0.10368137890371429, 0.08640114908642857, 0.03456045963457143, 0.03456045963457143, 0.03456045963457143, 0.40993313161827094, 0.20496656580913547, 0.1639732526473084, 0.0819866263236542, 0.0819866263236542, 0.0409933131618271, 0.0409933131618271, 0.0409933131618271, 0.25855164037532746, 0.25855164037532746, 0.17236776025021833, 0.08618388012510916, 0.08618388012510916, 0.24951310915559494, 0.24951310915559494, 0.24951310915559494, 0.08317103638519831, 0.08317103638519831, 0.08317103638519831, 0.40401499772202937, 0.19452573964394007, 0.14963518434149237, 0.08978111060489541, 0.07481759217074618, 0.029927036868298472, 0.029927036868298472, 0.029927036868298472, 0.4275346852956439, 0.18322915084099026, 0.14251156176521465, 0.10179397268943903, 0.06107638361366342, 0.020358794537887807, 0.04071758907577561, 0.020358794537887807, 0.24571219310190817, 0.24571219310190817, 0.12285609655095409, 0.12285609655095409, 0.12285609655095409, 0.12285609655095409, 0.38393458990457185, 0.19196729495228593, 0.19196729495228593, 0.19196729495228593, 0.19196729495228593, 0.25472940583538217, 0.21833949071604186, 0.18194957559670155, 0.07277983023868062, 0.14555966047736124, 0.03638991511934031, 0.03638991511934031, 0.03638991511934031, 0.27046082170476726, 0.2163686573638138, 0.16227649302286035, 0.1081843286819069, 0.1081843286819069, 0.05409216434095345, 0.05409216434095345, 0.05409216434095345, 0.3121159363708328, 0.3121159363708328, 0.20807729091388852, 0.10403864545694426, 0.10403864545694426, 0.24569890304744257, 0.24569890304744257, 0.16379926869829506, 0.08189963434914753, 0.08189963434914753, 0.08189963434914753, 0.2857090784221592, 0.19999635489551143, 0.17142544705329552, 0.11428363136886367, 0.11428363136886367, 0.028570907842215918, 0.057141815684431836, 0.028570907842215918, 0.411944570014363, 0.2288580944524239, 0.13731485667145432, 0.045771618890484776, 0.09154323778096955, 0.045771618890484776, 0.045771618890484776, 0.045771618890484776, 0.3434569878306166, 0.22897132522041108, 0.11448566261020554, 0.11448566261020554, 0.11448566261020554, 0.3176006054426092, 0.16715821339084694, 0.16715821339084694, 0.10029492803450817, 0.11701074937359286, 0.05014746401725408, 0.05014746401725408, 0.033431642678169386, 0.3147034046102745, 0.15735170230513726, 0.15735170230513726, 0.15735170230513726, 0.15735170230513726, 0.2950365685939423, 0.2496463272717973, 0.18156096528857987, 0.06808536198321745, 0.09078048264428994, 0.04539024132214497, 0.022695120661072484, 0.04539024132214497, 0.2978829677085464, 0.1985886451390309, 0.1985886451390309, 0.09929432256951545, 0.09929432256951545, 0.09929432256951545, 0.34334860762381547, 0.22889907174921031, 0.15259938116614022, 0.07629969058307011, 0.07629969058307011, 0.038149845291535055, 0.038149845291535055, 0.038149845291535055, 0.2767963350917457, 0.2767963350917457, 0.13839816754587286, 0.13839816754587286, 0.13839816754587286, 0.3576123370794154, 0.20434990690252308, 0.1873207479939795, 0.08514579454271795, 0.08514579454271795, 0.03405831781708718, 0.03405831781708718, 0.03405831781708718, 0.2834256147924498, 0.18895040986163322, 0.18895040986163322, 0.09447520493081661, 0.09447520493081661, 0.09447520493081661, 0.09447520493081661, 0.33995229696523943, 0.15108990976232864, 0.1888623872029108, 0.11331743232174649, 0.07554495488116432, 0.03777247744058216, 0.03777247744058216, 0.03777247744058216, 0.29314687258354166, 0.11725874903341667, 0.23451749806683334, 0.11725874903341667, 0.11725874903341667, 0.058629374516708334, 0.058629374516708334, 0.058629374516708334, 0.32001691119819237, 0.22155016929105625, 0.19693348381427223, 0.09846674190713611, 0.07385005643035208, 0.024616685476784028, 0.024616685476784028, 0.049233370953568056, 0.35481830511544266, 0.17740915255772133, 0.17740915255772133, 0.11827276837181422, 0.05913638418590711, 0.05913638418590711, 0.05913638418590711, 0.05913638418590711, 0.337543680735801, 0.1687718403679005, 0.1687718403679005, 0.1687718403679005, 0.1687718403679005, 0.2785235091192684, 0.25067115820734154, 0.1392617545596342, 0.11140940364770736, 0.1392617545596342, 0.02785235091192684, 0.02785235091192684, 0.02785235091192684, 0.23959556740540208, 0.15973037827026806, 0.23959556740540208, 0.07986518913513403, 0.07986518913513403, 0.07986518913513403, 0.264761181233002, 0.2443949365227711, 0.1629299576818474, 0.10183122355115462, 0.12219746826138556, 0.020366244710230925, 0.04073248942046185, 0.04073248942046185, 0.28771767239501356, 0.2637411996954291, 0.19181178159667572, 0.07192941809875339, 0.07192941809875339, 0.04795294539916893, 0.023976472699584465, 0.023976472699584465, 0.26174992086851495, 0.13087496043425748, 0.26174992086851495, 0.13087496043425748, 0.13087496043425748, 0.39510466688767626, 0.26340311125845084, 0.13170155562922542, 0.13170155562922542, 0.13170155562922542, 0.27027913050522484, 0.20270934787891862, 0.20270934787891862, 0.13513956525261242, 0.06756978262630621, 0.06756978262630621, 0.06756978262630621, 0.2228923150054761, 0.27861539375684513, 0.16716923625410707, 0.11144615750273805, 0.055723078751369025, 0.055723078751369025, 0.055723078751369025, 0.055723078751369025, 0.3006788782328961, 0.19134110433002477, 0.19134110433002477, 0.08200333042715348, 0.10933777390287129, 0.054668886951435645, 0.054668886951435645, 0.054668886951435645, 0.38956108312882465, 0.1731382591683665, 0.15149597677232068, 0.08656912958418325, 0.08656912958418325, 0.043284564792091626, 0.043284564792091626, 0.021642282396045813, 0.2932159220130995, 0.2932159220130995, 0.13031818756137756, 0.09773864067103316, 0.06515909378068878, 0.03257954689034439, 0.03257954689034439, 0.03257954689034439, 0.29573572924032543, 0.22748902249255804, 0.15924231574479064, 0.11374451124627902, 0.06824670674776741, 0.04549780449851161, 0.04549780449851161, 0.022748902249255805, 0.16739490200727142, 0.16739490200727142, 0.16739490200727142, 0.16739490200727142, 0.16739490200727142, 0.29952851025321975, 0.20592585079908857, 0.1684847870174361, 0.1123231913449574, 0.09360265945413117, 0.03744106378165247, 0.03744106378165247, 0.03744106378165247, 0.3125846096880948, 0.19891747889242398, 0.17050069619350627, 0.14208391349458854, 0.08525034809675314, 0.02841678269891771, 0.05683356539783542, 0.02841678269891771, 0.33112904920005376, 0.16556452460002688, 0.16556452460002688, 0.16556452460002688, 0.28764796026511097, 0.14382398013255548, 0.14382398013255548, 0.14382398013255548, 0.14382398013255548, 0.26686434538742837, 0.26686434538742837, 0.17790956359161894, 0.08895478179580947, 0.08895478179580947, 0.08895478179580947, 0.33901097563606836, 0.16950548781803418, 0.13560439025442736, 0.10170329269082051, 0.10170329269082051, 0.06780219512721368, 0.03390109756360684, 0.03390109756360684, 0.3737727565510298, 0.12459091885034328, 0.12459091885034328, 0.12459091885034328, 0.12459091885034328, 0.30840204850958075, 0.18504122910574844, 0.2467216388076646, 0.1233608194038323, 0.06168040970191615, 0.06168040970191615, 0.06168040970191615, 0.17391702565513215, 0.17391702565513215, 0.17391702565513215, 0.17391702565513215, 0.17391702565513215, 0.3457160758812053, 0.23047738392080352, 0.15365158928053568, 0.07682579464026784, 0.07682579464026784, 0.03841289732013392, 0.03841289732013392, 0.03841289732013392, 0.3098434953796818, 0.1549217476898409, 0.1549217476898409, 0.07746087384492045, 0.07746087384492045, 0.07746087384492045, 0.07746087384492045, 0.2998106697286862, 0.2748264472512957, 0.12492111238695257, 0.07495266743217155, 0.09993688990956207, 0.04996844495478103, 0.024984222477390516, 0.024984222477390516, 0.33509425533219606, 0.19547164894378102, 0.16754712766609803, 0.08377356383304901, 0.13962260638841503, 0.027924521277683006, 0.027924521277683006, 0.027924521277683006, 0.32349375054907376, 0.2352681822175082, 0.17645113666313114, 0.0735213069429713, 0.0735213069429713, 0.044112784165782785, 0.029408522777188525, 0.029408522777188525, 0.2782494793793943, 0.2504245314414549, 0.13912473968969716, 0.11129979175175773, 0.0834748438138183, 0.05564989587587887, 0.027824947937939434, 0.027824947937939434, 0.3494946889440521, 0.19971125082517266, 0.19971125082517266, 0.09985562541258633, 0.09985562541258633, 0.049927812706293165, 0.049927812706293165, 0.049927812706293165, 0.27336988722282174, 0.2186959097782574, 0.16402193233369305, 0.05467397744456435, 0.1093479548891287, 0.05467397744456435, 0.05467397744456435, 0.05467397744456435, 0.2819674384110013, 0.18797829227400087, 0.18797829227400087, 0.09398914613700043, 0.09398914613700043, 0.3591572928464174, 0.23943819523094492, 0.11971909761547246, 0.11971909761547246, 0.11971909761547246, 0.26894060571052597, 0.20170545428289446, 0.20170545428289446, 0.10085272714144723, 0.06723515142763149, 0.033617575713815746, 0.033617575713815746, 0.033617575713815746, 0.27385445335715486, 0.27385445335715486, 0.09128481778571829, 0.09128481778571829, 0.09128481778571829, 0.09128481778571829, 0.26625345156569086, 0.1863774160959836, 0.21300276125255266, 0.10650138062627633, 0.10650138062627633, 0.053250690313138165, 0.026625345156569082, 0.026625345156569082, 0.3257009103407341, 0.20356306896295884, 0.18320676206666295, 0.10178153448147942, 0.08142522758518353, 0.040712613792591765, 0.040712613792591765, 0.020356306896295882, 0.304327457357514, 0.152163728678757, 0.17752435012521647, 0.10144248578583799, 0.10144248578583799, 0.050721242892918995, 0.050721242892918995, 0.050721242892918995, 0.4345173288521076, 0.1580063014007664, 0.1185047260505748, 0.0790031507003832, 0.1185047260505748, 0.0395015753501916, 0.0395015753501916, 0.0395015753501916, 0.2767394233540547, 0.24906548101864925, 0.1937175963478383, 0.08302182700621641, 0.08302182700621641, 0.05534788467081094, 0.05534788467081094, 0.02767394233540547, 0.3675380510849053, 0.1750181195642406, 0.15751630760781654, 0.10501087173854437, 0.0875090597821203, 0.03500362391284812, 0.03500362391284812, 0.03500362391284812, 0.2651665526756629, 0.22981101231890785, 0.1944554719621528, 0.08838885089188764, 0.08838885089188764, 0.035355540356755054, 0.035355540356755054, 0.035355540356755054, 0.3941864179903865, 0.1854994908190054, 0.16231205446662972, 0.11593718176187838, 0.06956230905712703, 0.023187436352375676, 0.023187436352375676, 0.023187436352375676, 0.35265204827161856, 0.16276248381767008, 0.16276248381767008, 0.13563540318139175, 0.10850832254511339, 0.027127080636278347, 0.027127080636278347, 0.027127080636278347, 0.30618807078408505, 0.15309403539204253, 0.15309403539204253, 0.15309403539204253, 0.15309403539204253, 0.29601454586256104, 0.25901272762974087, 0.14800727293128052, 0.11100545469846039, 0.07400363646564026, 0.03700181823282013, 0.03700181823282013, 0.03700181823282013, 0.3030533097328548, 0.20203553982190323, 0.1515266548664274, 0.1515266548664274, 0.10101776991095161, 0.05050888495547581, 0.05050888495547581, 0.05050888495547581, 0.31085456301210507, 0.15542728150605253, 0.15542728150605253, 0.15542728150605253, 0.15542728150605253, 0.30263232851869354, 0.22697424638902014, 0.15131616425934677, 0.07565808212967338, 0.07565808212967338, 0.07565808212967338, 0.07565808212967338, 0.07565808212967338, 0.27520114821072034, 0.27520114821072034, 0.1834674321404802, 0.0917337160702401, 0.0917337160702401, 0.022933429017560025, 0.04586685803512005, 0.04586685803512005, 0.26009957720208193, 0.26009957720208193, 0.26009957720208193, 0.13004978860104097, 0.13004978860104097, 0.34244821858381935, 0.2282988123892129, 0.11414940619460645, 0.11414940619460645, 0.11414940619460645, 0.2895589421141847, 0.22751059737543083, 0.16546225263667697, 0.08273112631833848, 0.10341390789792311, 0.04136556315916924, 0.04136556315916924, 0.04136556315916924, 0.4053407008618737, 0.18015142260527717, 0.13511356695395788, 0.09007571130263858, 0.09007571130263858, 0.04503785565131929, 0.04503785565131929, 0.04503785565131929, 0.25574880301802916, 0.2045990424144233, 0.1534492818108175, 0.1534492818108175, 0.10229952120721165, 0.05114976060360583, 0.05114976060360583, 0.05114976060360583, 0.26298729558223405, 0.22541768192762918, 0.15027845461841946, 0.11270884096381459, 0.11270884096381459, 0.037569613654604865, 0.037569613654604865, 0.037569613654604865, 0.3744518314122864, 0.20534455270996352, 0.15702818736644267, 0.08455363935116145, 0.08455363935116145, 0.02415818267176041, 0.03623727400764062, 0.02415818267176041, 0.4080041099650495, 0.18133515998446642, 0.11333447499029152, 0.09066757999223321, 0.09066757999223321, 0.022666894998058303, 0.045333789996116605, 0.045333789996116605, 0.36484671707222044, 0.2432311447148136, 0.1216155723574068, 0.1216155723574068, 0.1216155723574068, 0.2924038734887766, 0.19493591565918442, 0.19493591565918442, 0.09746795782959221, 0.09746795782959221, 0.048733978914796104, 0.048733978914796104, 0.048733978914796104, 0.34887660575953966, 0.22752822114752588, 0.16685402884151898, 0.09101128845901035, 0.07584274038250863, 0.03033709615300345, 0.03033709615300345, 0.03033709615300345, 0.1976099403632361, 0.1976099403632361, 0.1976099403632361, 0.3682655630194672, 0.19829684162586694, 0.14164060116133353, 0.08498436069680013, 0.08498436069680013, 0.056656240464533415, 0.056656240464533415, 0.028328120232266708, 0.28565885252666734, 0.19043923501777824, 0.28565885252666734, 0.09521961750888912, 0.09521961750888912, 0.2884122608283254, 0.23597366795044805, 0.18353507507257072, 0.10487718575575469, 0.10487718575575469, 0.052438592877877346, 0.026219296438938673, 0.026219296438938673, 0.36280081818394344, 0.18140040909197172, 0.18140040909197172, 0.09070020454598586, 0.09070020454598586, 0.09070020454598586, 0.327635362185166, 0.280830310444428, 0.093610103481476, 0.093610103481476, 0.093610103481476, 0.046805051740738, 0.046805051740738, 0.046805051740738, 0.3142005253108921, 0.2827804727798029, 0.12568021012435684, 0.06284010506217842, 0.09426015759326763, 0.03142005253108921, 0.03142005253108921, 0.03142005253108921, 0.37373131857639, 0.12457710619213, 0.12457710619213, 0.12457710619213, 0.12457710619213, 0.28732517741810243, 0.23943764784841867, 0.14366258870905121, 0.09577505913936747, 0.09577505913936747, 0.047887529569683736, 0.047887529569683736, 0.047887529569683736, 0.2447911598125682, 0.2447911598125682, 0.2447911598125682, 0.1223955799062841, 0.1223955799062841, 0.34009740715842035, 0.2616133901218618, 0.1308066950609309, 0.07848401703655855, 0.07848401703655855, 0.05232267802437236, 0.02616133901218618, 0.02616133901218618, 0.44475862853453313, 0.22237931426726656, 0.11118965713363328, 0.11118965713363328, 0.11118965713363328, 0.2706168569172279, 0.23678974980257445, 0.23678974980257445, 0.06765421422930698, 0.06765421422930698, 0.03382710711465349, 0.03382710711465349, 0.03382710711465349, 0.29230153469780007, 0.1948676897985334, 0.14615076734890003, 0.0974338448992667, 0.0974338448992667, 0.04871692244963335, 0.04871692244963335, 0.04871692244963335, 0.33645802244267325, 0.2243053482951155, 0.11215267414755775, 0.11215267414755775, 0.11215267414755775, 0.11215267414755775, 0.4075066043521462, 0.18111404637873163, 0.18111404637873163, 0.09055702318936582, 0.09055702318936582, 0.04527851159468291, 0.04527851159468291, 0.04527851159468291, 0.1753148741164124, 0.3155667734095423, 0.1753148741164124, 0.10518892446984743, 0.10518892446984743, 0.035062974823282474, 0.035062974823282474, 0.035062974823282474, 0.34765964435746805, 0.18541847699064962, 0.1390638577429872, 0.11588654811915601, 0.09270923849532481, 0.046354619247662404, 0.023177309623831202, 0.046354619247662404, 0.28368428926197964, 0.14184214463098982, 0.14184214463098982, 0.14184214463098982, 0.14184214463098982, 0.2654483264304667, 0.2654483264304667, 0.13272416321523334, 0.13272416321523334, 0.13272416321523334, 0.24775381073722727, 0.24775381073722727, 0.12387690536861364, 0.12387690536861364, 0.12387690536861364, 0.3229650570790316, 0.2469732789427889, 0.1329856117384248, 0.07599177813624274, 0.07599177813624274, 0.03799588906812137, 0.03799588906812137, 0.03799588906812137, 0.25157193512232673, 0.25157193512232673, 0.20125754809786137, 0.05031438702446534, 0.10062877404893068, 0.05031438702446534, 0.05031438702446534, 0.05031438702446534, 0.2780021440705778, 0.2780021440705778, 0.1390010720352889, 0.1390010720352889, 0.1390010720352889, 0.303816718259686, 0.151908359129843, 0.151908359129843, 0.151908359129843, 0.151908359129843, 0.2706934534090468, 0.21053935265148085, 0.15038525189391488, 0.12030820151513191, 0.09023115113634894, 0.060154100757565956, 0.030077050378782978, 0.030077050378782978, 0.32799610009963986, 0.1967976600597839, 0.13119844003985595, 0.06559922001992798, 0.06559922001992798, 0.06559922001992798, 0.06559922001992798, 0.06559922001992798, 0.33824674760758944, 0.13009490292599593, 0.15611388351119512, 0.15611388351119512, 0.10407592234079675, 0.05203796117039838, 0.02601898058519919, 0.05203796117039838, 0.2560428653636296, 0.2560428653636296, 0.1280214326818148, 0.0640107163409074, 0.1280214326818148, 0.0640107163409074, 0.0640107163409074, 0.0640107163409074, 0.33775977077535846, 0.22517318051690563, 0.22517318051690563, 0.11258659025845281, 0.11258659025845281, 0.4199506867992382, 0.18160029699426516, 0.12485020418355729, 0.09080014849713258, 0.10215016705927416, 0.03405005568642472, 0.022700037124283146, 0.03405005568642472, 0.2949502055018068, 0.20419629611663548, 0.20419629611663548, 0.09075390938517132, 0.09075390938517132, 0.02268847734629283, 0.04537695469258566, 0.04537695469258566, 0.3069838842904854, 0.1534919421452427, 0.1534919421452427, 0.1534919421452427, 0.1534919421452427, 0.4199792908978631, 0.20998964544893156, 0.13999309696595438, 0.06999654848297719, 0.06999654848297719, 0.06999654848297719, 0.27847883152677727, 0.20885912364508294, 0.20885912364508294, 0.06961970788169432, 0.06961970788169432, 0.06961970788169432, 0.06961970788169432, 0.06961970788169432, 0.2468337890215083, 0.2468337890215083, 0.12341689451075415, 0.12341689451075415, 0.12341689451075415, 0.2789914414899912, 0.21699334338110426, 0.15499524527221734, 0.0929971471633304, 0.15499524527221734, 0.030999049054443464, 0.030999049054443464, 0.030999049054443464, 0.4126093400386739, 0.16504373601546957, 0.13753644667955797, 0.11002915734364636, 0.08252186800773478, 0.02750728933591159, 0.02750728933591159, 0.02750728933591159, 0.3933735045707637, 0.18029618959493338, 0.1475150642140364, 0.08195281345224244, 0.08195281345224244, 0.032781125380896975, 0.032781125380896975, 0.032781125380896975, 0.3265580596375452, 0.14843548165342962, 0.17812257798411554, 0.08906128899205777, 0.05937419266137185, 0.05937419266137185, 0.05937419266137185, 0.029687096330685926, 0.4086219355392468, 0.2043109677696234, 0.1362073118464156, 0.1362073118464156, 0.0681036559232078, 0.0681036559232078, 0.26702688633568306, 0.13351344316784153, 0.13351344316784153, 0.13351344316784153, 0.13351344316784153, 0.1745491530587141, 0.1745491530587141, 0.1745491530587141, 0.1745491530587141, 0.3914382992668054, 0.17985002939285652, 0.14811178891176421, 0.07405589445588211, 0.08463530794957955, 0.04231765397478977, 0.03173824048109233, 0.04231765397478977, 0.26887679556413224, 0.20912639654988063, 0.17925119704275483, 0.11950079802850322, 0.11950079802850322, 0.05975039901425161, 0.029875199507125805, 0.029875199507125805, 0.2794757424329603, 0.18631716162197354, 0.18631716162197354, 0.09315858081098677, 0.09315858081098677, 0.09315858081098677, 0.2813494358135307, 0.2461807563368394, 0.14067471790676536, 0.10550603843007403, 0.07033735895338268, 0.03516867947669134, 0.03516867947669134, 0.03516867947669134, 0.3926643493688435, 0.1402372676317298, 0.16828472115807577, 0.08414236057903789, 0.08414236057903789, 0.05609490705269193, 0.05609490705269193, 0.028047453526345963, 0.3444813357774209, 0.14763485819032324, 0.14763485819032324, 0.09842323879354883, 0.09842323879354883, 0.04921161939677442, 0.04921161939677442, 0.04921161939677442, 0.3805821516788736, 0.2003063956204598, 0.14021447693432185, 0.0901378780292069, 0.08012255824818391, 0.030045959343068966, 0.04006127912409196, 0.02003063956204598, 0.3072183666512773, 0.23041377498845794, 0.15360918332563864, 0.11520688749422897, 0.11520688749422897, 0.03840229583140966, 0.03840229583140966, 0.03840229583140966, 0.3050975720630098, 0.22882317904725735, 0.1525487860315049, 0.07627439301575245, 0.1525487860315049, 0.07627439301575245, 0.07627439301575245, 0.29556397448971183, 0.18808616558436209, 0.21495561781069952, 0.08060835667901232, 0.08060835667901232, 0.02686945222633744, 0.02686945222633744, 0.02686945222633744, 0.3477671328273117, 0.21168434172097234, 0.19656403159804572, 0.06048124049170638, 0.09072186073755957, 0.045360930368779785, 0.03024062024585319, 0.03024062024585319, 0.27200570526611595, 0.27200570526611595, 0.10880228210644638, 0.10880228210644638, 0.10880228210644638, 0.05440114105322319, 0.05440114105322319, 0.05440114105322319, 0.2587555325539651, 0.17250368836931007, 0.2587555325539651, 0.08625184418465504, 0.08625184418465504, 0.08625184418465504, 0.26606373705962544, 0.19004552647116105, 0.19004552647116105, 0.11402731588269663, 0.07601821058846442, 0.07601821058846442, 0.03800910529423221, 0.03800910529423221, 0.2232946660817093, 0.2232946660817093, 0.2232946660817093, 0.11164733304085465, 0.11164733304085465, 0.3289527045202877, 0.16447635226014384, 0.16447635226014384, 0.16447635226014384, 0.16447635226014384, 0.4617518588752692, 0.2308759294376346, 0.1154379647188173, 0.1154379647188173, 0.1154379647188173, 0.24784171429047797, 0.19827337143238236, 0.14870502857428677, 0.14870502857428677, 0.09913668571619118, 0.04956834285809559, 0.04956834285809559, 0.04956834285809559, 0.23204670700731672, 0.23204670700731672, 0.11602335350365836, 0.11602335350365836, 0.11602335350365836, 0.33940908453154883, 0.22627272302103257, 0.11313636151051629, 0.11313636151051629, 0.11313636151051629, 0.3183528703854407, 0.2728738889018063, 0.13643694445090315, 0.13643694445090315, 0.045478981483634384, 0.045478981483634384, 0.045478981483634384, 0.045478981483634384, 0.3435846683087566, 0.22905644553917107, 0.11452822276958553, 0.11452822276958553, 0.05726411138479277, 0.05726411138479277, 0.05726411138479277, 0.05726411138479277, 0.38948635502492074, 0.19474317751246037, 0.1298287850083069, 0.1298287850083069, 0.06491439250415346, 0.06491439250415346, 0.3380248879521794, 0.20281493277130763, 0.1690124439760897, 0.13520995518087175, 0.06760497759043588, 0.03380248879521794, 0.03380248879521794, 0.03380248879521794, 0.3919672662020891, 0.19598363310104455, 0.19598363310104455, 0.19598363310104455, 0.19598363310104455, 0.3495912336193006, 0.20392821961125868, 0.1456630140080419, 0.08739780840482515, 0.08739780840482515, 0.029132602801608384, 0.029132602801608384, 0.029132602801608384, 0.2734897694933315, 0.2237643568581803, 0.1989016505406047, 0.12431353158787795, 0.09945082527030236, 0.04972541263515118, 0.04972541263515118, 0.02486270631757559, 0.2954573802040119, 0.21666874548294204, 0.15757726944213968, 0.0984857934013373, 0.0984857934013373, 0.03939431736053492, 0.03939431736053492, 0.03939431736053492, 0.3183449952582733, 0.1736427246863309, 0.1736427246863309, 0.11576181645755393, 0.08682136234316545, 0.057880908228776964, 0.028940454114388482, 0.028940454114388482, 0.3616185569095657, 0.20089919828309205, 0.12053951896985522, 0.08035967931323681, 0.16071935862647363, 0.04017983965661841, 0.04017983965661841, 0.04017983965661841, 0.2558617411114094, 0.2193100638097795, 0.18275838650814957, 0.07310335460325983, 0.10965503190488975, 0.07310335460325983, 0.03655167730162991, 0.03655167730162991, 0.2894835164792843, 0.17369010988757058, 0.17369010988757058, 0.11579340659171371, 0.11579340659171371, 0.057896703295856854, 0.057896703295856854, 0.057896703295856854, 0.37759485114821856, 0.16182636477780796, 0.16182636477780796, 0.07192282879013687, 0.1078842431852053, 0.05394212159260265, 0.03596141439506843, 0.03596141439506843, 0.30343965931800154, 0.21674261379857254, 0.13004556827914351, 0.08669704551942901, 0.13004556827914351, 0.04334852275971451, 0.04334852275971451, 0.04334852275971451, 0.27348474502296577, 0.13674237251148288, 0.13674237251148288, 0.13674237251148288, 0.13674237251148288, 0.2702833091407596, 0.2702833091407596, 0.1801888727605064, 0.0900944363802532, 0.0900944363802532, 0.3601481883709127, 0.14092755197122672, 0.21139132795684007, 0.09395170131415115, 0.0782930844284593, 0.046975850657075574, 0.031317233771383714, 0.031317233771383714, 0.27782422952073915, 0.1852161530138261, 0.1852161530138261, 0.09260807650691305, 0.09260807650691305, 0.09260807650691305, 0.3389683690454823, 0.19939315826204837, 0.13957521078343388, 0.09969657913102419, 0.09969657913102419, 0.03987863165240968, 0.03987863165240968, 0.03987863165240968, 0.392607367232638, 0.196303683616319, 0.196303683616319, 0.196303683616319, 0.196303683616319, 0.39946826992725804, 0.13315608997575268, 0.16644511246969085, 0.09986706748181451, 0.09986706748181451, 0.03328902249393817, 0.03328902249393817, 0.03328902249393817, 0.2724076714665648, 0.21187263336288376, 0.21187263336288376, 0.0908025571555216, 0.12107007620736214, 0.030267519051840535, 0.030267519051840535, 0.030267519051840535, 0.3520316425307496, 0.21121898551844975, 0.1548939227135298, 0.09856885990860988, 0.07040632850614992, 0.028162531402459966, 0.028162531402459966, 0.04224379710368995, 0.35977213585664636, 0.17988606792832318, 0.17988606792832318, 0.1079316407569939, 0.07195442717132927, 0.035977213585664636, 0.035977213585664636, 0.035977213585664636, 0.2868642340439856, 0.22949138723518844, 0.17211854042639135, 0.05737284680879711, 0.11474569361759422, 0.05737284680879711, 0.05737284680879711, 0.05737284680879711, 0.324109410928172, 0.21607294061878135, 0.21607294061878135, 0.10803647030939068, 0.10803647030939068, 0.3339735317369134, 0.2146972704023015, 0.11927626133461194, 0.11927626133461194, 0.09542100906768955, 0.047710504533844775, 0.047710504533844775, 0.023855252266922387, 0.31759855957171784, 0.15879927978585892, 0.15879927978585892, 0.15879927978585892, 0.15879927978585892, 0.32911548063527746, 0.25167654401521217, 0.13551813908511426, 0.09679867077508161, 0.09679867077508161, 0.038719468310032645, 0.038719468310032645, 0.038719468310032645, 0.24739489138913043, 0.1855461685418478, 0.1855461685418478, 0.12369744569456521, 0.12369744569456521, 0.06184872284728261, 0.06184872284728261, 0.30674876952143876, 0.20449917968095918, 0.10224958984047959, 0.10224958984047959, 0.10224958984047959, 0.10224958984047959, 0.319987860257684, 0.1919927161546104, 0.1919927161546104, 0.06399757205153679, 0.06399757205153679, 0.06399757205153679, 0.06399757205153679, 0.06399757205153679, 0.40703508927470494, 0.20351754463735247, 0.11629573979277284, 0.08722180484457963, 0.08722180484457963, 0.02907393494819321, 0.02907393494819321, 0.02907393494819321, 0.30172081818246255, 0.2514340151520521, 0.15086040909123127, 0.10057360606082086, 0.10057360606082086, 0.05028680303041043, 0.05028680303041043, 0.05028680303041043, 0.31479161421745705, 0.1678888609159771, 0.23084718375946853, 0.10493053807248569, 0.08394443045798855, 0.041972215228994275, 0.041972215228994275, 0.041972215228994275, 0.32082576452829553, 0.16041288226414777, 0.16041288226414777, 0.09624772935848866, 0.1283303058113182, 0.03208257645282955, 0.03208257645282955, 0.03208257645282955, 0.30308877014027485, 0.16320164546014798, 0.16320164546014798, 0.11657260390010571, 0.11657260390010571, 0.04662904156004228, 0.04662904156004228, 0.04662904156004228, 0.28532402263742057, 0.17119441358245235, 0.17119441358245235, 0.11412960905496823, 0.057064804527484116, 0.057064804527484116, 0.057064804527484116, 0.057064804527484116, 0.3227622787992755, 0.1793123771107086, 0.16138113939963775, 0.0896561885553543, 0.10758742626642517, 0.05379371313321259, 0.03586247542214172, 0.03586247542214172, 0.413193354839656, 0.206596677419828, 0.103298338709914, 0.103298338709914, 0.103298338709914, 0.34012947914426506, 0.17006473957213253, 0.17006473957213253, 0.17006473957213253, 0.43111559369374003, 0.21555779684687001, 0.10777889842343501, 0.10777889842343501, 0.10777889842343501, 0.22470646250158005, 0.19973907777918226, 0.22470646250158005, 0.09986953888959113, 0.09986953888959113, 0.049934769444795565, 0.049934769444795565, 0.049934769444795565, 0.3323330449685413, 0.18990459712488075, 0.18990459712488075, 0.09495229856244038, 0.09495229856244038, 0.04747614928122019, 0.04747614928122019, 0.04747614928122019, 0.2852705737376263, 0.2852705737376263, 0.13424497587653003, 0.08390310992283127, 0.08390310992283127, 0.03356124396913251, 0.03356124396913251, 0.03356124396913251, 0.32652616712386406, 0.24489462534289805, 0.16326308356193203, 0.08163154178096602, 0.08163154178096602, 0.08163154178096602, 0.08163154178096602, 0.3326725410388051, 0.2162371516752233, 0.19960352462328307, 0.06653450820776102, 0.09980176231164153, 0.03326725410388051, 0.016633627051940256, 0.03326725410388051, 0.358928749357737, 0.23928583290515798, 0.11964291645257899, 0.11964291645257899, 0.11964291645257899, 0.29605546370948743, 0.21146818836391962, 0.21146818836391962, 0.06344045650917589, 0.12688091301835178, 0.04229363767278392, 0.02114681883639196, 0.02114681883639196, 0.2474202999529784, 0.2474202999529784, 0.1237101499764892, 0.1237101499764892, 0.1237101499764892, 0.041236716658829733, 0.041236716658829733, 0.041236716658829733, 0.39800067799286665, 0.19900033899643332, 0.13266689266428888, 0.06633344633214444, 0.06633344633214444, 0.06633344633214444, 0.06633344633214444, 0.3302424681311284, 0.2701983830163778, 0.09006612767212592, 0.09006612767212592, 0.09006612767212592, 0.030022042557375308, 0.030022042557375308, 0.030022042557375308, 0.2572728056902197, 0.19295460426766478, 0.16079550355638733, 0.12863640284510985, 0.12863640284510985, 0.032159100711277464, 0.032159100711277464, 0.032159100711277464, 0.2578250221125756, 0.1718833480750504, 0.1718833480750504, 0.0859416740375252, 0.0859416740375252, 0.0859416740375252, 0.0859416740375252, 0.2710081820681811, 0.2710081820681811, 0.1806721213787874, 0.0903360606893937, 0.0903360606893937, 0.2729581841318768, 0.1364790920659384, 0.1364790920659384, 0.1364790920659384, 0.1364790920659384, 0.32215275840326824, 0.18121092660183838, 0.20134547400204264, 0.10067273700102132, 0.08053818960081706, 0.020134547400204265, 0.04026909480040853, 0.04026909480040853, 0.33974565042443616, 0.20907424641503763, 0.1568056848112782, 0.0784028424056391, 0.0784028424056391, 0.026134280801879704, 0.026134280801879704, 0.05226856160375941, 0.2723022049262413, 0.2269185041052011, 0.13615110246312065, 0.09076740164208043, 0.13615110246312065, 0.045383700821040214, 0.045383700821040214, 0.045383700821040214, 0.31555012291118556, 0.24542787337536653, 0.21036674860745702, 0.07012224953581901, 0.07012224953581901, 0.035061124767909506, 0.035061124767909506, 0.035061124767909506, 0.38184549142507695, 0.19092274571253848, 0.16155001560291718, 0.08811819032886391, 0.08811819032886391, 0.044059095164431954, 0.029372730109621305, 0.029372730109621305, 0.3169559115298281, 0.15847795576491405, 0.15847795576491405, 0.15847795576491405, 0.15847795576491405, 0.2678021916950215, 0.2678021916950215, 0.13390109584751075, 0.13390109584751075, 0.13390109584751075, 0.23431382078277263, 0.19526151731897717, 0.19526151731897717, 0.11715691039138632, 0.11715691039138632, 0.039052303463795436, 0.039052303463795436, 0.039052303463795436, 0.3331922856932981, 0.2120314545320988, 0.18174124674179895, 0.07572551947574957, 0.09087062337089948, 0.04543531168544974, 0.03029020779029983, 0.03029020779029983, 0.27404123396359975, 0.18269415597573316, 0.18269415597573316, 0.09134707798786658, 0.09134707798786658, 0.09134707798786658, 0.09134707798786658, 0.2778322506775804, 0.17860644686415883, 0.23814192915221177, 0.09922580381342157, 0.09922580381342157, 0.03969032152536863, 0.03969032152536863, 0.03969032152536863, 0.3162511667417146, 0.20330432147681654, 0.20330432147681654, 0.06776810715893884, 0.09035747621191846, 0.04517873810595923, 0.04517873810595923, 0.022589369052979615, 0.366829992437986, 0.16303555219466043, 0.16303555219466043, 0.10189722012166277, 0.10189722012166277, 0.04075888804866511, 0.04075888804866511, 0.020379444024332553, 0.34025311517438495, 0.17012655758719247, 0.17012655758719247, 0.10632909849199529, 0.08506327879359624, 0.04253163939679812, 0.04253163939679812, 0.02126581969839906, 0.3119714830297931, 0.18718288981787587, 0.21838003812085519, 0.06239429660595862, 0.12478859321191724, 0.03119714830297931, 0.03119714830297931, 0.03119714830297931, 0.29677524107493525, 0.1978501607166235, 0.1978501607166235, 0.09892508035831175, 0.09892508035831175, 0.09892508035831175, 0.40078073925973184, 0.20039036962986592, 0.12023422177791956, 0.08015614785194637, 0.08015614785194637, 0.040078073925973186, 0.040078073925973186, 0.040078073925973186, 0.2923052601069488, 0.19487017340463253, 0.24358771675579066, 0.09743508670231626, 0.0730763150267372, 0.04871754335115813, 0.024358771675579066, 0.024358771675579066, 0.3599086940031979, 0.19195130346837222, 0.14396347760127914, 0.11996956466773263, 0.09597565173418611, 0.047987825867093055, 0.023993912933546527, 0.023993912933546527, 0.37083590890757195, 0.18541795445378598, 0.18541795445378598, 0.3051099585559833, 0.15255497927799164, 0.15255497927799164, 0.15255497927799164, 0.15255497927799164, 0.31258041842435, 0.19235718056883078, 0.21640182813993464, 0.09617859028441539, 0.07213394271331154, 0.048089295142207694, 0.048089295142207694, 0.024044647571103847, 0.239513664540362, 0.2052974267488817, 0.18818930785314156, 0.119756832270181, 0.13686495116592112, 0.03421623779148028, 0.05132435668722043, 0.03421623779148028, 0.34882736350379395, 0.232551575669196, 0.116275787834598, 0.116275787834598, 0.116275787834598, 0.3832937257322287, 0.21464448641004805, 0.15331749029289146, 0.07665874514644573, 0.061326996117156585, 0.030663498058578292, 0.030663498058578292, 0.030663498058578292, 0.4035753253832934, 0.2017876626916467, 0.10089383134582335, 0.10089383134582335, 0.10089383134582335, 0.340571009942209, 0.1702855049711045, 0.1702855049711045, 0.1702855049711045, 0.1702855049711045, 0.31008437519625953, 0.31008437519625953, 0.12920182299844146, 0.07752109379906488, 0.07752109379906488, 0.051680729199376586, 0.025840364599688293, 0.025840364599688293, 0.23123438966167487, 0.3468515844925123, 0.11561719483083743, 0.11561719483083743, 0.11561719483083743, 0.35009243689598807, 0.21005546213759285, 0.1633764705514611, 0.09335798317226349, 0.09335798317226349, 0.04667899158613174, 0.02333949579306587, 0.02333949579306587, 0.30739279655691265, 0.15369639827845633, 0.15369639827845633, 0.15369639827845633, 0.15369639827845633, 0.2459345945282122, 0.2384820310576603, 0.20867177717545277, 0.11924101552883015, 0.08197819817607073, 0.05216794429386319, 0.022357690411655654, 0.029810253882207538, 0.3302443428874158, 0.2101554909283555, 0.12008885195906029, 0.12008885195906029, 0.09006663896929522, 0.06004442597953014, 0.03002221298976507, 0.03002221298976507, 0.31595497448908366, 0.2106366496593891, 0.2106366496593891, 0.10531832482969455, 0.10531832482969455, 0.10531832482969455, 0.31996945357872697, 0.15998472678936349, 0.15998472678936349, 0.15998472678936349, 0.07999236339468174, 0.34326672870769726, 0.17163336435384863, 0.17163336435384863, 0.08581668217692431, 0.17163336435384863, 0.08581668217692431, 0.3041222493320781, 0.15206112466603905, 0.15206112466603905, 0.15206112466603905, 0.15206112466603905, 0.2674233096988528, 0.3056266396558318, 0.11460998987093693, 0.07640665991395795, 0.07640665991395795, 0.038203329956978976, 0.038203329956978976, 0.038203329956978976, 0.29016583767674164, 0.14508291883837082, 0.14508291883837082, 0.14508291883837082, 0.14508291883837082, 0.2817173015800348, 0.2817173015800348, 0.1408586507900174, 0.1408586507900174, 0.1408586507900174, 0.30016464294660244, 0.18009878576796146, 0.21011525006262172, 0.09004939288398073, 0.12006585717864098, 0.030016464294660244, 0.030016464294660244, 0.030016464294660244, 0.3456564789322805, 0.15711658133285478, 0.1256932650662838, 0.09426994879971287, 0.1256932650662838, 0.03142331626657095, 0.03142331626657095, 0.03142331626657095, 0.2916550858632319, 0.2916550858632319, 0.05833101717264638, 0.05833101717264638, 0.05833101717264638, 0.05833101717264638, 0.05833101717264638, 0.05833101717264638, 0.2535102630944281, 0.21729451122379548, 0.1448630074825303, 0.1448630074825303, 0.10864725561189774, 0.03621575187063258, 0.03621575187063258, 0.03621575187063258, 0.3226359291420463, 0.21509061942803087, 0.1935815574852278, 0.08603624777121234, 0.08603624777121234, 0.04301812388560617, 0.021509061942803085, 0.04301812388560617, 0.4173663102776485, 0.1669465241110594, 0.1669465241110594, 0.0834732620555297, 0.0834732620555297, 0.027824420685176564, 0.027824420685176564, 0.027824420685176564, 0.31434787108597256, 0.18860872265158352, 0.14669567317345386, 0.08382609895625934, 0.12573914843438902, 0.04191304947812967, 0.020956524739064834, 0.04191304947812967, 0.2842025530661599, 0.25262449161436434, 0.12631224580718217, 0.09473418435538662, 0.12631224580718217, 0.03157806145179554, 0.03157806145179554, 0.03157806145179554, 0.3379616084918487, 0.18775644916213818, 0.18775644916213818, 0.07510257966485527, 0.1126538694972829, 0.07510257966485527, 0.037551289832427635, 0.037551289832427635, 0.40625363294357525, 0.17410869983296082, 0.11607246655530722, 0.05803623327765361, 0.11607246655530722, 0.05803623327765361, 0.05803623327765361, 0.21123895142215518, 0.21123895142215518, 0.21123895142215518, 0.10561947571107759, 0.10561947571107759, 0.30279578116426736, 0.1816774686985604, 0.12111831246570694, 0.12111831246570694, 0.12111831246570694, 0.06055915623285347, 0.06055915623285347, 0.06055915623285347, 0.3051583848973432, 0.21797027492667373, 0.13078216495600423, 0.13078216495600423, 0.08718810997066949, 0.043594054985334744, 0.043594054985334744, 0.043594054985334744, 0.3943149636410695, 0.17525109495158644, 0.13143832121368984, 0.10953193434474152, 0.08762554747579322, 0.04381277373789661, 0.04381277373789661, 0.021906386868948306, 0.2691576163795769, 0.13457880818978846, 0.13457880818978846, 0.13457880818978846, 0.13457880818978846, 0.2684213688994866, 0.20131602667461493, 0.17894757926632437, 0.08947378963316219, 0.11184223704145274, 0.044736894816581094, 0.044736894816581094, 0.044736894816581094, 0.31242795795188416, 0.2677953925301864, 0.1338976962650932, 0.08926513084339548, 0.08926513084339548, 0.04463256542169774, 0.04463256542169774, 0.04463256542169774, 0.3494991591495322, 0.1747495795747661, 0.26212436936214917, 0.08737478978738306, 0.08737478978738306, 0.37722758066077516, 0.18861379033038758, 0.17146708211853418, 0.08573354105926709, 0.06858683284741367, 0.034293416423706834, 0.034293416423706834, 0.034293416423706834, 0.33703394530488573, 0.16851697265244286, 0.16851697265244286, 0.16851697265244286, 0.16851697265244286, 0.2108296338163918, 0.31624445072458773, 0.1054148169081959, 0.1054148169081959, 0.1054148169081959, 0.28647180444872644, 0.1909812029658176, 0.15915100247151467, 0.0954906014829088, 0.0954906014829088, 0.06366040098860587, 0.031830200494302935, 0.031830200494302935, 0.3236902920068293, 0.23120735143344953, 0.1387244108600697, 0.0924829405733798, 0.0924829405733798, 0.0462414702866899, 0.0462414702866899, 0.0462414702866899, 0.3360243071299182, 0.19601417915911895, 0.14001012797079926, 0.1120081023766394, 0.08400607678247955, 0.0560040511883197, 0.0560040511883197, 0.02800202559415985, 0.3121468032851871, 0.24971744262814968, 0.12485872131407484, 0.12485872131407484, 0.06242936065703742, 0.06242936065703742, 0.06242936065703742, 0.06242936065703742, 0.4009507502743825, 0.1886827060114741, 0.14151202950860559, 0.09434135300573705, 0.07075601475430279, 0.047170676502868526, 0.023585338251434263, 0.047170676502868526, 0.3402885204366676, 0.1701442602183338, 0.1701442602183338, 0.0850721301091669, 0.0850721301091669, 0.0850721301091669, 0.0850721301091669, 0.30685033654263427, 0.22502358013126514, 0.20456689102842285, 0.10228344551421142, 0.06137006730852686, 0.04091337820568457, 0.04091337820568457, 0.04091337820568457, 0.34261313930304677, 0.17130656965152338, 0.17130656965152338, 0.17130656965152338, 0.17130656965152338, 0.24385344964683803, 0.24385344964683803, 0.12192672482341901, 0.12192672482341901, 0.12192672482341901, 0.26253199394653143, 0.13126599697326571, 0.26253199394653143, 0.13126599697326571, 0.13126599697326571, 0.2919609003340301, 0.25952080029691565, 0.19464060022268673, 0.06488020007422891, 0.09732030011134336, 0.03244010003711446, 0.03244010003711446, 0.03244010003711446, 0.25000849241814344, 0.25000849241814344, 0.1666723282787623, 0.08333616413938115, 0.1666723282787623, 0.19886953826574522, 0.19886953826574522, 0.19886953826574522, 0.19886953826574522, 0.19886953826574522, 0.3068719811837058, 0.2045813207891372, 0.1022906603945686, 0.1022906603945686, 0.1022906603945686, 0.39919642357676893, 0.19959821178838447, 0.14969865884128836, 0.07484932942064418, 0.07484932942064418, 0.02494977647354806, 0.02494977647354806, 0.02494977647354806, 0.2802172457471718, 0.24908199621970822, 0.15567624763731763, 0.12454099810985411, 0.12454099810985411, 0.031135249527463528, 0.031135249527463528, 0.031135249527463528, 0.3402129875445046, 0.1701064937722523, 0.1701064937722523, 0.1701064937722523, 0.1701064937722523, 0.3311059277522409, 0.19866355665134455, 0.13244237110089638, 0.13244237110089638, 0.06622118555044819, 0.033110592775224094, 0.033110592775224094, 0.033110592775224094, 0.3789601451840489, 0.1994527079916047, 0.1396168955941233, 0.07978108319664189, 0.07978108319664189, 0.03989054159832094, 0.01994527079916047, 0.03989054159832094, 0.3054253151745084, 0.1527126575872542, 0.22906898638088127, 0.0763563287936271, 0.0763563287936271, 0.0763563287936271, 0.0763563287936271, 0.0763563287936271, 0.3221985821070165, 0.214799054738011, 0.14319936982534068, 0.1073995273690055, 0.1073995273690055, 0.03579984245633517, 0.03579984245633517, 0.03579984245633517, 0.31219519332118456, 0.18731711599271075, 0.18731711599271075, 0.12487807732847384, 0.12487807732847384, 0.06243903866423692, 0.06243903866423692, 0.06243903866423692, 0.37770003867302787, 0.2518000257820186, 0.1259000128910093, 0.1259000128910093, 0.1259000128910093, 0.24910412816541064, 0.24910412816541064, 0.20758677347117552, 0.08303470938847021, 0.08303470938847021, 0.041517354694235106, 0.041517354694235106, 0.041517354694235106, 0.3047537557317931, 0.2176812540941379, 0.13060875245648276, 0.08707250163765516, 0.08707250163765516, 0.04353625081882758, 0.04353625081882758, 0.04353625081882758, 0.3096950226840593, 0.15484751134202965, 0.15484751134202965, 0.15484751134202965, 0.15484751134202965, 0.3328212783008449, 0.1901835876004828, 0.14263769070036209, 0.11886474225030175, 0.11886474225030175, 0.0475458969001207, 0.0475458969001207, 0.02377294845006035, 0.3057640751541303, 0.23520313473394638, 0.1881625077871571, 0.07056094042018392, 0.07056094042018392, 0.047040626946789274, 0.047040626946789274, 0.047040626946789274, 0.2672705918836672, 0.17818039458911147, 0.17818039458911147, 0.08909019729455574, 0.08909019729455574, 0.08909019729455574, 0.3951284544821471, 0.13170948482738237, 0.19756422724107356, 0.13170948482738237, 0.06585474241369119, 0.2661257549476802, 0.17741716996512014, 0.17741716996512014, 0.17741716996512014, 0.08870858498256007, 0.28938364138096806, 0.28938364138096806, 0.19292242758731204, 0.09646121379365602, 0.09646121379365602, 0.3383957039048009, 0.2255971359365339, 0.2255971359365339, 0.11279856796826696, 0.11279856796826696, 0.19208982452095227, 0.19208982452095227, 0.19208982452095227, 0.19208982452095227, 0.3368578696403558, 0.1684289348201779, 0.1684289348201779, 0.1684289348201779, 0.1684289348201779, 0.34848396278031646, 0.21445166940327165, 0.16083875205245374, 0.08041937602622687, 0.08041937602622687, 0.05361291735081791, 0.05361291735081791, 0.026806458675408956, 0.2708865340053361, 0.2708865340053361, 0.16253192040320166, 0.10835461360213443, 0.10835461360213443, 0.054177306801067214, 0.054177306801067214, 0.054177306801067214, 0.37078575037061856, 0.16853897744119026, 0.16853897744119026, 0.1348311819529522, 0.0674155909764761, 0.03370779548823805, 0.03370779548823805, 0.03370779548823805, 0.2526705172141596, 0.2526705172141596, 0.1263352586070798, 0.1263352586070798, 0.1263352586070798, 0.29159289672880107, 0.2527138438316276, 0.17495573803728062, 0.07775810579434694, 0.09719763224293368, 0.03887905289717347, 0.03887905289717347, 0.03887905289717347, 0.41803543579579716, 0.20901771789789858, 0.10450885894894929, 0.052254429474474645, 0.10450885894894929, 0.052254429474474645, 0.052254429474474645, 0.3242257142339344, 0.19453542854036066, 0.19453542854036066, 0.08646019046238251, 0.08646019046238251, 0.04323009523119126, 0.02161504761559563, 0.04323009523119126, 0.34964606405570436, 0.15539825069142416, 0.1942478133642802, 0.15539825069142416, 0.07769912534571208, 0.03884956267285604, 0.03884956267285604, 0.03884956267285604, 0.37838957976910786, 0.23123807652556594, 0.14715150324354195, 0.08408657328202397, 0.06306492996151798, 0.042043286641011986, 0.021021643320505993, 0.042043286641011986, 0.23957339003370282, 0.15971559335580188, 0.15971559335580188, 0.15971559335580188, 0.07985779667790094, 0.31298830731337235, 0.31298830731337235, 0.15649415365668617, 0.19685623327577245, 0.19685623327577245, 0.19685623327577245, 0.19685623327577245, 0.19685623327577245, 0.3235541684643159, 0.1991102575165021, 0.17422147532693935, 0.09955512875825105, 0.09955512875825105, 0.049777564379125526, 0.024888782189562763, 0.024888782189562763, 0.350798819548447, 0.23386587969896466, 0.11693293984948233, 0.11693293984948233, 0.11693293984948233, 0.30649957360463626, 0.22987468020347718, 0.15324978680231813, 0.11493734010173859, 0.07662489340115906, 0.03831244670057953, 0.03831244670057953, 0.03831244670057953, 0.2963506368047658, 0.1481753184023829, 0.22226297760357439, 0.07408765920119145, 0.07408765920119145, 0.07408765920119145, 0.07408765920119145, 0.31874377424407724, 0.15937188712203862, 0.2124958494960515, 0.05312396237401287, 0.10624792474802575, 0.05312396237401287, 0.05312396237401287, 0.05312396237401287], \"Term\": [\"ability\", \"ability\", \"ability\", \"ability\", \"ability\", \"ability\", \"ability\", \"ability\", \"abuse\", \"abuse\", \"abuse\", \"abuse\", \"abuse\", \"abuse\", \"abuse\", \"abuse\", \"achieved\", \"achieved\", \"achieved\", \"achieved\", \"achieved\", \"achieved\", \"achieved\", \"achieved\", \"acid\", \"acid\", \"acid\", \"acid\", \"acid\", \"actions\", \"actions\", \"actions\", \"actions\", \"actions\", \"actions\", \"actions\", \"actions\", \"active\", \"active\", \"active\", \"active\", \"active\", \"active\", \"active\", \"active\", \"acts\", \"acts\", \"acts\", \"acts\", \"acts\", \"additional\", \"additional\", \"additional\", \"additional\", \"additional\", \"additional\", \"additional\", \"additional\", \"addressed\", \"addressed\", \"addressed\", \"addressed\", \"addressed\", \"addressed\", \"addressed\", \"addressed\", \"adequate\", \"adequate\", \"adequate\", \"adequate\", \"adequate\", \"affected\", \"affected\", \"affected\", \"affected\", \"affected\", \"affected\", \"affected\", \"affected\", \"agency\", \"agency\", \"agency\", \"agency\", \"agency\", \"agency\", \"agency\", \"agency\", \"agree\", \"agree\", \"agree\", \"agree\", \"agree\", \"agree\", \"agree\", \"agree\", \"agreed\", \"agreed\", \"agreed\", \"agreed\", \"agreed\", \"agreed\", \"agreed\", \"agreed\", \"agricultural\", \"agricultural\", \"agricultural\", \"agricultural\", \"agricultural\", \"agricultural\", \"agricultural\", \"agricultural\", \"aimed\", \"aimed\", \"aimed\", \"aimed\", \"aimed\", \"aimed\", \"aimed\", \"aimed\", \"aims\", \"aims\", \"aims\", \"aims\", \"aims\", \"alleviation\", \"alleviation\", \"alleviation\", \"alleviation\", \"alleviation\", \"alleviation\", \"alleviation\", \"alliance\", \"alliance\", \"alliance\", \"alliance\", \"alliance\", \"alliance\", \"alliance\", \"allocated\", \"allocated\", \"allocated\", \"allocated\", \"allocated\", \"allocated\", \"allocated\", \"allocated\", \"allocations\", \"allocations\", \"allocations\", \"allocations\", \"allocations\", \"amount\", \"amount\", \"amount\", \"amount\", \"amount\", \"amount\", \"amount\", \"anc\", \"anc\", \"anc\", \"anc\", \"anc\", \"anc\", \"anc\", \"anniversary\", \"anniversary\", \"anniversary\", \"anniversary\", \"anniversary\", \"anniversary\", \"anniversary\", \"anniversary\", \"applies\", \"applies\", \"applies\", \"applies\", \"applies\", \"applies\", \"approach\", \"approach\", \"approach\", \"approach\", \"approach\", \"approach\", \"approach\", \"approach\", \"arise\", \"arise\", \"arise\", \"arise\", \"arise\", \"around\", \"around\", \"around\", \"around\", \"around\", \"around\", \"around\", \"around\", \"asgisa\", \"asgisa\", \"asgisa\", \"asgisa\", \"asgisa\", \"asgisa\", \"asgisa\", \"asgisa\", \"attend\", \"attend\", \"attend\", \"attend\", \"attend\", \"attend\", \"attend\", \"attend\", \"attitudes\", \"attitudes\", \"attitudes\", \"attitudes\", \"attitudes\", \"attracting\", \"attracting\", \"attracting\", \"attracting\", \"attracting\", \"au\", \"au\", \"au\", \"au\", \"au\", \"au\", \"au\", \"au\", \"authorities\", \"authorities\", \"authorities\", \"authorities\", \"authorities\", \"authorities\", \"authorities\", \"authorities\", \"available\", \"available\", \"available\", \"available\", \"available\", \"available\", \"available\", \"available\", \"aware\", \"aware\", \"aware\", \"aware\", \"aware\", \"aware\", \"aware\", \"aware\", \"bank\", \"bank\", \"bank\", \"bank\", \"bank\", \"bank\", \"bank\", \"bank\", \"basis\", \"basis\", \"basis\", \"basis\", \"basis\", \"basis\", \"basis\", \"basis\", \"battle\", \"battle\", \"battle\", \"battle\", \"battle\", \"battle\", \"believe\", \"believe\", \"believe\", \"believe\", \"believe\", \"believe\", \"believe\", \"believe\", \"belongs\", \"belongs\", \"belongs\", \"belongs\", \"belongs\", \"belongs\", \"belongs\", \"belongs\", \"benefit\", \"benefit\", \"benefit\", \"benefit\", \"benefit\", \"benefit\", \"benefit\", \"benefit\", \"benefits\", \"benefits\", \"benefits\", \"benefits\", \"benefits\", \"benefits\", \"benefits\", \"benefits\", \"bill\", \"bill\", \"bill\", \"bill\", \"bill\", \"bill\", \"bill\", \"bill\", \"body\", \"body\", \"body\", \"body\", \"body\", \"bold\", \"bold\", \"bold\", \"bold\", \"bold\", \"bold\", \"born\", \"born\", \"born\", \"born\", \"born\", \"brics\", \"brics\", \"brics\", \"brics\", \"brics\", \"brics\", \"brics\", \"broad\", \"broad\", \"broad\", \"broad\", \"broad\", \"broad\", \"broad\", \"broad\", \"broadband\", \"broadband\", \"broadband\", \"broadband\", \"broadband\", \"broadband\", \"broadband\", \"broadband\", \"broaden\", \"broaden\", \"broaden\", \"broaden\", \"broaden\", \"brought\", \"brought\", \"brought\", \"brought\", \"brought\", \"brought\", \"brought\", \"brought\", \"businesses\", \"businesses\", \"businesses\", \"businesses\", \"businesses\", \"businesses\", \"businesses\", \"businesses\", \"cabinet\", \"cabinet\", \"cabinet\", \"cabinet\", \"cabinet\", \"cabinet\", \"cabinet\", \"cabinet\", \"calendar\", \"calendar\", \"calendar\", \"calendar\", \"calendar\", \"calendar\", \"capital\", \"capital\", \"capital\", \"capital\", \"capital\", \"capital\", \"capital\", \"capital\", \"capture\", \"capture\", \"capture\", \"capture\", \"capture\", \"capture\", \"capture\", \"capture\", \"carry\", \"carry\", \"carry\", \"carry\", \"carry\", \"carry\", \"carry\", \"carry\", \"catalyst\", \"catalyst\", \"catalyst\", \"catalyst\", \"catalyst\", \"cent\", \"cent\", \"cent\", \"cent\", \"cent\", \"cent\", \"cent\", \"cent\", \"centre\", \"centre\", \"centre\", \"centre\", \"centre\", \"centre\", \"centre\", \"centre\", \"centres\", \"centres\", \"centres\", \"centres\", \"centres\", \"centres\", \"centres\", \"centres\", \"certainty\", \"certainty\", \"certainty\", \"certainty\", \"certainty\", \"certainty\", \"certainty\", \"challenging\", \"challenging\", \"challenging\", \"challenging\", \"challenging\", \"challenging\", \"challenging\", \"changes\", \"changes\", \"changes\", \"changes\", \"changes\", \"changes\", \"changes\", \"changes\", \"changing\", \"changing\", \"changing\", \"changing\", \"changing\", \"changing\", \"changing\", \"changing\", \"chief\", \"chief\", \"chief\", \"chief\", \"chief\", \"chief\", \"chief\", \"chief\", \"child\", \"child\", \"child\", \"child\", \"child\", \"child\", \"child\", \"child\", \"childhood\", \"childhood\", \"childhood\", \"childhood\", \"childhood\", \"childhood\", \"childhood\", \"civil\", \"civil\", \"civil\", \"civil\", \"civil\", \"civil\", \"civil\", \"civil\", \"clear\", \"clear\", \"clear\", \"clear\", \"clear\", \"clear\", \"clear\", \"clear\", \"colleges\", \"colleges\", \"colleges\", \"colleges\", \"colleges\", \"colleges\", \"colleges\", \"colleges\", \"comes\", \"comes\", \"comes\", \"comes\", \"comes\", \"comes\", \"coming\", \"coming\", \"coming\", \"coming\", \"coming\", \"coming\", \"coming\", \"coming\", \"commitments\", \"commitments\", \"commitments\", \"commitments\", \"commitments\", \"commitments\", \"commitments\", \"commitments\", \"committee\", \"committee\", \"committee\", \"committee\", \"committee\", \"committee\", \"committee\", \"committee\", \"compatriots\", \"compatriots\", \"compatriots\", \"compatriots\", \"compatriots\", \"compatriots\", \"compatriots\", \"compatriots\", \"compensation\", \"compensation\", \"compensation\", \"compensation\", \"compensation\", \"competitiveness\", \"competitiveness\", \"competitiveness\", \"competitiveness\", \"competitiveness\", \"competitiveness\", \"competitiveness\", \"competitiveness\", \"complete\", \"complete\", \"complete\", \"complete\", \"complete\", \"complete\", \"complete\", \"complete\", \"concerns\", \"concerns\", \"concerns\", \"concerns\", \"concerns\", \"concerns\", \"concerns\", \"concerns\", \"conference\", \"conference\", \"conference\", \"conference\", \"conference\", \"conference\", \"conference\", \"conference\", \"confidence\", \"confidence\", \"confidence\", \"confidence\", \"confidence\", \"confidence\", \"confidence\", \"confidence\", \"confident\", \"confident\", \"confident\", \"confident\", \"confident\", \"confident\", \"confident\", \"confident\", \"conflict\", \"conflict\", \"conflict\", \"conflict\", \"conflict\", \"conflict\", \"conflict\", \"conflict\", \"confront\", \"confront\", \"confront\", \"confront\", \"confront\", \"confront\", \"confront\", \"confront\", \"congratulate\", \"congratulate\", \"congratulate\", \"congratulate\", \"congratulate\", \"congratulate\", \"congratulate\", \"congratulate\", \"consequence\", \"consequence\", \"consequence\", \"consequence\", \"consequence\", \"consultations\", \"consultations\", \"consultations\", \"consultations\", \"consultations\", \"consultations\", \"consultations\", \"consultations\", \"contain\", \"contain\", \"contain\", \"contain\", \"contain\", \"contain\", \"context\", \"context\", \"context\", \"context\", \"context\", \"context\", \"context\", \"context\", \"continued\", \"continued\", \"continued\", \"continued\", \"continued\", \"continued\", \"continued\", \"continued\", \"contractors\", \"contractors\", \"contractors\", \"contractors\", \"controls\", \"controls\", \"controls\", \"controls\", \"controls\", \"cost\", \"cost\", \"cost\", \"cost\", \"cost\", \"cost\", \"cost\", \"cost\", \"could\", \"could\", \"could\", \"could\", \"could\", \"could\", \"could\", \"could\", \"councillors\", \"councillors\", \"councillors\", \"councillors\", \"councillors\", \"councils\", \"councils\", \"councils\", \"councils\", \"councils\", \"courts\", \"courts\", \"courts\", \"courts\", \"courts\", \"courts\", \"courts\", \"courts\", \"covid\", \"covid\", \"covid\", \"covid\", \"covid\", \"covid\", \"covid\", \"covid\", \"crimes\", \"crimes\", \"crimes\", \"crimes\", \"crimes\", \"crimes\", \"crimes\", \"crimes\", \"criminal\", \"criminal\", \"criminal\", \"criminal\", \"criminal\", \"criminal\", \"criminal\", \"criminal\", \"crisis\", \"crisis\", \"crisis\", \"crisis\", \"crisis\", \"crisis\", \"crisis\", \"crisis\", \"cup\", \"cup\", \"cup\", \"cup\", \"cup\", \"cup\", \"cup\", \"cup\", \"current\", \"current\", \"current\", \"current\", \"current\", \"current\", \"current\", \"current\", \"currently\", \"currently\", \"currently\", \"currently\", \"currently\", \"currently\", \"currently\", \"currently\", \"cut\", \"cut\", \"cut\", \"cut\", \"cut\", \"cut\", \"cut\", \"dealt\", \"dealt\", \"dealt\", \"dealt\", \"dealt\", \"debt\", \"debt\", \"debt\", \"debt\", \"debt\", \"debt\", \"debt\", \"debt\", \"decade\", \"decade\", \"decade\", \"decade\", \"decade\", \"decade\", \"decade\", \"decade\", \"decades\", \"decades\", \"decades\", \"decades\", \"decades\", \"decades\", \"decades\", \"decades\", \"decisive\", \"decisive\", \"decisive\", \"decisive\", \"decisive\", \"decisive\", \"decisive\", \"decisive\", \"deeds\", \"deeds\", \"deeds\", \"deeds\", \"deeds\", \"deeds\", \"deeds\", \"defence\", \"defence\", \"defence\", \"defence\", \"defence\", \"defence\", \"defence\", \"defence\", \"demand\", \"demand\", \"demand\", \"demand\", \"demand\", \"demand\", \"demand\", \"demand\", \"deserves\", \"deserves\", \"deserves\", \"deserves\", \"deserves\", \"devastation\", \"devastation\", \"devastation\", \"devastation\", \"devastation\", \"developing\", \"developing\", \"developing\", \"developing\", \"developing\", \"developing\", \"developing\", \"developing\", \"developmental\", \"developmental\", \"developmental\", \"developmental\", \"developmental\", \"developmental\", \"developmental\", \"developmental\", \"difficult\", \"difficult\", \"difficult\", \"difficult\", \"difficult\", \"difficult\", \"difficult\", \"difficult\", \"direct\", \"direct\", \"direct\", \"direct\", \"direct\", \"direct\", \"direct\", \"direct\", \"disabilities\", \"disabilities\", \"disabilities\", \"disabilities\", \"disabilities\", \"disabilities\", \"disabilities\", \"disabilities\", \"disability\", \"disability\", \"disability\", \"disability\", \"disability\", \"disability\", \"disability\", \"disability\", \"discuss\", \"discuss\", \"discuss\", \"discuss\", \"discuss\", \"diversity\", \"diversity\", \"diversity\", \"diversity\", \"diversity\", \"diversity\", \"dr\", \"dr\", \"dr\", \"dr\", \"dr\", \"dr\", \"dr\", \"dr\", \"drive\", \"drive\", \"drive\", \"drive\", \"drive\", \"drive\", \"drive\", \"drive\", \"early\", \"early\", \"early\", \"early\", \"early\", \"early\", \"early\", \"early\", \"earnings\", \"earnings\", \"earnings\", \"earnings\", \"earnings\", \"eastern\", \"eastern\", \"eastern\", \"eastern\", \"eastern\", \"eastern\", \"eastern\", \"eastern\", \"effectiveness\", \"effectiveness\", \"effectiveness\", \"effectiveness\", \"effectiveness\", \"effectiveness\", \"effectiveness\", \"effectiveness\", \"election\", \"election\", \"election\", \"election\", \"election\", \"election\", \"election\", \"election\", \"elections\", \"elections\", \"elections\", \"elections\", \"elections\", \"elections\", \"elections\", \"elections\", \"elements\", \"elements\", \"elements\", \"elements\", \"elements\", \"elements\", \"elements\", \"elements\", \"engagements\", \"engagements\", \"engagements\", \"engagements\", \"engagements\", \"ensured\", \"ensured\", \"ensured\", \"ensured\", \"ensured\", \"ensured\", \"eskom\", \"eskom\", \"eskom\", \"eskom\", \"eskom\", \"eskom\", \"eskom\", \"eskom\", \"establish\", \"establish\", \"establish\", \"establish\", \"establish\", \"establish\", \"establish\", \"establish\", \"evolution\", \"evolution\", \"evolution\", \"evolution\", \"evolution\", \"evolution\", \"evolved\", \"evolved\", \"evolved\", \"evolved\", \"evolved\", \"example\", \"example\", \"example\", \"example\", \"example\", \"example\", \"example\", \"example\", \"exchange\", \"exchange\", \"exchange\", \"exchange\", \"exchange\", \"exchange\", \"exchange\", \"exchange\", \"expectancy\", \"expectancy\", \"expectancy\", \"expectancy\", \"expectancy\", \"expectations\", \"expectations\", \"expectations\", \"expectations\", \"expectations\", \"expectations\", \"extend\", \"extend\", \"extend\", \"extend\", \"extend\", \"extend\", \"extend\", \"extend\", \"extension\", \"extension\", \"extension\", \"extension\", \"extension\", \"extension\", \"extension\", \"extension\", \"eyes\", \"eyes\", \"eyes\", \"eyes\", \"eyes\", \"fact\", \"fact\", \"fact\", \"fact\", \"fact\", \"fact\", \"fact\", \"fact\", \"faith\", \"faith\", \"faith\", \"faith\", \"faith\", \"farmers\", \"farmers\", \"farmers\", \"farmers\", \"farmers\", \"farmers\", \"farmers\", \"farmers\", \"farms\", \"farms\", \"farms\", \"farms\", \"farms\", \"farms\", \"faster\", \"faster\", \"faster\", \"faster\", \"faster\", \"faster\", \"faster\", \"faster\", \"favourable\", \"favourable\", \"favourable\", \"favourable\", \"favourable\", \"fellow\", \"fellow\", \"fellow\", \"fellow\", \"fellow\", \"fellow\", \"fellow\", \"fellow\", \"field\", \"field\", \"field\", \"field\", \"field\", \"field\", \"field\", \"finalised\", \"finalised\", \"finalised\", \"finalised\", \"finalised\", \"finalised\", \"finalised\", \"finalised\", \"financing\", \"financing\", \"financing\", \"financing\", \"financing\", \"financing\", \"financing\", \"financing\", \"find\", \"find\", \"find\", \"find\", \"find\", \"find\", \"find\", \"find\", \"firmly\", \"firmly\", \"firmly\", \"firmly\", \"firmly\", \"firmly\", \"firmly\", \"firmly\", \"focuses\", \"focuses\", \"focuses\", \"focuses\", \"focuses\", \"food\", \"food\", \"food\", \"food\", \"food\", \"food\", \"food\", \"food\", \"formal\", \"formal\", \"formal\", \"formal\", \"formal\", \"formal\", \"former\", \"former\", \"former\", \"former\", \"former\", \"former\", \"former\", \"former\", \"foundation\", \"foundation\", \"foundation\", \"foundation\", \"foundation\", \"foundation\", \"foundation\", \"foundation\", \"frames\", \"frames\", \"frames\", \"frames\", \"frames\", \"frene\", \"frene\", \"frene\", \"frene\", \"frene\", \"fulfil\", \"fulfil\", \"fulfil\", \"fulfil\", \"fulfil\", \"fulfil\", \"fulfil\", \"functioning\", \"functioning\", \"functioning\", \"functioning\", \"functioning\", \"functioning\", \"functioning\", \"functioning\", \"funding\", \"funding\", \"funding\", \"funding\", \"funding\", \"funding\", \"funding\", \"funding\", \"funds\", \"funds\", \"funds\", \"funds\", \"funds\", \"funds\", \"funds\", \"funds\", \"gauteng\", \"gauteng\", \"gauteng\", \"gauteng\", \"gauteng\", \"gauteng\", \"gauteng\", \"gauteng\", \"general\", \"general\", \"general\", \"general\", \"general\", \"general\", \"general\", \"general\", \"george\", \"george\", \"george\", \"george\", \"george\", \"governance\", \"governance\", \"governance\", \"governance\", \"governance\", \"governance\", \"governance\", \"governance\", \"group\", \"group\", \"group\", \"group\", \"group\", \"group\", \"group\", \"group\", \"groupings\", \"groupings\", \"groupings\", \"groupings\", \"grows\", \"grows\", \"grows\", \"grows\", \"grows\", \"guest\", \"guest\", \"guest\", \"guest\", \"guest\", \"guest\", \"hand\", \"hand\", \"hand\", \"hand\", \"hand\", \"hand\", \"hand\", \"hand\", \"hardship\", \"hardship\", \"hardship\", \"hardship\", \"hardship\", \"head\", \"head\", \"head\", \"head\", \"head\", \"head\", \"head\", \"healthier\", \"healthier\", \"healthier\", \"healthier\", \"healthier\", \"historic\", \"historic\", \"historic\", \"historic\", \"historic\", \"historic\", \"historic\", \"historic\", \"historical\", \"historical\", \"historical\", \"historical\", \"historical\", \"historical\", \"historical\", \"host\", \"host\", \"host\", \"host\", \"host\", \"host\", \"host\", \"host\", \"house\", \"house\", \"house\", \"house\", \"house\", \"house\", \"house\", \"house\", \"households\", \"households\", \"households\", \"households\", \"households\", \"households\", \"households\", \"households\", \"houses\", \"houses\", \"houses\", \"houses\", \"houses\", \"houses\", \"houses\", \"houses\", \"huge\", \"huge\", \"huge\", \"huge\", \"huge\", \"huge\", \"huge\", \"huge\", \"hundred\", \"hundred\", \"hundred\", \"hundred\", \"hundred\", \"hundred\", \"hundred\", \"hundred\", \"hydrogen\", \"hydrogen\", \"hydrogen\", \"hydrogen\", \"hydrogen\", \"ideals\", \"ideals\", \"ideals\", \"ideals\", \"ideals\", \"immediate\", \"immediate\", \"immediate\", \"immediate\", \"immediate\", \"immediate\", \"immediate\", \"immediate\", \"impetus\", \"impetus\", \"impetus\", \"impetus\", \"impetus\", \"impetus\", \"implemented\", \"implemented\", \"implemented\", \"implemented\", \"implemented\", \"implemented\", \"implemented\", \"implemented\", \"implementing\", \"implementing\", \"implementing\", \"implementing\", \"implementing\", \"implementing\", \"implementing\", \"implementing\", \"improvement\", \"improvement\", \"improvement\", \"improvement\", \"improvement\", \"improvement\", \"improvement\", \"improvement\", \"improvements\", \"improvements\", \"improvements\", \"improvements\", \"improvements\", \"improvements\", \"improvements\", \"improvements\", \"inclusive\", \"inclusive\", \"inclusive\", \"inclusive\", \"inclusive\", \"inclusive\", \"inclusive\", \"inclusive\", \"increased\", \"increased\", \"increased\", \"increased\", \"increased\", \"increased\", \"increased\", \"increased\", \"indeed\", \"indeed\", \"indeed\", \"indeed\", \"indeed\", \"indeed\", \"indeed\", \"indeed\", \"industries\", \"industries\", \"industries\", \"industries\", \"industries\", \"industries\", \"industries\", \"industries\", \"inequality\", \"inequality\", \"inequality\", \"inequality\", \"inequality\", \"inequality\", \"inequality\", \"inequality\", \"inflows\", \"inflows\", \"inflows\", \"inflows\", \"inflows\", \"informed\", \"informed\", \"informed\", \"informed\", \"informed\", \"informed\", \"informed\", \"informed\", \"institution\", \"institution\", \"institution\", \"institution\", \"institution\", \"institution\", \"institution\", \"institution\", \"integral\", \"integral\", \"integral\", \"integral\", \"integral\", \"intensified\", \"intensified\", \"intensified\", \"intensified\", \"intensified\", \"intensified\", \"intensified\", \"intensified\", \"intensify\", \"intensify\", \"intensify\", \"intensify\", \"intensify\", \"intensify\", \"intensify\", \"intensify\", \"interactions\", \"interactions\", \"interactions\", \"interactions\", \"interactions\", \"intervene\", \"intervene\", \"intervene\", \"intervene\", \"intervene\", \"investments\", \"investments\", \"investments\", \"investments\", \"investments\", \"investments\", \"investments\", \"investments\", \"investors\", \"investors\", \"investors\", \"investors\", \"investors\", \"investors\", \"investors\", \"investors\", \"involvement\", \"involvement\", \"involvement\", \"involvement\", \"involvement\", \"involvement\", \"involvement\", \"involvement\", \"joint\", \"joint\", \"joint\", \"joint\", \"joint\", \"joint\", \"joint\", \"joint\", \"key\", \"key\", \"key\", \"key\", \"key\", \"key\", \"key\", \"key\", \"kwazulunatal\", \"kwazulunatal\", \"kwazulunatal\", \"kwazulunatal\", \"kwazulunatal\", \"kwazulunatal\", \"kwazulunatal\", \"kwazulunatal\", \"languages\", \"languages\", \"languages\", \"languages\", \"languages\", \"largest\", \"largest\", \"largest\", \"largest\", \"largest\", \"largest\", \"largest\", \"largest\", \"law\", \"law\", \"law\", \"law\", \"law\", \"law\", \"law\", \"law\", \"leaks\", \"leaks\", \"leaks\", \"learning\", \"learning\", \"learning\", \"learning\", \"learning\", \"learning\", \"learning\", \"learning\", \"leave\", \"leave\", \"leave\", \"leave\", \"leave\", \"led\", \"led\", \"led\", \"led\", \"led\", \"led\", \"led\", \"led\", \"left\", \"left\", \"left\", \"left\", \"left\", \"left\", \"legal\", \"legal\", \"legal\", \"legal\", \"legal\", \"legal\", \"legal\", \"legal\", \"liberation\", \"liberation\", \"liberation\", \"liberation\", \"liberation\", \"liberation\", \"liberation\", \"liberation\", \"licence\", \"licence\", \"licence\", \"licence\", \"licence\", \"line\", \"line\", \"line\", \"line\", \"line\", \"line\", \"line\", \"line\", \"livestock\", \"livestock\", \"livestock\", \"livestock\", \"livestock\", \"living\", \"living\", \"living\", \"living\", \"living\", \"living\", \"living\", \"living\", \"located\", \"located\", \"located\", \"located\", \"located\", \"look\", \"look\", \"look\", \"look\", \"look\", \"look\", \"look\", \"look\", \"low\", \"low\", \"low\", \"low\", \"low\", \"low\", \"low\", \"low\", \"machel\", \"machel\", \"machel\", \"machel\", \"machel\", \"machel\", \"madiba\", \"madiba\", \"madiba\", \"madiba\", \"madiba\", \"madiba\", \"madiba\", \"madiba\", \"mandate\", \"mandate\", \"mandate\", \"mandate\", \"mandate\", \"mandate\", \"mandate\", \"mandate\", \"manufacturing\", \"manufacturing\", \"manufacturing\", \"manufacturing\", \"manufacturing\", \"manufacturing\", \"manufacturing\", \"manufacturing\", \"maputo\", \"maputo\", \"maputo\", \"maputo\", \"maputo\", \"marks\", \"marks\", \"marks\", \"marks\", \"marks\", \"maths\", \"maths\", \"maths\", \"maths\", \"maths\", \"matters\", \"matters\", \"matters\", \"matters\", \"matters\", \"matters\", \"matters\", \"matters\", \"media\", \"media\", \"media\", \"media\", \"media\", \"media\", \"media\", \"media\", \"medicines\", \"medicines\", \"medicines\", \"medicines\", \"medicines\", \"meerkat\", \"meerkat\", \"meerkat\", \"meerkat\", \"meerkat\", \"meeting\", \"meeting\", \"meeting\", \"meeting\", \"meeting\", \"meeting\", \"meeting\", \"meeting\", \"mere\", \"mere\", \"mere\", \"mere\", \"mere\", \"mere\", \"mere\", \"mere\", \"millions\", \"millions\", \"millions\", \"millions\", \"millions\", \"millions\", \"millions\", \"millions\", \"mind\", \"mind\", \"mind\", \"mind\", \"mind\", \"mind\", \"mind\", \"mind\", \"minimise\", \"minimise\", \"minimise\", \"minimise\", \"minimise\", \"mining\", \"mining\", \"mining\", \"mining\", \"mining\", \"mining\", \"mining\", \"mining\", \"ministers\", \"ministers\", \"ministers\", \"ministers\", \"ministers\", \"ministers\", \"ministers\", \"ministers\", \"miss\", \"miss\", \"miss\", \"miss\", \"miss\", \"mogoeng\", \"mogoeng\", \"mogoeng\", \"mogoeng\", \"mogoeng\", \"mogoeng\", \"moral\", \"moral\", \"moral\", \"moral\", \"moral\", \"moral\", \"moral\", \"moral\", \"morality\", \"morality\", \"morality\", \"morality\", \"morality\", \"movement\", \"movement\", \"movement\", \"movement\", \"movement\", \"movement\", \"movement\", \"movement\", \"municipal\", \"municipal\", \"municipal\", \"municipal\", \"municipal\", \"municipal\", \"municipal\", \"municipal\", \"municipalities\", \"municipalities\", \"municipalities\", \"municipalities\", \"municipalities\", \"municipalities\", \"municipalities\", \"municipalities\", \"municipality\", \"municipality\", \"municipality\", \"municipality\", \"municipality\", \"municipality\", \"municipality\", \"municipality\", \"na\", \"na\", \"na\", \"na\", \"na\", \"na\", \"nationally\", \"nationally\", \"nationally\", \"nationally\", \"nationally\", \"nd\", \"nd\", \"nd\", \"nd\", \"necessary\", \"necessary\", \"necessary\", \"necessary\", \"necessary\", \"necessary\", \"necessary\", \"necessary\", \"negotiations\", \"negotiations\", \"negotiations\", \"negotiations\", \"negotiations\", \"negotiations\", \"negotiations\", \"negotiations\", \"neither\", \"neither\", \"neither\", \"neither\", \"neither\", \"neither\", \"north\", \"north\", \"north\", \"north\", \"north\", \"north\", \"north\", \"north\", \"northern\", \"northern\", \"northern\", \"northern\", \"northern\", \"northern\", \"northern\", \"northern\", \"nuclear\", \"nuclear\", \"nuclear\", \"nuclear\", \"nuclear\", \"nuclear\", \"nuclear\", \"nuclear\", \"number\", \"number\", \"number\", \"number\", \"number\", \"number\", \"number\", \"number\", \"numbers\", \"numbers\", \"numbers\", \"numbers\", \"numbers\", \"numbers\", \"numbers\", \"numbers\", \"nutrition\", \"nutrition\", \"nutrition\", \"nutrition\", \"nutrition\", \"nutrition\", \"nutrition\", \"objective\", \"objective\", \"objective\", \"objective\", \"objective\", \"objective\", \"objective\", \"objective\", \"objectives\", \"objectives\", \"objectives\", \"objectives\", \"objectives\", \"objectives\", \"objectives\", \"objectives\", \"obligations\", \"obligations\", \"obligations\", \"obligations\", \"obligations\", \"obligations\", \"obligations\", \"obligations\", \"obstacles\", \"obstacles\", \"obstacles\", \"obstacles\", \"obstacles\", \"obstacles\", \"occasion\", \"occasion\", \"occasion\", \"occasion\", \"occasion\", \"occasion\", \"occasion\", \"occasion\", \"occupy\", \"occupy\", \"occupy\", \"occupy\", \"occupy\", \"offering\", \"offering\", \"offering\", \"offering\", \"offering\", \"oil\", \"oil\", \"oil\", \"oil\", \"oil\", \"operations\", \"operations\", \"operations\", \"operations\", \"operations\", \"operations\", \"operations\", \"operations\", \"opposition\", \"opposition\", \"opposition\", \"opposition\", \"opposition\", \"orders\", \"orders\", \"orders\", \"orders\", \"orders\", \"ordinary\", \"ordinary\", \"ordinary\", \"ordinary\", \"ordinary\", \"ordinary\", \"ordinary\", \"ordinary\", \"outcomes\", \"outcomes\", \"outcomes\", \"outcomes\", \"outcomes\", \"outcomes\", \"outcomes\", \"outcomes\", \"pain\", \"pain\", \"pain\", \"pain\", \"pain\", \"pain\", \"pandemic\", \"pandemic\", \"pandemic\", \"pandemic\", \"pandemic\", \"pandemic\", \"pandemic\", \"pandemic\", \"parameters\", \"parameters\", \"parameters\", \"parameters\", \"parameters\", \"participate\", \"participate\", \"participate\", \"participate\", \"participate\", \"participate\", \"participate\", \"participate\", \"particularly\", \"particularly\", \"particularly\", \"particularly\", \"particularly\", \"particularly\", \"particularly\", \"particularly\", \"parties\", \"parties\", \"parties\", \"parties\", \"parties\", \"parties\", \"parties\", \"parties\", \"partnerships\", \"partnerships\", \"partnerships\", \"partnerships\", \"partnerships\", \"partnerships\", \"partnerships\", \"partnerships\", \"party\", \"party\", \"party\", \"party\", \"party\", \"party\", \"party\", \"party\", \"path\", \"path\", \"path\", \"path\", \"path\", \"path\", \"path\", \"path\", \"patriotism\", \"patriotism\", \"patriotism\", \"patriotism\", \"patriotism\", \"patriotism\", \"patriotism\", \"patriotism\", \"per\", \"per\", \"per\", \"per\", \"per\", \"per\", \"per\", \"per\", \"percent\", \"percent\", \"percent\", \"percent\", \"percent\", \"percent\", \"percent\", \"percent\", \"petroleum\", \"petroleum\", \"petroleum\", \"petroleum\", \"petroleum\", \"pilot\", \"pilot\", \"pilot\", \"pilot\", \"pilot\", \"plan\", \"plan\", \"plan\", \"plan\", \"plan\", \"plan\", \"plan\", \"plan\", \"plays\", \"plays\", \"plays\", \"plays\", \"plays\", \"plays\", \"pleased\", \"pleased\", \"pleased\", \"pleased\", \"pleased\", \"pleased\", \"pleased\", \"pleased\", \"plumbers\", \"plumbers\", \"plumbers\", \"plumbers\", \"plumbers\", \"position\", \"position\", \"position\", \"position\", \"position\", \"position\", \"position\", \"position\", \"positive\", \"positive\", \"positive\", \"positive\", \"positive\", \"positive\", \"positive\", \"positive\", \"possible\", \"possible\", \"possible\", \"possible\", \"possible\", \"possible\", \"possible\", \"possible\", \"potential\", \"potential\", \"potential\", \"potential\", \"potential\", \"potential\", \"potential\", \"potential\", \"practice\", \"practice\", \"practice\", \"practice\", \"practice\", \"practice\", \"practice\", \"practice\", \"preferential\", \"preferential\", \"preferential\", \"preferential\", \"preferential\", \"present\", \"present\", \"present\", \"present\", \"present\", \"present\", \"present\", \"present\", \"preserve\", \"preserve\", \"preserve\", \"preserve\", \"preserve\", \"presidential\", \"presidential\", \"presidential\", \"presidential\", \"presidential\", \"presidential\", \"presidential\", \"presidential\", \"presidents\", \"presidents\", \"presidents\", \"presidents\", \"presidents\", \"presidents\", \"presidents\", \"pretoria\", \"pretoria\", \"pretoria\", \"pretoria\", \"pretoria\", \"pretoria\", \"prevent\", \"prevent\", \"prevent\", \"prevent\", \"prevent\", \"prevent\", \"prevent\", \"prevent\", \"prevention\", \"prevention\", \"prevention\", \"prevention\", \"prevention\", \"prevention\", \"prevention\", \"prevention\", \"principle\", \"principle\", \"principle\", \"principle\", \"principle\", \"principle\", \"principle\", \"principle\", \"priority\", \"priority\", \"priority\", \"priority\", \"priority\", \"priority\", \"priority\", \"priority\", \"problem\", \"problem\", \"problem\", \"problem\", \"problem\", \"problem\", \"problem\", \"problem\", \"procurement\", \"procurement\", \"procurement\", \"procurement\", \"procurement\", \"procurement\", \"procurement\", \"procurement\", \"producers\", \"producers\", \"producers\", \"producers\", \"producers\", \"producers\", \"producers\", \"producers\", \"project\", \"project\", \"project\", \"project\", \"project\", \"project\", \"project\", \"project\", \"promotes\", \"promotes\", \"promotes\", \"promotes\", \"promotes\", \"prospects\", \"prospects\", \"prospects\", \"prospects\", \"proteas\", \"proteas\", \"proteas\", \"proteas\", \"proteas\", \"proud\", \"proud\", \"proud\", \"proud\", \"proud\", \"proud\", \"proud\", \"proud\", \"pursuit\", \"pursuit\", \"pursuit\", \"pursuit\", \"pursuit\", \"pursuit\", \"pursuit\", \"pursuit\", \"question\", \"question\", \"question\", \"question\", \"question\", \"question\", \"question\", \"question\", \"raising\", \"raising\", \"raising\", \"raising\", \"raising\", \"raising\", \"raising\", \"rand\", \"rand\", \"rand\", \"rand\", \"rand\", \"rand\", \"rand\", \"rand\", \"ranks\", \"ranks\", \"ranks\", \"ranks\", \"ranks\", \"rates\", \"rates\", \"rates\", \"rates\", \"rates\", \"rates\", \"rates\", \"rates\", \"rather\", \"rather\", \"rather\", \"rather\", \"rather\", \"rather\", \"rather\", \"rather\", \"reading\", \"reading\", \"reading\", \"reading\", \"reading\", \"reading\", \"reading\", \"ready\", \"ready\", \"ready\", \"ready\", \"ready\", \"ready\", \"ready\", \"ready\", \"received\", \"received\", \"received\", \"received\", \"received\", \"received\", \"received\", \"received\", \"recently\", \"recently\", \"recently\", \"recently\", \"recently\", \"recently\", \"recently\", \"recession\", \"recession\", \"recession\", \"recession\", \"recession\", \"recognised\", \"recognised\", \"recognised\", \"recognised\", \"recognised\", \"reconstruction\", \"reconstruction\", \"reconstruction\", \"reconstruction\", \"reconstruction\", \"reconstruction\", \"reconstruction\", \"reconstruction\", \"recovery\", \"recovery\", \"recovery\", \"recovery\", \"recovery\", \"recovery\", \"recovery\", \"recovery\", \"redistribution\", \"redistribution\", \"redistribution\", \"redistribution\", \"redistribution\", \"redistribution\", \"redistribution\", \"redistribution\", \"reduction\", \"reduction\", \"reduction\", \"reduction\", \"reduction\", \"reduction\", \"reduction\", \"reduction\", \"reform\", \"reform\", \"reform\", \"reform\", \"reform\", \"reform\", \"reform\", \"reform\", \"refurbishment\", \"refurbishment\", \"refurbishment\", \"refurbishment\", \"refurbishment\", \"refused\", \"refused\", \"refused\", \"refused\", \"refused\", \"related\", \"related\", \"related\", \"related\", \"related\", \"related\", \"related\", \"related\", \"relations\", \"relations\", \"relations\", \"relations\", \"relations\", \"relations\", \"relations\", \"relations\", \"relationship\", \"relationship\", \"relationship\", \"relationship\", \"relationship\", \"relationship\", \"relationship\", \"republic\", \"republic\", \"republic\", \"republic\", \"republic\", \"republic\", \"republic\", \"republic\", \"respond\", \"respond\", \"respond\", \"respond\", \"respond\", \"respond\", \"respond\", \"respond\", \"rest\", \"rest\", \"rest\", \"rest\", \"rest\", \"rest\", \"rest\", \"rest\", \"restructuring\", \"restructuring\", \"restructuring\", \"restructuring\", \"restructuring\", \"restructuring\", \"restructuring\", \"restructuring\", \"results\", \"results\", \"results\", \"results\", \"results\", \"results\", \"results\", \"results\", \"rich\", \"rich\", \"rich\", \"rich\", \"rich\", \"rich\", \"rise\", \"rise\", \"rise\", \"rise\", \"rise\", \"rise\", \"rise\", \"rise\", \"road\", \"road\", \"road\", \"road\", \"road\", \"road\", \"road\", \"road\", \"roads\", \"roads\", \"roads\", \"roads\", \"roads\", \"roads\", \"roads\", \"roads\", \"robotics\", \"robotics\", \"robotics\", \"room\", \"room\", \"room\", \"room\", \"room\", \"sanitation\", \"sanitation\", \"sanitation\", \"sanitation\", \"sanitation\", \"sanitation\", \"sanitation\", \"sanitation\", \"say\", \"say\", \"say\", \"say\", \"say\", \"say\", \"say\", \"say\", \"scaling\", \"scaling\", \"scaling\", \"scaling\", \"scaling\", \"second\", \"second\", \"second\", \"second\", \"second\", \"second\", \"second\", \"second\", \"secured\", \"secured\", \"secured\", \"secured\", \"secured\", \"sefako\", \"sefako\", \"sefako\", \"sefako\", \"sefako\", \"serious\", \"serious\", \"serious\", \"serious\", \"serious\", \"serious\", \"serious\", \"serious\", \"seriously\", \"seriously\", \"seriously\", \"seriously\", \"seriously\", \"serve\", \"serve\", \"serve\", \"serve\", \"serve\", \"serve\", \"serve\", \"serve\", \"setas\", \"setas\", \"setas\", \"setas\", \"setas\", \"shall\", \"shall\", \"shall\", \"shall\", \"shall\", \"shall\", \"shall\", \"shall\", \"shared\", \"shared\", \"shared\", \"shared\", \"shared\", \"shared\", \"shared\", \"shared\", \"shortage\", \"shortage\", \"shortage\", \"shortage\", \"shortage\", \"shortage\", \"shows\", \"shows\", \"shows\", \"shows\", \"shows\", \"slow\", \"slow\", \"slow\", \"slow\", \"slow\", \"slow\", \"smme\", \"smme\", \"smme\", \"smme\", \"smme\", \"solutions\", \"solutions\", \"solutions\", \"solutions\", \"solutions\", \"solutions\", \"solutions\", \"solutions\", \"someone\", \"someone\", \"someone\", \"someone\", \"someone\", \"somewhat\", \"somewhat\", \"somewhat\", \"somewhat\", \"somewhat\", \"speak\", \"speak\", \"speak\", \"speak\", \"speak\", \"speak\", \"speak\", \"speak\", \"specific\", \"specific\", \"specific\", \"specific\", \"specific\", \"specific\", \"specific\", \"specific\", \"speech\", \"speech\", \"speech\", \"speech\", \"speech\", \"speech\", \"speech\", \"speech\", \"spending\", \"spending\", \"spending\", \"spending\", \"spending\", \"spending\", \"spending\", \"spending\", \"spirit\", \"spirit\", \"spirit\", \"spirit\", \"spirit\", \"spirit\", \"spirit\", \"spirit\", \"stakeholders\", \"stakeholders\", \"stakeholders\", \"stakeholders\", \"stakeholders\", \"stakeholders\", \"stakeholders\", \"stakeholders\", \"started\", \"started\", \"started\", \"started\", \"started\", \"started\", \"started\", \"started\", \"starting\", \"starting\", \"starting\", \"starting\", \"starting\", \"starting\", \"starting\", \"starting\", \"stations\", \"stations\", \"stations\", \"stations\", \"stations\", \"stations\", \"stations\", \"stations\", \"story\", \"story\", \"story\", \"story\", \"story\", \"story\", \"story\", \"streets\", \"streets\", \"streets\", \"streets\", \"streets\", \"strengthened\", \"strengthened\", \"strengthened\", \"strengthened\", \"strengthened\", \"strengthened\", \"strengthened\", \"strengthened\", \"structure\", \"structure\", \"structure\", \"structure\", \"structure\", \"structure\", \"structure\", \"structure\", \"structures\", \"structures\", \"structures\", \"structures\", \"structures\", \"structures\", \"structures\", \"structures\", \"struggled\", \"struggled\", \"struggled\", \"struggled\", \"struggled\", \"students\", \"students\", \"students\", \"students\", \"students\", \"students\", \"students\", \"students\", \"sudan\", \"sudan\", \"sudan\", \"sudan\", \"sudan\", \"sudan\", \"sudan\", \"sudan\", \"sugar\", \"sugar\", \"sugar\", \"sugar\", \"sugar\", \"supply\", \"supply\", \"supply\", \"supply\", \"supply\", \"supply\", \"supply\", \"supply\", \"surpassed\", \"surpassed\", \"surpassed\", \"surpassed\", \"surpassed\", \"syndicates\", \"syndicates\", \"syndicates\", \"syndicates\", \"syndicates\", \"systems\", \"systems\", \"systems\", \"systems\", \"systems\", \"systems\", \"systems\", \"systems\", \"teams\", \"teams\", \"teams\", \"teams\", \"teams\", \"teams\", \"teams\", \"teams\", \"technical\", \"technical\", \"technical\", \"technical\", \"technical\", \"technical\", \"technical\", \"technical\", \"technologies\", \"technologies\", \"technologies\", \"technologies\", \"technologies\", \"technologies\", \"technologies\", \"technologies\", \"technology\", \"technology\", \"technology\", \"technology\", \"technology\", \"technology\", \"technology\", \"technology\", \"telescope\", \"telescope\", \"telescope\", \"telescope\", \"telescope\", \"telescope\", \"telescope\", \"terms\", \"terms\", \"terms\", \"terms\", \"terms\", \"terms\", \"terms\", \"terms\", \"terrorism\", \"terrorism\", \"terrorism\", \"terrorism\", \"terrorism\", \"testing\", \"testing\", \"testing\", \"testing\", \"testing\", \"thorough\", \"thorough\", \"thorough\", \"thorough\", \"thorough\", \"thousand\", \"thousand\", \"thousand\", \"thousand\", \"thousand\", \"thousand\", \"thousand\", \"thousand\", \"tide\", \"tide\", \"tide\", \"tide\", \"tide\", \"tone\", \"tone\", \"tone\", \"tone\", \"tone\", \"tons\", \"tons\", \"tons\", \"tons\", \"tons\", \"total\", \"total\", \"total\", \"total\", \"total\", \"total\", \"total\", \"total\", \"town\", \"town\", \"town\", \"town\", \"town\", \"town\", \"town\", \"town\", \"transferred\", \"transferred\", \"transferred\", \"transferred\", \"transferred\", \"transition\", \"transition\", \"transition\", \"transition\", \"transition\", \"transition\", \"transition\", \"transition\", \"transport\", \"transport\", \"transport\", \"transport\", \"transport\", \"transport\", \"transport\", \"transport\", \"trends\", \"trends\", \"trends\", \"trends\", \"trends\", \"trends\", \"trends\", \"trends\", \"turn\", \"turn\", \"turn\", \"turn\", \"turn\", \"turn\", \"turn\", \"turn\", \"turned\", \"turned\", \"turned\", \"turned\", \"turned\", \"turned\", \"turned\", \"turned\", \"twenty\", \"twenty\", \"twenty\", \"twenty\", \"twenty\", \"undertaken\", \"undertaken\", \"undertaken\", \"undertaken\", \"undertaken\", \"undertaken\", \"undertaken\", \"undertaken\", \"unions\", \"unions\", \"unions\", \"unions\", \"unions\", \"unions\", \"unions\", \"unions\", \"upliftment\", \"upliftment\", \"upliftment\", \"upliftment\", \"upliftment\", \"urban\", \"urban\", \"urban\", \"urban\", \"urban\", \"urban\", \"urban\", \"urban\", \"urgent\", \"urgent\", \"urgent\", \"urgent\", \"urgent\", \"urgent\", \"urgent\", \"urgent\", \"utilise\", \"utilise\", \"utilise\", \"utilise\", \"utilise\", \"utilise\", \"vaccine\", \"vaccine\", \"vaccine\", \"vaccine\", \"vaccine\", \"vaccines\", \"vaccines\", \"vaccines\", \"vaccines\", \"vaccines\", \"valued\", \"valued\", \"valued\", \"valued\", \"valued\", \"van\", \"van\", \"van\", \"van\", \"van\", \"victories\", \"victories\", \"victories\", \"victories\", \"vigour\", \"vigour\", \"vigour\", \"vigour\", \"vigour\", \"vision\", \"vision\", \"vision\", \"vision\", \"vision\", \"vision\", \"vision\", \"vision\", \"visit\", \"visit\", \"visit\", \"visit\", \"visit\", \"visit\", \"visit\", \"visit\", \"vital\", \"vital\", \"vital\", \"vital\", \"vital\", \"vital\", \"vital\", \"vital\", \"voters\", \"voters\", \"voters\", \"voters\", \"voters\", \"want\", \"want\", \"want\", \"want\", \"want\", \"want\", \"want\", \"want\", \"weapons\", \"weapons\", \"weapons\", \"weapons\", \"weapons\", \"weapons\", \"weapons\", \"week\", \"week\", \"week\", \"week\", \"week\", \"week\", \"week\", \"week\", \"weeks\", \"weeks\", \"weeks\", \"weeks\", \"weeks\", \"weeks\", \"weeks\", \"weeks\", \"welcome\", \"welcome\", \"welcome\", \"welcome\", \"welcome\", \"welcome\", \"welcome\", \"welcome\", \"welfare\", \"welfare\", \"welfare\", \"welfare\", \"welfare\", \"welllocated\", \"welllocated\", \"welllocated\", \"wetlands\", \"wetlands\", \"wetlands\", \"wetlands\", \"wetlands\", \"white\", \"white\", \"white\", \"white\", \"white\", \"white\", \"white\", \"white\", \"wife\", \"wife\", \"wife\", \"wife\", \"wife\", \"words\", \"words\", \"words\", \"words\", \"words\", \"words\", \"words\", \"words\", \"worst\", \"worst\", \"worst\", \"worst\", \"worst\", \"worst\", \"worst\", \"zimbabwe\", \"zimbabwe\", \"zimbabwe\", \"zimbabwe\", \"zimbabwe\", \"zimbabwe\", \"zimbabwe\", \"zimbabwe\"]}, \"R\": 30, \"lambda.step\": 0.01, \"plot.opts\": {\"xlab\": \"PC1\", \"ylab\": \"PC2\"}, \"topic.order\": [5, 6, 9, 8, 1, 4, 2, 7, 3]};\n",
       "\n",
       "function LDAvis_load_lib(url, callback){\n",
       "  var s = document.createElement('script');\n",
       "  s.src = url;\n",
       "  s.async = true;\n",
       "  s.onreadystatechange = s.onload = callback;\n",
       "  s.onerror = function(){console.warn(\"failed to load library \" + url);};\n",
       "  document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "}\n",
       "\n",
       "if(typeof(LDAvis) !== \"undefined\"){\n",
       "   // already loaded: just create the visualization\n",
       "   !function(LDAvis){\n",
       "       new LDAvis(\"#\" + \"ldavis_el1699455308817122844973370\", ldavis_el1699455308817122844973370_data);\n",
       "   }(LDAvis);\n",
       "}else if(typeof define === \"function\" && define.amd){\n",
       "   // require.js is available: use it to load d3/LDAvis\n",
       "   require.config({paths: {d3: \"https://d3js.org/d3.v5\"}});\n",
       "   require([\"d3\"], function(d3){\n",
       "      window.d3 = d3;\n",
       "      LDAvis_load_lib(\"https://cdn.jsdelivr.net/gh/bmabey/pyLDAvis@3.4.0/pyLDAvis/js/ldavis.v3.0.0.js\", function(){\n",
       "        new LDAvis(\"#\" + \"ldavis_el1699455308817122844973370\", ldavis_el1699455308817122844973370_data);\n",
       "      });\n",
       "    });\n",
       "}else{\n",
       "    // require.js not available: dynamically load d3 & LDAvis\n",
       "    LDAvis_load_lib(\"https://d3js.org/d3.v5.js\", function(){\n",
       "         LDAvis_load_lib(\"https://cdn.jsdelivr.net/gh/bmabey/pyLDAvis@3.4.0/pyLDAvis/js/ldavis.v3.0.0.js\", function(){\n",
       "                 new LDAvis(\"#\" + \"ldavis_el1699455308817122844973370\", ldavis_el1699455308817122844973370_data);\n",
       "            })\n",
       "         });\n",
       "}\n",
       "</script>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| eval: true\n",
    "#| output: true\n",
    "\n",
    "# Enable the automatic display of visualizations in the Jupyter notebook\n",
    "pyLDAvis.enable_notebook()\n",
    "\n",
    "# Display the visualization\n",
    "pyLDAvis.display(vis_data_speeches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "dc73cb60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<link rel=\"stylesheet\" type=\"text/css\" href=\"https://cdn.jsdelivr.net/gh/bmabey/pyLDAvis@3.4.0/pyLDAvis/js/ldavis.v1.0.0.css\">\n",
       "\n",
       "\n",
       "<div id=\"ldavis_el1699455308886566944433272\" style=\"background-color:white;\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "\n",
       "var ldavis_el1699455308886566944433272_data = {\"mdsDat\": {\"x\": [0.0032438668293990626, -0.0032438668293990626], \"y\": [0.0, 0.0], \"topics\": [1, 2], \"cluster\": [1, 1], \"Freq\": [52.74609524192664, 47.25390475807336]}, \"tinfo\": {\"Term\": [\"require\", \"introduction\", \"block\", \"retrenchment\", \"introduce\", \"central\", \"jobs\", \"efforts\", \"laid\", \"built\", \"alive\", \"saw\", \"limiting\", \"leading\", \"plays\", \"put\", \"home\", \"comfort\", \"capacity\", \"femicide\", \"china\", \"intelligence\", \"audits\", \"families\", \"boost\", \"asserted\", \"making\", \"proposals\", \"functional\", \"healthy\", \"saw\", \"functional\", \"removal\", \"audits\", \"tolerated\", \"aid\", \"strikes\", \"nonracial\", \"israelpalestine\", \"upswing\", \"families\", \"counts\", \"break\", \"farmer\", \"relations\", \"peaceloving\", \"expansion\", \"procured\", \"arise\", \"retrenchments\", \"rd\", \"period\", \"depend\", \"taxi\", \"exclusive\", \"institute\", \"directorate\", \"volunteers\", \"unit\", \"census\", \"line\", \"govern\", \"outside\", \"sporting\", \"subject\", \"phased\", \"met\", \"water\", \"cannabis\", \"extension\", \"responses\", \"introducing\", \"highly\", \"positions\", \"proposal\", \"echelon\", \"well\", \"gone\", \"budget\", \"depends\", \"reforms\", \"gang\", \"standards\", \"various\", \"railways\", \"skilled\", \"wish\", \"reverse\", \"market\", \"economic\", \"literacy\", \"phase\", \"allocated\", \"introduce\", \"planning\", \"finally\", \"starts\", \"present\", \"guests\", \"amendments\", \"establishment\", \"maintenance\", \"accepted\", \"dealing\", \"unity\", \"reinforce\", \"laws\", \"greater\", \"lost\", \"intelligence\", \"continued\", \"considering\", \"efforts\", \"retrenchment\", \"comfort\", \"introduction\", \"built\", \"making\", \"laid\", \"guarantee\", \"fighting\", \"leading\", \"red\", \"require\", \"block\", \"virus\", \"central\", \"reducing\", \"jobs\", \"else\", \"hundreds\", \"allowed\", \"informed\", \"alive\", \"permit\", \"opportunity\", \"son\", \"disciplinary\", \"private\", \"pieces\", \"healthy\", \"plays\", \"boost\", \"scale\", \"limiting\", \"home\", \"capacity\", \"proposals\", \"put\", \"introduce\", \"china\", \"femicide\", \"asserted\", \"intelligence\", \"infrastructural\", \"professionals\", \"developing\", \"finally\", \"signs\", \"railways\", \"phase\", \"laws\", \"immediate\", \"greater\", \"presidency\", \"supported\", \"alongside\", \"maintenance\", \"governing\", \"wish\", \"continued\", \"governments\", \"currently\", \"agreement\", \"establishment\", \"reforms\", \"present\", \"planning\", \"bringing\", \"unity\", \"allow\"], \"Freq\": [132.0, 92.0, 129.0, 84.0, 289.0, 99.0, 100.0, 54.0, 63.0, 56.0, 86.0, 30.0, 115.0, 65.0, 77.0, 100.0, 89.0, 38.0, 82.0, 89.0, 85.0, 141.0, 54.0, 73.0, 60.0, 75.0, 33.0, 65.0, 28.0, 52.0, 24.434569516902833, 21.388780055484244, 7.403814891673375, 38.67634328307027, 8.44378480354531, 10.892986691133679, 5.937124806058235, 7.1709971389602725, 5.8580773484005855, 14.394196419873843, 50.69423379447457, 9.445203171890618, 8.2287514161675, 40.896489479200675, 34.709540553362025, 18.979494021186497, 23.699455746913447, 11.712635994404359, 6.924234590296557, 5.729541783454542, 6.919211146739761, 52.15624047219798, 23.563136448894205, 42.59722786326291, 5.692182865017883, 37.686616140973776, 9.21629582932575, 18.612147158220225, 13.886669020564208, 10.337310908387584, 42.671138948852445, 48.33458963359317, 31.943640448486963, 39.73039383847469, 89.2250651486784, 90.25547322431191, 24.01760911682564, 63.69702094132167, 45.65935570501952, 59.18068724004017, 48.72855762208266, 54.818973089556046, 67.26612862495668, 51.65190090724832, 38.52674759254007, 39.54967154985918, 51.54787999688567, 41.56105584973827, 51.27709492859008, 67.99455836054946, 98.29056266133502, 53.930806599774975, 76.36957011741484, 68.95624589501014, 150.3625976348678, 54.490958685357, 99.86879486285048, 72.69642485500222, 72.39467987277087, 64.33514787205904, 72.71041298883172, 102.90955379663592, 61.4175018511247, 130.80621310580236, 75.6957194397478, 98.86722981245357, 61.88511756424745, 75.56439899937331, 60.94359881188125, 58.881095817020885, 75.20816830233993, 77.03511992443472, 61.13599232464764, 65.04874271834267, 66.57586955960882, 64.6431801804111, 69.17961599012726, 66.73073246926451, 62.700803118442046, 64.39373401693976, 62.77367742862144, 12.803380818698026, 36.524286184317596, 55.08137925572149, 25.287178486198904, 60.151123773873394, 36.75171295979643, 21.76357317389369, 40.54967141934293, 9.136189768488814, 10.03269028864224, 40.572602250362316, 19.15288701205306, 81.92865763669651, 79.5436779900238, 10.857448338094962, 61.07188713216195, 22.90451790572658, 61.01327881906941, 8.828960535305692, 24.85486452650243, 12.829202642739917, 13.813657444414702, 52.78451516497428, 7.782769742394969, 12.721422062479014, 5.765716369523855, 4.771292420422303, 30.570157632264632, 19.65864354346063, 31.46067009352331, 46.00192298537699, 36.14083836907791, 16.605731019194515, 66.97101812583747, 51.8476661781535, 47.94232883107495, 38.50530032766737, 57.87643790025955, 159.15603498371024, 49.478661657180545, 51.2387000012174, 43.923938126688775, 77.21625538625362, 40.1150637792319, 45.239095053757794, 72.0361095787734, 102.84021962675877, 56.56166922638571, 136.13308641392294, 101.12474976296447, 74.93382834124068, 52.97758451924354, 70.82876757744026, 61.89666832747823, 60.7761516868374, 62.71618122559235, 71.7363601927234, 56.91486017146944, 80.89863385616084, 63.376838835620866, 58.100633214529346, 55.4310400832922, 59.886914487996705, 67.13953195879371, 73.082422074128, 65.33333030985105, 63.76894469711286, 59.48721678325761, 60.38969640261666, 58.06965060955473], \"Total\": [132.0, 92.0, 129.0, 84.0, 289.0, 99.0, 100.0, 54.0, 63.0, 56.0, 86.0, 30.0, 115.0, 65.0, 77.0, 100.0, 89.0, 38.0, 82.0, 89.0, 85.0, 141.0, 54.0, 73.0, 60.0, 75.0, 33.0, 65.0, 28.0, 52.0, 30.508203895710857, 28.36162600704505, 10.318460186828762, 54.659889597314645, 12.046088665191242, 15.581574461217787, 8.50046125082552, 10.269541359911104, 8.482251948889976, 20.85185651386975, 73.79726107904736, 13.769321614891668, 12.002998431780089, 59.654657387101935, 50.78851203657175, 27.855808006769333, 34.881661456671964, 17.265133415470352, 10.215659265404383, 8.455092839746609, 10.216016488484122, 77.11025142521332, 34.85694479296378, 63.015472356978265, 8.44842664300952, 55.94965968188907, 13.718133065892, 27.771158001536715, 20.743367535508185, 15.465987781721843, 64.55024582065829, 73.29873250172095, 48.69299976410879, 60.903366704845375, 139.2824809110847, 140.96298909640112, 36.46495578242607, 100.68351658136388, 71.22568823854616, 93.67422404670663, 76.4004768299111, 86.70621398205034, 108.9947248983093, 83.04428323686719, 60.64517672041217, 62.37907291529086, 83.02641571051291, 65.8322668143482, 82.95479557165076, 113.65502759358904, 171.37298473546304, 88.03880798363694, 130.50695055439832, 116.8960301743752, 286.49568404879074, 89.67618349203522, 180.76742871901132, 126.73287738061957, 126.64747902498948, 109.8736817093107, 129.75804977992345, 204.0343035596004, 104.71827269362325, 289.9622480895126, 139.46466413686068, 201.70744943921233, 106.33530900448577, 140.89772930922436, 104.64156760585462, 99.67746928970135, 142.34770026113364, 148.77148011715812, 106.19581594537993, 117.55891220248327, 126.96556596222547, 122.02034836418547, 144.11344433136793, 137.55950004670478, 118.5890919762498, 141.60998940319337, 126.1505162642423, 19.160877632664192, 54.91162976373465, 84.39091342269258, 38.778845243125716, 92.59322650419719, 56.76641722696471, 33.973244633814815, 63.38935532134964, 14.395660998008236, 16.06977391233881, 65.30053587045758, 30.850635611641152, 132.69148529239027, 129.51399918935815, 17.761887935502386, 99.96938361116042, 37.510843241672475, 100.01671307615027, 14.478117637512891, 40.80517082756463, 21.066412446075883, 22.711155669590283, 86.90318041031085, 12.849080712060118, 21.090507364407458, 9.558943810238533, 7.911579918385073, 50.74596713048892, 32.63385529802839, 52.43444328716298, 77.22091742846575, 60.73913672555054, 27.711984245720863, 115.62109418295017, 89.06743668677541, 82.45197175125826, 65.84874994608018, 100.86280324535824, 289.9622480895126, 85.87263555109669, 89.2283779292039, 75.88893414841495, 141.60998940319337, 69.2525132302167, 79.3694848621731, 133.40908675987268, 201.70744943921233, 103.12982025153352, 286.49568404879074, 204.0343035596004, 144.11344433136793, 96.42000103116982, 137.55950004670478, 116.9790057069923, 115.3817590639318, 120.59947905399596, 148.77148011715812, 106.85994205688027, 180.76742871901132, 126.1505162642423, 110.36436162740043, 103.4045351763684, 117.50211753027833, 142.34770026113364, 171.37298473546304, 140.89772930922436, 139.46466413686068, 119.54317582727035, 126.96556596222547, 116.07520792796379], \"Category\": [\"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\"], \"logprob\": [30.0, 29.0, 28.0, 27.0, 26.0, 25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, -7.7596, -7.8927, -8.9536, -7.3003, -8.8221, -8.5675, -9.1743, -8.9855, -9.1877, -8.2887, -7.0298, -8.7101, -8.8479, -7.2445, -7.4086, -8.0122, -7.7901, -8.4949, -9.0205, -9.2099, -9.0213, -7.0013, -7.7959, -7.2038, -9.2165, -7.3263, -8.7346, -8.0318, -8.3246, -8.6198, -7.202, -7.0774, -7.4916, -7.2735, -6.4644, -6.4529, -7.7768, -6.8014, -7.1344, -6.875, -7.0693, -6.9515, -6.7469, -7.011, -7.3042, -7.278, -7.0131, -7.2284, -7.0183, -6.7361, -6.3676, -6.9679, -6.62, -6.7221, -5.9425, -6.9575, -6.3517, -6.6693, -6.6734, -6.7915, -6.6691, -6.3217, -6.8379, -6.0819, -6.6288, -6.3618, -6.8303, -6.6306, -6.8456, -6.8801, -6.6353, -6.6113, -6.8425, -6.7804, -6.7572, -6.7867, -6.7189, -6.7549, -6.8172, -6.7906, -6.816, -8.2959, -7.2476, -6.8368, -7.6153, -6.7488, -7.2414, -7.7654, -7.1431, -8.6334, -8.5398, -7.1425, -7.8932, -6.4398, -6.4693, -8.4608, -6.7336, -7.7143, -6.7345, -8.6676, -7.6326, -8.2939, -8.22, -6.8794, -8.7937, -8.3023, -9.0937, -9.283, -7.4256, -7.8671, -7.3969, -7.0169, -7.2582, -8.0359, -6.6414, -6.8973, -6.9756, -7.1948, -6.7873, -5.7757, -6.9441, -6.9091, -7.0632, -6.499, -7.1539, -7.0337, -6.5684, -6.2124, -6.8103, -5.932, -6.2293, -6.529, -6.8757, -6.5854, -6.7202, -6.7384, -6.707, -6.5726, -6.8041, -6.4524, -6.6965, -6.7834, -6.8305, -6.7532, -6.6388, -6.554, -6.6661, -6.6904, -6.7599, -6.7448, -6.784], \"loglift\": [30.0, 29.0, 28.0, 27.0, 26.0, 25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, 0.4177, 0.3575, 0.3077, 0.2938, 0.2844, 0.2817, 0.2808, 0.2805, 0.2695, 0.2691, 0.2642, 0.2627, 0.2622, 0.2622, 0.259, 0.256, 0.2532, 0.2517, 0.2508, 0.2505, 0.25, 0.2487, 0.2481, 0.2481, 0.2448, 0.2445, 0.2419, 0.2395, 0.2384, 0.2368, 0.2258, 0.2233, 0.2181, 0.2125, 0.1943, 0.1938, 0.2221, 0.1818, 0.195, 0.1805, 0.19, 0.1812, 0.157, 0.1648, 0.186, 0.184, 0.163, 0.1797, 0.1586, 0.1259, 0.0838, 0.1496, 0.1038, 0.1119, -0.005, 0.1415, 0.0463, 0.0839, 0.0804, 0.1045, 0.0605, -0.0448, 0.1061, -0.1564, 0.0286, -0.0734, 0.0984, 0.0166, 0.0991, 0.1133, 0.0017, -0.0185, 0.0875, 0.0479, -0.0059, 0.0044, -0.0942, -0.0837, 0.0024, -0.1484, -0.0583, 0.3465, 0.3419, 0.323, 0.3221, 0.3183, 0.3149, 0.3043, 0.3029, 0.295, 0.2785, 0.2737, 0.2729, 0.2675, 0.2622, 0.2574, 0.2568, 0.2563, 0.2554, 0.255, 0.2539, 0.2537, 0.2524, 0.2511, 0.2483, 0.2441, 0.2441, 0.2439, 0.2428, 0.2428, 0.2388, 0.2316, 0.2305, 0.2375, 0.2036, 0.2086, 0.2074, 0.2131, 0.1942, 0.1498, 0.1983, 0.1949, 0.2028, 0.1432, 0.2036, 0.1875, 0.1334, 0.076, 0.149, 0.0055, 0.0477, 0.0956, 0.1508, 0.0858, 0.1131, 0.1086, 0.0958, 0.0202, 0.1197, -0.0544, 0.0613, 0.108, 0.1261, 0.0756, -0.0019, -0.1026, -0.0189, -0.0329, 0.0517, 0.0065, 0.057]}, \"token.table\": {\"Topic\": [1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2], \"Freq\": [0.5744105778270431, 0.42374550823306456, 0.49360812570083573, 0.5106290955525887, 0.705962034027997, 0.3208918336490895, 0.39123999650496083, 0.6098741121989095, 0.5825153378767921, 0.41062556604429606, 0.4996760379356354, 0.4996760379356354, 0.3797514180678727, 0.6170960543602931, 0.4809307673214051, 0.5223903162284228, 0.5919090885877443, 0.41132665478131386, 0.6852225410165839, 0.29366680329282163, 0.4216688554014745, 0.5797946761770274, 0.7135030876812457, 0.29271921545897256, 0.3860586524464946, 0.6176938439143914, 0.4115962351088782, 0.5926985785567846, 0.666500128736047, 0.3332500643680235, 0.5019107078658749, 0.49354552940144364, 0.6147926668802365, 0.38575226157191306, 0.3523209844305581, 0.6517938211965325, 0.6458344052210304, 0.3650368377336259, 0.424489545326924, 0.5821570907340672, 0.6465801047520736, 0.3232900523760368, 0.3901194404848376, 0.6101868171685921, 0.4192255165917082, 0.5706125086942695, 0.33523432475866455, 0.6446813937666626, 0.3131380574014834, 0.6784657910365475, 0.4994034258887731, 0.4994034258887731, 0.653626972462202, 0.29050087664986757, 0.4641962745457, 0.5318915645836146, 0.5529142689585637, 0.45083778853544415, 0.6885285024992964, 0.31557556364551087, 0.5983017332339787, 0.40473352542298563, 0.4572402186501424, 0.5396933728329549, 0.6560659498468562, 0.36448108324825346, 0.37919101253449333, 0.6319850208908223, 0.6412406938833308, 0.3687133989829152, 0.5824870797478395, 0.41866258856875965, 0.3277994129376171, 0.6738099043717685, 0.4144185142172048, 0.6216277713258073, 0.5268789018889255, 0.4706784856874401, 0.7101914064631878, 0.3550957032315939, 0.6880406207087196, 0.31535195115816317, 0.6298424203715014, 0.36296003885815337, 0.691082558543355, 0.3116646832646503, 0.6872891706333846, 0.31849985956181237, 0.4258734819784596, 0.5715670416026695, 0.3733717744089129, 0.6222862906815215, 0.49080983511139575, 0.510640535519937, 0.7404370960530818, 0.2468123653510273, 0.613365869402009, 0.38619332517904265, 0.6379850190855962, 0.36456286804891214, 0.6548544342001142, 0.3410700178125595, 0.4679021814683897, 0.5334084868739642, 0.4711665906749542, 0.5255319665220644, 0.48706196211277214, 0.516140288209057, 0.34732687861236755, 0.6251883815022615, 0.5829423373105803, 0.420482997404353, 0.4005001041966098, 0.5912144395283288, 0.6147086481709105, 0.3853397495996753, 0.4154155702281898, 0.5838272878882668, 0.39210716866284284, 0.612667451035692, 0.44596556254027975, 0.5496784840612751, 0.3962810229006001, 0.6164371467342667, 0.41875736557884996, 0.5775963663156551, 0.6791819685062467, 0.32171777455559053, 0.4519455178954824, 0.5437469512180022, 0.4517829505845179, 0.5483472453659416, 0.634325932065099, 0.3690623604742394, 0.3455976339538126, 0.6479955636633986, 0.7073593234618769, 0.35367966173093845, 0.38993482989494327, 0.6098980672715779, 0.36283694452171783, 0.6467962924082796, 0.47878947255846943, 0.5204233397374668, 0.38284525029924255, 0.6278662104907579, 0.42379809970026805, 0.57947903428404, 0.6661477342699527, 0.34081977102183625, 0.5625855206964954, 0.4392791052013731, 0.5312461622744965, 0.4722188109106636, 0.5175723192332442, 0.4839637270752413, 0.35321913256574733, 0.6475684097038702, 0.5685071708833092, 0.4263803781624818, 0.6581661621420797, 0.32908308107103984, 0.6816273244027905, 0.29212599617262447, 0.37931756983242976, 0.6163910509776984, 0.6571786530922857, 0.34912615945527675, 0.6820839659500362, 0.32309240492370134, 0.674359103217723, 0.32421110731621294, 0.3891328969010998, 0.6226126350417597, 0.5048170734187974, 0.4950148001485295, 0.6384654622955761, 0.3617970953008265, 0.3983593075742237, 0.6128604731911134, 0.5449409029187423, 0.4588976024578883, 0.40144563199106137, 0.5956935184383492, 0.6261719407184286, 0.3732948108129094, 0.5393983307793762, 0.46132751974551905, 0.4701698366095142, 0.5300096339961796, 0.3941199888568821, 0.6108859827281673, 0.6950424135875747, 0.34752120679378734, 0.4283762211515139, 0.5669685279946508, 0.6430849427613795, 0.3627658651474448, 0.41003056279897154, 0.5922663684874033, 0.4263216826861183, 0.5750385487394154, 0.5235680966644326, 0.4747017409757522, 0.6851985808647297, 0.29365653465631275, 0.38897091622552926, 0.6158706173570879, 0.39988437218963474, 0.6131560373574398, 0.5718520929729737, 0.4259714570104804, 0.5326980366094278, 0.46713520133442127, 0.6891322190104177, 0.3150318715476195, 0.6783957948430438, 0.29074105493273306, 0.3843502082113237, 0.6179748445750696, 0.6413572536868815, 0.36648985924964655, 0.3436388921962059, 0.6517289334755629, 0.7096314746296523, 0.35481573731482613, 0.5760146972814129, 0.426093063742415, 0.786673646276966, 0.1966684115692415, 0.3969401794712178, 0.6134530046373367, 0.45573627380874954, 0.552701438448909, 0.6021665719616193, 0.3902931484936421, 0.4184562729321226, 0.6276844093981839, 0.6567781415738658, 0.34480852432627956, 0.5823444627060032, 0.4137710656068969, 0.5830612670470965, 0.4137854153237459, 0.7058440504528283, 0.35292202522641414, 0.638989192451389, 0.35898269238842073, 0.4766784667368877, 0.5286797540172754, 0.6823720967512231, 0.3173823705819643, 0.6641159817391227, 0.33205799086956134, 0.6749145227280484, 0.3374572613640242, 0.5277021331904566, 0.47256907449891633, 0.6714030470470487, 0.28774416302016376, 0.5902681202866503, 0.4106213010689741, 0.39410225002086796, 0.6193035357470782, 0.6841630442255464, 0.3240772314752588, 0.6356551913667082, 0.3674881575088782, 0.626306694742884, 0.37337514494287316, 0.5531970040656057, 0.44808957329314064], \"Term\": [\"accepted\", \"accepted\", \"agreement\", \"agreement\", \"aid\", \"aid\", \"alive\", \"alive\", \"allocated\", \"allocated\", \"allow\", \"allow\", \"allowed\", \"allowed\", \"alongside\", \"alongside\", \"amendments\", \"amendments\", \"arise\", \"arise\", \"asserted\", \"asserted\", \"audits\", \"audits\", \"block\", \"block\", \"boost\", \"boost\", \"break\", \"break\", \"bringing\", \"bringing\", \"budget\", \"budget\", \"built\", \"built\", \"cannabis\", \"cannabis\", \"capacity\", \"capacity\", \"census\", \"census\", \"central\", \"central\", \"china\", \"china\", \"comfort\", \"comfort\", \"considering\", \"considering\", \"continued\", \"continued\", \"counts\", \"counts\", \"currently\", \"currently\", \"dealing\", \"dealing\", \"depend\", \"depend\", \"depends\", \"depends\", \"developing\", \"developing\", \"directorate\", \"directorate\", \"disciplinary\", \"disciplinary\", \"echelon\", \"echelon\", \"economic\", \"economic\", \"efforts\", \"efforts\", \"else\", \"else\", \"establishment\", \"establishment\", \"exclusive\", \"exclusive\", \"expansion\", \"expansion\", \"extension\", \"extension\", \"families\", \"families\", \"farmer\", \"farmer\", \"femicide\", \"femicide\", \"fighting\", \"fighting\", \"finally\", \"finally\", \"functional\", \"functional\", \"gang\", \"gang\", \"gone\", \"gone\", \"govern\", \"govern\", \"governing\", \"governing\", \"governments\", \"governments\", \"greater\", \"greater\", \"guarantee\", \"guarantee\", \"guests\", \"guests\", \"healthy\", \"healthy\", \"highly\", \"highly\", \"home\", \"home\", \"hundreds\", \"hundreds\", \"immediate\", \"immediate\", \"informed\", \"informed\", \"infrastructural\", \"infrastructural\", \"institute\", \"institute\", \"intelligence\", \"intelligence\", \"introduce\", \"introduce\", \"introducing\", \"introducing\", \"introduction\", \"introduction\", \"israelpalestine\", \"israelpalestine\", \"jobs\", \"jobs\", \"laid\", \"laid\", \"laws\", \"laws\", \"leading\", \"leading\", \"limiting\", \"limiting\", \"line\", \"line\", \"literacy\", \"literacy\", \"lost\", \"lost\", \"maintenance\", \"maintenance\", \"making\", \"making\", \"market\", \"market\", \"met\", \"met\", \"nonracial\", \"nonracial\", \"opportunity\", \"opportunity\", \"outside\", \"outside\", \"peaceloving\", \"peaceloving\", \"period\", \"period\", \"permit\", \"permit\", \"phase\", \"phase\", \"phased\", \"phased\", \"pieces\", \"pieces\", \"planning\", \"planning\", \"plays\", \"plays\", \"positions\", \"positions\", \"present\", \"present\", \"presidency\", \"presidency\", \"private\", \"private\", \"procured\", \"procured\", \"professionals\", \"professionals\", \"proposal\", \"proposal\", \"proposals\", \"proposals\", \"put\", \"put\", \"railways\", \"railways\", \"rd\", \"rd\", \"red\", \"red\", \"reducing\", \"reducing\", \"reforms\", \"reforms\", \"reinforce\", \"reinforce\", \"relations\", \"relations\", \"removal\", \"removal\", \"require\", \"require\", \"responses\", \"responses\", \"retrenchment\", \"retrenchment\", \"retrenchments\", \"retrenchments\", \"reverse\", \"reverse\", \"saw\", \"saw\", \"scale\", \"scale\", \"signs\", \"signs\", \"skilled\", \"skilled\", \"son\", \"son\", \"sporting\", \"sporting\", \"standards\", \"standards\", \"starts\", \"starts\", \"strikes\", \"strikes\", \"subject\", \"subject\", \"supported\", \"supported\", \"taxi\", \"taxi\", \"tolerated\", \"tolerated\", \"unit\", \"unit\", \"unity\", \"unity\", \"upswing\", \"upswing\", \"various\", \"various\", \"virus\", \"virus\", \"volunteers\", \"volunteers\", \"water\", \"water\", \"well\", \"well\", \"wish\", \"wish\"]}, \"R\": 30, \"lambda.step\": 0.01, \"plot.opts\": {\"xlab\": \"PC1\", \"ylab\": \"PC2\"}, \"topic.order\": [1, 2]};\n",
       "\n",
       "function LDAvis_load_lib(url, callback){\n",
       "  var s = document.createElement('script');\n",
       "  s.src = url;\n",
       "  s.async = true;\n",
       "  s.onreadystatechange = s.onload = callback;\n",
       "  s.onerror = function(){console.warn(\"failed to load library \" + url);};\n",
       "  document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "}\n",
       "\n",
       "if(typeof(LDAvis) !== \"undefined\"){\n",
       "   // already loaded: just create the visualization\n",
       "   !function(LDAvis){\n",
       "       new LDAvis(\"#\" + \"ldavis_el1699455308886566944433272\", ldavis_el1699455308886566944433272_data);\n",
       "   }(LDAvis);\n",
       "}else if(typeof define === \"function\" && define.amd){\n",
       "   // require.js is available: use it to load d3/LDAvis\n",
       "   require.config({paths: {d3: \"https://d3js.org/d3.v5\"}});\n",
       "   require([\"d3\"], function(d3){\n",
       "      window.d3 = d3;\n",
       "      LDAvis_load_lib(\"https://cdn.jsdelivr.net/gh/bmabey/pyLDAvis@3.4.0/pyLDAvis/js/ldavis.v3.0.0.js\", function(){\n",
       "        new LDAvis(\"#\" + \"ldavis_el1699455308886566944433272\", ldavis_el1699455308886566944433272_data);\n",
       "      });\n",
       "    });\n",
       "}else{\n",
       "    // require.js not available: dynamically load d3 & LDAvis\n",
       "    LDAvis_load_lib(\"https://d3js.org/d3.v5.js\", function(){\n",
       "         LDAvis_load_lib(\"https://cdn.jsdelivr.net/gh/bmabey/pyLDAvis@3.4.0/pyLDAvis/js/ldavis.v3.0.0.js\", function(){\n",
       "                 new LDAvis(\"#\" + \"ldavis_el1699455308886566944433272\", ldavis_el1699455308886566944433272_data);\n",
       "            })\n",
       "         });\n",
       "}\n",
       "</script>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| eval: true\n",
    "#| output: true\n",
    "\n",
    "# Display the visualization\n",
    "pyLDAvis.display(vis_data_sentences)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "7081c0cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tomotopy as tp\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Function to calculate coherence scores\n",
    "def calculate_coherence(model, metric='u_mass'):\n",
    "    coherence = tp.coherence.Coherence(model, coherence=metric)\n",
    "    return coherence.get_score()\n",
    "\n",
    "# Prepare the data for the CTM model\n",
    "tokenized_docs = [text.split() for text in sona_speeches_clean['speech']]  # Ensure the texts are tokenized\n",
    "tokenized_sentences = [text.split() for text in sona_sentences_alltogether['sentence']]  # Ensure the texts are tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "1358b1cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the range of topic numbers you want to test\n",
    "topic_numbers = range(2, 12, 1)  # for example from 2 to 20 by step of 2\n",
    "\n",
    "# Store coherence scores for plotting\n",
    "coherence_scores = []\n",
    "coherence_scores_sentences = []\n",
    "\n",
    "for k in topic_numbers:\n",
    "    # Initialize CTM with the current number of topics\n",
    "    ctm = tp.CTModel(k=k)\n",
    "    ctms = tp.CTModel(k=k)\n",
    "\n",
    "    # Add documents to the model\n",
    "    for tokens in tokenized_docs:\n",
    "        ctm.add_doc(tokens)\n",
    "\n",
    "    # Add sentences to the model\n",
    "    for tokens in tokenized_sentences:\n",
    "        ctms.add_doc(tokens)    \n",
    "\n",
    "    # Train the model\n",
    "    ctm.train(0)\n",
    "    ctms.train(0)\n",
    "\n",
    "    for _ in range(100):\n",
    "        ctm.train(10)\n",
    "        ctms.train(10)\n",
    "\n",
    "    # Calculate and store the coherence score\n",
    "    score = calculate_coherence(ctm)\n",
    "    score_sentences = calculate_coherence(ctms)\n",
    "\n",
    "    coherence_scores.append(score)\n",
    "    coherence_scores_sentences.append(score_sentences)\n",
    "\n",
    "    #(f\"Topics: {k}, Coherence Score: {score}\")\n",
    "\n",
    "# Save the coherence scores to a csv\n",
    "pd.DataFrame({\n",
    "    'topic_number': topic_numbers,\n",
    "    'bow_coherence_values': coherence_scores,\n",
    "    'bow_coherence_values_sentences': coherence_scores_sentences\n",
    "}).to_csv('data/saved_ctm_coherence_values.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "5a579c02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the saved values\n",
    "coherence_values = pd.read_csv('data/saved_ctm_coherence_values.csv')\n",
    "\n",
    "topic_numbers = coherence_values['topic_number']\n",
    "coherence_scores = coherence_values['bow_coherence_values']\n",
    "coherence_scores_sentences = coherence_values['bow_coherence_values_sentences']\n",
    "\n",
    "# Plot the speech coherence scores\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(topic_numbers, coherence_scores,  color='midnightblue')\n",
    "plt.xlabel('Number of Topics')\n",
    "plt.ylabel('Coherence Score')\n",
    "plt.xticks(topic_numbers)\n",
    "plt.savefig(f'ctm_plots/words_coherence_plots.png', bbox_inches='tight')\n",
    "plt.close()\n",
    "\n",
    "# Plot the sentence coherence scores\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(topic_numbers, coherence_scores_sentences, color='lightsteelblue')\n",
    "plt.xlabel('Number of Topics')\n",
    "plt.ylabel('Coherence Score')\n",
    "plt.xticks(topic_numbers)\n",
    "plt.savefig(f'ctm_plots/sentences_coherence_plots.png', bbox_inches='tight')\n",
    "plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c8b19151",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Set the number of topics based on the plots\n",
    "\n",
    "# Run 7\n",
    "\n",
    "# Train the models with the optimal number of topics\n",
    "ctm = tp.CTModel(k=2)\n",
    "ctms = tp.CTModel(k=2)\n",
    "\n",
    "# Add documents to the model\n",
    "for tokens in tokenized_docs:\n",
    "    ctm.add_doc(tokens)\n",
    "\n",
    "# Add sentences to the model\n",
    "for tokens in tokenized_sentences:\n",
    "    ctms.add_doc(tokens)\n",
    "\n",
    "# Train the model\n",
    "ctm.train(0)\n",
    "ctms.train(0)\n",
    "\n",
    "for _ in range(100):\n",
    "    ctm.train(10)\n",
    "    ctms.train(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b332e0c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run 8\n",
    "\n",
    "# Function to plot the top words for one topic\n",
    "def plot_top_words_for_topic(model, modtype, topic_num, color, top_n=10):\n",
    "    # Extract the top words for this topic\n",
    "    top_words = model.get_topic_words(topic_num, top_n=top_n)\n",
    "    words, weights = zip(*top_words)\n",
    "\n",
    "    # Create a bar chart for the top words in this topic\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.barh(words, weights, color=color)\n",
    "    plt.xlabel('Weight')\n",
    "    plt.gca().invert_yaxis()  # Highest weights on top\n",
    "    plt.savefig(f'ctm_plots/{modtype}_{topic_num+1}.png', bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "# Plot the top words for each topic for the CTModel of documents\n",
    "i = 0\n",
    "colours = ['midnightblue', 'lightsteelblue']\n",
    "\n",
    "for k in range(ctm.k):\n",
    "    plot_top_words_for_topic(ctm, 'words', k, colours[i])\n",
    "    i+=1\n",
    "\n",
    "    if i == 2:\n",
    "        i = 0\n",
    "\n",
    "i = 0\n",
    "# Plot the top words for each topic for the CTModel of sentences\n",
    "for k in range(ctms.k):\n",
    "    plot_top_words_for_topic(ctms, 'sentences', k, colours[i])\n",
    "    i+=1\n",
    "\n",
    "    if i == 2:\n",
    "        i = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "f10de336",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note that ATM only works for BoW. Raw word counts (BoW) is standard because these models are based on the assumption that the data is generated from a multinomial distribution, which does not hold with TF-IDF weights.\n",
    "\n",
    "texts = sona_speeches_clean['speech']\n",
    "sentences = sona_sentences_alltogether['sentence']\n",
    "\n",
    "# Further process tokens using gensim's simple_preprocess\n",
    "tokenized_texts = [simple_preprocess(doc, deacc=True) for doc in texts]  # deacc=True removes punctuations\n",
    "tokenized_sentences = [simple_preprocess(doc, deacc=True) for doc in sentences]  # deacc=True removes punctuations\n",
    "\n",
    "# Create a Gensim dictionary from the tokenized documents\n",
    "dictionary = corpora.Dictionary(tokenized_texts)\n",
    "dictionary.filter_extremes(no_below=3, no_above=0.7)\n",
    "\n",
    "dict_sentences = corpora.Dictionary(tokenized_sentences)\n",
    "dict_sentences.filter_extremes(no_below=3, no_above=0.7)\n",
    "\n",
    "# Create a BOW corpus\n",
    "bow_corpus = [dictionary.doc2bow(text) for text in tokenized_texts]\n",
    "bow_corpus_sentences = [dict_sentences.doc2bow(text) for text in tokenized_sentences]\n",
    "\n",
    "# Prepare the data for the AuthorTopicModel\n",
    "# Create a mapping of authors to documents\n",
    "author2doc = {author: [] for author in sona_speeches_clean['president'].unique()}\n",
    "for i, row in sona_speeches_clean.iterrows():\n",
    "    author2doc[row['president']].append(i)\n",
    "\n",
    "# Create a mapping of authors to sentences\n",
    "author2sent = {author: [] for author in sona_sentences_alltogether['president'].unique()}\n",
    "for i, row in sona_sentences_alltogether.iterrows():\n",
    "    author2sent[row['president']].append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "e1b6cd4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the range of topic numbers you want to test\n",
    "topic_numbers = range(2, 12, 1)  # for example from 2 to 20 by step of 2\n",
    "\n",
    "# Store coherence scores for plotting\n",
    "coherence_scores = []\n",
    "coherence_scores_sentences = []\n",
    "\n",
    "for num_topics in topic_numbers:\n",
    "    # Author-Topic LDA model with the current number of topics\n",
    "    author_topic_model = AuthorTopicModel(corpus=bow_corpus, author2doc=author2doc, id2word=dictionary, num_topics=num_topics)\n",
    "    author_topic_model_sentences = AuthorTopicModel(corpus=bow_corpus_sentences, author2doc=author2sent, id2word=dict_sentences, num_topics=num_topics)\n",
    "\n",
    "    # Train the model\n",
    "    author_topic_model.update(bow_corpus, author2doc=author2doc)\n",
    "    author_topic_model_sentences.update(bow_corpus_sentences, author2doc=author2sent)\n",
    "\n",
    "    # Compute coherence score\n",
    "    cm = CoherenceModel(model=author_topic_model, texts=tokenized_docs, dictionary=dictionary, coherence='u_mass')\n",
    "    cm_sentences = CoherenceModel(model=author_topic_model_sentences, texts=tokenized_sentences, dictionary=dict_sentences, coherence='u_mass')\n",
    "\n",
    "    coherence = cm.get_coherence()\n",
    "    coherence_sentences = cm_sentences.get_coherence()\n",
    "\n",
    "    coherence_scores.append(coherence)\n",
    "    coherence_scores_sentences.append(coherence_sentences)\n",
    "\n",
    "# Save the coherence scores to a csv\n",
    "pd.DataFrame({\n",
    "    'topic_number': topic_numbers,\n",
    "    'bow_coherence_values': coherence_scores,\n",
    "    'bow_coherence_values_sentences': coherence_scores_sentences\n",
    "}).to_csv('data/saved_atm_coherence_values.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "572cb578",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the saved values\n",
    "coherence_values = pd.read_csv('data/saved_atm_coherence_values.csv')\n",
    "\n",
    "topic_numbers = coherence_values['topic_number']\n",
    "coherence_scores = coherence_values['bow_coherence_values']\n",
    "coherence_scores_sentences = coherence_values['bow_coherence_values_sentences']\n",
    "\n",
    "# Plot the speech coherence scores\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(topic_numbers, coherence_scores,  color='midnightblue')\n",
    "plt.title('Coherence Scores by Number of Topics')\n",
    "plt.xlabel('Number of Topics')\n",
    "plt.ylabel('Coherence Score')\n",
    "plt.xticks(topic_numbers)\n",
    "plt.savefig(f'atm_plots/words_coherence_plots.png', bbox_inches='tight')\n",
    "plt.close()\n",
    "\n",
    "# Plot the sentence coherence scores\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(topic_numbers, coherence_scores_sentences, color='lightsteelblue')\n",
    "plt.title('Coherence Scores by Number of Topics')\n",
    "plt.xlabel('Number of Topics')\n",
    "plt.ylabel('Coherence Score')\n",
    "plt.xticks(topic_numbers)\n",
    "plt.savefig(f'atm_plots/sentences_coherence_plots.png', bbox_inches='tight')\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "351ec6af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run 9\n",
    "\n",
    "# TODO: Set the number of topics based on the plots\n",
    "\n",
    "# Author-Topic LDA model with the current number of topics\n",
    "author_topic_model = AuthorTopicModel(corpus=bow_corpus, author2doc=author2doc, id2word=dictionary, num_topics=3)\n",
    "author_topic_model_sentences = AuthorTopicModel(corpus=bow_corpus_sentences, author2doc=author2sent, id2word=dict_sentences, num_topics=2)\n",
    "\n",
    "# Train the model\n",
    "author_topic_model.update(bow_corpus, author2doc=author2doc)\n",
    "author_topic_model_sentences.update(bow_corpus_sentences, author2doc=author2sent)\n",
    "\n",
    "# Function to plot the top words for one topic in an AuthorTopicModel\n",
    "def plot_top_words_for_author_topic_model(model, modtype, topic_num, top_n=10):\n",
    "    # Extract the top words for this topic\n",
    "    top_words = model.get_topic_terms(topic_num, topn=top_n)\n",
    "    words, weights = zip(*[(model.id2word[word_id], weight) for word_id, weight in top_words])\n",
    "\n",
    "    # Create a bar chart for the top words in this topic\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.barh(words, weights, color='midnightblue')\n",
    "    plt.xlabel('Weight')\n",
    "    plt.gca().invert_yaxis()  # Highest weights on top\n",
    "    plt.savefig(f'atm_plots/{modtype}_{topic_num+1}.png', bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "# Plot the top words for each topic for the AuthorTopicModel of documents\n",
    "for k in range(author_topic_model.num_topics):\n",
    "    plot_top_words_for_author_topic_model(author_topic_model, \"words\",k)\n",
    "\n",
    "# Plot the top words for each topic for the AuthorTopicModel of sentences\n",
    "for k in range(author_topic_model_sentences.num_topics):\n",
    "    plot_top_words_for_author_topic_model(author_topic_model_sentences, \"sentences\", k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "fa794b07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run 10\n",
    "\n",
    "from pprint import pprint\n",
    "\n",
    "def show_author(name, model, topic_labels):\n",
    "    print('\\n%s' % name)\n",
    "    print('Docs:', pd.Series(author_topic_model.author2doc[name]).unique())\n",
    "    print('Topics:')\n",
    "    pprint([(topic_labels[topic[0]], topic[1]) for topic in model[name]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "cb291c08",
   "metadata": {},
   "outputs": [],
   "source": [
    "sona_speeches_clean['president'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "f19bc035",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run 11\n",
    "\n",
    "# TODO: Add topic labels\n",
    "\n",
    "topic_labels_words = ['Circuits', 'Neuroscience', 'Numerical optimization']\n",
    "\n",
    "topic_labels_sents = ['Circuits', 'Neuroscience', 'Numerical optimization', 'Object recognition', \\\n",
    "               'Math/general', 'Robotics', 'Character recognition', \\\n",
    "                'Reinforcement learning', 'Speech recognition', 'Bayesian modelling']\n",
    "\n",
    "# Show the topics for each author\n",
    "show_author('Mandela', author_topic_model, topic_labels_words)\n",
    "#show_author('deKlerk', author_topic_model_sentences)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}