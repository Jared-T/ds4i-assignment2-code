{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "87c0cc98",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# imports\n",
    "import os\n",
    "import sys\n",
    "import types\n",
    "import json\n",
    "\n",
    "# figure size/format\n",
    "fig_width = 7\n",
    "fig_height = 5\n",
    "fig_format = 'retina'\n",
    "fig_dpi = 96\n",
    "\n",
    "# matplotlib defaults / format\n",
    "try:\n",
    "  import matplotlib.pyplot as plt\n",
    "  plt.rcParams['figure.figsize'] = (fig_width, fig_height)\n",
    "  plt.rcParams['figure.dpi'] = fig_dpi\n",
    "  plt.rcParams['savefig.dpi'] = fig_dpi\n",
    "  from IPython.display import set_matplotlib_formats\n",
    "  set_matplotlib_formats(fig_format)\n",
    "except Exception:\n",
    "  pass\n",
    "\n",
    "# plotly use connected mode\n",
    "try:\n",
    "  import plotly.io as pio\n",
    "  pio.renderers.default = \"notebook_connected\"\n",
    "except Exception:\n",
    "  pass\n",
    "\n",
    "# enable pandas latex repr when targeting pdfs\n",
    "try:\n",
    "  import pandas as pd\n",
    "  if fig_format == 'pdf':\n",
    "    pd.set_option('display.latex.repr', True)\n",
    "except Exception:\n",
    "  pass\n",
    "\n",
    "\n",
    "\n",
    "# output kernel dependencies\n",
    "kernel_deps = dict()\n",
    "for module in list(sys.modules.values()):\n",
    "  # Some modules play games with sys.modules (e.g. email/__init__.py\n",
    "  # in the standard library), and occasionally this can cause strange\n",
    "  # failures in getattr.  Just ignore anything that's not an ordinary\n",
    "  # module.\n",
    "  if not isinstance(module, types.ModuleType):\n",
    "    continue\n",
    "  path = getattr(module, \"__file__\", None)\n",
    "  if not path:\n",
    "    continue\n",
    "  if path.endswith(\".pyc\") or path.endswith(\".pyo\"):\n",
    "    path = path[:-1]\n",
    "  if not os.path.exists(path):\n",
    "    continue\n",
    "  kernel_deps[path] = os.stat(path).st_mtime\n",
    "print(json.dumps(kernel_deps))\n",
    "\n",
    "# set run_path if requested\n",
    "if r'/Users/jared/Library/Group Containers/UBF8T346G9.OneDriveStandaloneSuite/OneDrive.noindex/OneDrive/Documents/University stuff/Masters Year/DS4I/Assignment1/project':\n",
    "  os.chdir(r'/Users/jared/Library/Group Containers/UBF8T346G9.OneDriveStandaloneSuite/OneDrive.noindex/OneDrive/Documents/University stuff/Masters Year/DS4I/Assignment1/project')\n",
    "\n",
    "# reset state\n",
    "%reset\n",
    "\n",
    "def ojs_define(**kwargs):\n",
    "  import json\n",
    "  try:\n",
    "    # IPython 7.14 preferred import\n",
    "    from IPython.display import display, HTML\n",
    "  except:\n",
    "    from IPython.core.display import display, HTML\n",
    "\n",
    "  # do some minor magic for convenience when handling pandas\n",
    "  # dataframes\n",
    "  def convert(v):\n",
    "    try:\n",
    "      import pandas as pd\n",
    "    except ModuleNotFoundError: # don't do the magic when pandas is not available\n",
    "      return v\n",
    "    if type(v) == pd.Series:\n",
    "      v = pd.DataFrame(v)\n",
    "    if type(v) == pd.DataFrame:\n",
    "      j = json.loads(v.T.to_json(orient='split'))\n",
    "      return dict((k,v) for (k,v) in zip(j[\"index\"], j[\"data\"]))\n",
    "    else:\n",
    "      return v\n",
    "  \n",
    "  v = dict(contents=list(dict(name=key, value=convert(value)) for (key, value) in kwargs.items()))\n",
    "  display(HTML('<script type=\"ojs-define\">' + json.dumps(v) + '</script>'), metadata=dict(ojs_define = True))\n",
    "globals()[\"ojs_define\"] = ojs_define\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d9c11c2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Dropout, BatchNormalization\n",
    "from keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "from keras.initializers import he_normal\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import re\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "import pickle\n",
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.regularizers import l2\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import load_model\n",
    "import json\n",
    "import cuml\n",
    "from IPython.display import Image, display\n",
    "import zipfile\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "from itertools import cycle\n",
    "from collections import Counter\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2367be2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unzip the file and get the list of filenames\n",
    "with zipfile.ZipFile(\"data/speeches.zip\", 'r') as zip_ref:\n",
    "    zip_ref.extractall(\"data\")\n",
    "\n",
    "filenames = os.listdir(\"data\")\n",
    "filenames = [filename for filename in filenames if filename.endswith('.txt')]\n",
    "\n",
    "# Read the content of each speech file and extract the date from the first line\n",
    "speeches = []\n",
    "dates = []\n",
    "for filename in filenames:\n",
    "    with open(os.path.join(\"data\", filename), 'r', encoding='utf-8') as file:\n",
    "        # Extract date from the first line\n",
    "        date = file.readline().strip()\n",
    "        dates.append(date)\n",
    "        \n",
    "        # Read the rest of the file\n",
    "        speeches.append(file.read())\n",
    "\n",
    "# Create DataFrame\n",
    "sona = pd.DataFrame({'filename': filenames, 'speech': speeches, 'date': dates})\n",
    "\n",
    "# Extract year and president for each speech\n",
    "sona['year'] = sona['filename'].str[:4]\n",
    "sona['president'] = sona['filename'].str.split('_').str[-1].str.split('.').str[0]\n",
    "\n",
    "# Clean the sona dataset by removing unnecessary text\n",
    "replace_reg = r'(http.*?(\\s|.$))|(www.*?(\\s|.$))|&amp;|&lt;|&gt;|\\n'\n",
    "sona['speech'] = sona['speech'].str.replace(replace_reg, ' ')\n",
    "\n",
    "# Split speeches into sentences\n",
    "sona_sentences = sona['speech'].str.split(r'(?<!\\w\\.\\w.)(?<![A-Z][a-z]\\.)(?<=\\.|\\?)\\s', expand=True).stack().reset_index(level=-1, drop=True)\n",
    "sona_sentences.name = 'sentence'\n",
    "\n",
    "# Remove newline characters from the sentences\n",
    "sona_sentences = sona_sentences.str.replace('\\n', '').str.strip()\n",
    "\n",
    "# Merge with the president, date, and year columns to associate each sentence with the respective details\n",
    "df_sentences = sona[['president', 'date', 'year']].join(sona_sentences)\n",
    "\n",
    "# Make a csv of the sentences\n",
    "df_sentences.to_csv('data/sentences.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b80c8a85",
   "metadata": {},
   "outputs": [],
   "source": [
    "speeches_per_president = sona.groupby('president').size().reset_index(name='number_of_speeches')\n",
    "\n",
    "# Display the table for number of speeches per president in a well-formatted manner\n",
    "speeches_per_president.set_index('president', inplace=True)\n",
    "\n",
    "# Plot the number of speeches per president\n",
    "speeches_per_president.plot(kind='bar', legend=False, color='mediumpurple', edgecolor='black', ax=plt.gca())\n",
    "plt.title('Number of Speeches per President')\n",
    "plt.ylabel('Number of Speeches')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "sentences_per_president = df_sentences.groupby('president').size().reset_index(name='number_of_sentences')\n",
    "sentences_per_president.plot(x='president', y='number_of_sentences', kind='bar', legend=False, color='skyblue', edgecolor='black')\n",
    "plt.title('Number of Sentences per President')\n",
    "plt.ylabel('Number of Sentences')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "df_sentences['sentence_length'] = df_sentences['sentence'].str.split().str.len()\n",
    "avg_sentence_length = df_sentences.groupby('president')['sentence_length'].mean().reset_index()\n",
    "\n",
    "avg_sentence_length.plot(x='president', y='sentence_length', kind='bar', legend=False, color='lightcoral', edgecolor='black')\n",
    "plt.title('Average Sentence Length per President')\n",
    "plt.ylabel('Average Sentence Length (words)')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Set the stopwords\n",
    "stopwords = set(STOPWORDS)\n",
    "\n",
    "# Selected colormaps\n",
    "colormaps = cycle(['viridis', 'plasma', 'magma', 'cividis'])\n",
    "\n",
    "# Adjusting the layout for the word clouds\n",
    "plt.figure(figsize=(15, 30))\n",
    "\n",
    "# Generate a word cloud for each president, with adjusted layout\n",
    "for idx, (president, group) in enumerate(df_sentences.groupby('president')):\n",
    "    text = ' '.join(group['sentence'])\n",
    "    wc = WordCloud(stopwords=stopwords, background_color='white', colormap=next(colormaps), max_words=200, width=800, height=400).generate(text)\n",
    "    \n",
    "    # Display the word cloud\n",
    "    plt.subplot(len(df_sentences['president'].unique()), 1, idx + 1)\n",
    "    plt.imshow(wc, interpolation='bilinear')\n",
    "    plt.axis('off')\n",
    "    plt.title(president, fontdict={'fontsize': 20, 'fontweight': 'medium'})\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Define a simple tokenizer function\n",
    "def simple_tokenize(text):\n",
    "    return [word for word in text.split() if word.isalpha()]\n",
    "\n",
    "# Update the plotting function to use the simple tokenizer\n",
    "def plot_word_frequency(text, president_name, n=10):\n",
    "    tokens = simple_tokenize(text.lower())\n",
    "    filtered_tokens = [word for word in tokens if word not in stopwords]\n",
    "    frequency = Counter(filtered_tokens)\n",
    "    most_common = frequency.most_common(n)\n",
    "    \n",
    "    words, counts = zip(*most_common)\n",
    "    plt.bar(words, counts, color='lightseagreen')\n",
    "    plt.title(f\"Top {n} Words used by {president_name}\")\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    plt.show()\n",
    "\n",
    "# Plot word frequency distribution for each president using the updated function\n",
    "for president, group in df_sentences.groupby('president'):\n",
    "    plot_word_frequency(' '.join(group['sentence']), president)\n",
    "\n",
    "\n",
    "# Define a function to generate n-grams\n",
    "def generate_ngrams(text, n):\n",
    "    tokens = simple_tokenize(text.lower())\n",
    "    filtered_tokens = [word for word in tokens if word not in stopwords]\n",
    "    ngrams = zip(*[filtered_tokens[i:] for i in range(n)])\n",
    "    return [\" \".join(ngram) for ngram in ngrams]\n",
    "\n",
    "# Define function to plot N-gram frequency\n",
    "def plot_ngram_frequency(text, president_name, n=2, top_n=10):\n",
    "    ngrams = generate_ngrams(text, n)\n",
    "    frequency = Counter(ngrams)\n",
    "    most_common = frequency.most_common(top_n)\n",
    "    \n",
    "    phrases, counts = zip(*most_common)\n",
    "    plt.bar(phrases, counts, color='lightsalmon')\n",
    "    plt.title(f\"Top {top_n} {n}-grams used by {president_name}\")\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    plt.show()\n",
    "\n",
    "# Plot bigram frequency distribution for each president\n",
    "for president, group in df_sentences.groupby('president'):\n",
    "    plot_ngram_frequency(' '.join(group['sentence']), president)\n",
    "\n",
    "\n",
    "# Plot trigram frequency distribution for each president\n",
    "for president, group in df_sentences.groupby('president'):\n",
    "    plot_ngram_frequency(' '.join(group['sentence']), president, n=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "df2e4229",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bow_x(data):\n",
    "    # Extract relevant columns\n",
    "    text_data = data['sentence']\n",
    "    y = data['president']\n",
    "\n",
    "    # Initialize a CountVectorizer for BOW representation\n",
    "    vectorizer = CountVectorizer(lowercase=True, token_pattern=r\"(?u)\\b\\w+\\b\", stop_words='english')\n",
    "\n",
    "    # Fit and transform the text data\n",
    "    X = vectorizer.fit_transform(text_data)\n",
    "\n",
    "    # Create a DataFrame from the BOW representation\n",
    "    bow_df = pd.DataFrame(X.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "\n",
    "    return bow_df\n",
    "\n",
    "\n",
    "def tf_idf(df):\n",
    "    sentences = df['sentence'].tolist()\n",
    "\n",
    "    # Create a TfidfVectorizer with stop words removal\n",
    "    tfidf_vectorizer = TfidfVectorizer(stop_words='english')\n",
    "\n",
    "    # Fit and transform the sentences to compute TF-IDF values\n",
    "    tfidf_matrix = tfidf_vectorizer.fit_transform(sentences)\n",
    "\n",
    "    # Create a new dataframe with TF-IDF values\n",
    "    tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=tfidf_vectorizer.get_feature_names_out())\n",
    "\n",
    "    return tfidf_df\n",
    "\n",
    "\n",
    "def tokenize_text(text_data, labels, max_features=10000, maxlen=100):\n",
    "    # Tokenization\n",
    "    tokenizer = Tokenizer(num_words=max_features)\n",
    "    tokenizer.fit_on_texts(text_data)\n",
    "    sequences = tokenizer.texts_to_sequences(text_data)\n",
    "\n",
    "    # Filter out sequences that have length 0\n",
    "    seq_ok = [i for i, s in enumerate(sequences) if len(s) > 0]\n",
    "    valid_labels = [labels.iloc[i] for i in seq_ok]\n",
    "\n",
    "    # Padding sequences\n",
    "    filtered_sequences = [sequences[i] for i in seq_ok]\n",
    "    x_pad = pad_sequences(filtered_sequences, maxlen=maxlen)\n",
    "\n",
    "\n",
    "    return x_pad, valid_labels, tokenizer\n",
    "\n",
    "\n",
    "# Sample a subset of the data to alleviate memory issues\n",
    "sample_df = df_sentences.sample(frac=0.4, random_state=1)\n",
    "\n",
    "# Encoding sentences using Bag of Words (BOW)\n",
    "bow_encoded = bow_x(sample_df)\n",
    "y_bow = sample_df['president']\n",
    "\n",
    "# Dimensionality reduction using t-SNE\n",
    "tsne = TSNE(n_components=2, random_state=1)\n",
    "bow_tsne = tsne.fit_transform(bow_encoded)\n",
    "\n",
    "# Visualise the results\n",
    "plt.figure(figsize=(12, 8))\n",
    "for president in sample_df['president'].unique():\n",
    "    indices = [i for i, label in enumerate(y_bow) if label == president]\n",
    "    plt.scatter(bow_tsne[indices, 0], bow_tsne[indices, 1], label=president, alpha=0.7)\n",
    "\n",
    "plt.title('Visualisation of Sentences using Bag of Words (BOW)')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Encoding sentences using TF-IDF for the sample data\n",
    "tfidf_encoded_sample = tf_idf(sample_df)\n",
    "\n",
    "# Dimensionality reduction using t-SNE for the TF-IDF encoded data\n",
    "tfidf_tsne_sample = tsne.fit_transform(tfidf_encoded_sample)\n",
    "\n",
    "# Visualise the results for the TF-IDF encoded sample data\n",
    "plt.figure(figsize=(12, 8))\n",
    "for president in sample_df['president'].unique():\n",
    "    indices = [i for i, label in enumerate(sample_df['president']) if label == president]\n",
    "    plt.scatter(tfidf_tsne_sample[indices, 0], tfidf_tsne_sample[indices, 1], label=president, alpha=0.7)\n",
    "\n",
    "plt.title('Visualisation of Sampled Sentences using TF-IDF')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Encoding sentences using Tokenization with Padding for the sample data\n",
    "x_pad_sample, valid_labels_sample, _ = tokenize_text(sample_df['sentence'], sample_df['president'])\n",
    "\n",
    "# Dimensionality reduction using t-SNE for the Tokenized data\n",
    "tokenized_tsne_sample = tsne.fit_transform(x_pad_sample)\n",
    "\n",
    "# Visualise the results for the Tokenized sample data\n",
    "plt.figure(figsize=(12, 8))\n",
    "for president in sample_df['president'].unique():\n",
    "    indices = [i for i, label in enumerate(valid_labels_sample) if label == president]\n",
    "    plt.scatter(tokenized_tsne_sample[indices, 0], tokenized_tsne_sample[indices, 1], label=president, alpha=0.7)\n",
    "\n",
    "plt.title('Visualisation of Sampled Sentences using Tokenization with Padding')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "96aaabee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def neural_net(neurons_per_layer, l2_reg_value, epochs, dropout_rate=0.5):\n",
    "    model = Sequential()\n",
    "\n",
    "    # Input layer with He initialization\n",
    "    model.add(Dense(neurons_per_layer[0], activation='relu', input_dim=inp_dim, kernel_regularizer=l2(l2_reg_value), kernel_initializer=he_normal()))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(dropout_rate))\n",
    "\n",
    "    # Hidden layers with He initialization\n",
    "    for num_neurons in neurons_per_layer[1:]:\n",
    "        model.add(Dense(num_neurons, activation='relu', kernel_regularizer=l2(l2_reg_value), kernel_initializer=he_normal()))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Dropout(dropout_rate))\n",
    "\n",
    "    # Output layer\n",
    "    model.add(Dense(count_classes, activation='softmax'))\n",
    "\n",
    "    # Adjusted learning rate for Adam optimizer\n",
    "    opt = keras.optimizers.Adam(learning_rate=0.001)\n",
    "    model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    # Callbacks\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=10, verbose=1, restore_best_weights=True)\n",
    "    reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, verbose=1, min_delta=0.0001)\n",
    "    model_checkpoint = ModelCheckpoint('best_model.h5', monitor='val_loss', mode='min', save_best_only=True, verbose=1)\n",
    "\n",
    "    history = model.fit(X_train, y_train, epochs=epochs, validation_data=(X_val, y_val), callbacks=[early_stopping, reduce_lr, model_checkpoint])\n",
    "\n",
    "    # Evaluate the model on training and test data\n",
    "    scores_train = model.evaluate(X_train, y_train, verbose=1)\n",
    "    scores_test = model.evaluate(X_test, y_test, verbose=0)\n",
    "\n",
    "\n",
    "    print('Accuracy on training data: {:.2f}%\\nError on training data: {:.2f}'.format(scores_train[1] * 100, (1 - scores_train[1]) * 100))\n",
    "    print('Accuracy on test data: {:.2f}%\\nError on test data: {:.2f}'.format(scores_test[1] * 100, (1 - scores_test[1]) * 100))\n",
    "\n",
    "    # Access validation scores from the history object\n",
    "    val_loss = history.history['val_loss']\n",
    "    val_accuracy = history.history['val_accuracy']\n",
    "\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_pred_classes = np.argmax(y_pred, axis=1)  # Convert one-hot encoded predictions to class labels\n",
    "\n",
    "    # Convert one-hot encoded ground truth labels to class labels\n",
    "    y_true_classes = np.argmax(y_test, axis=1)\n",
    "\n",
    "    # Calculate accuracy, precision, recall, and F1 score\n",
    "    accuracy = accuracy_score(y_true_classes, y_pred_classes)\n",
    "    precision = precision_score(y_true_classes, y_pred_classes, average='weighted')\n",
    "    recall = recall_score(y_true_classes, y_pred_classes, average='weighted')\n",
    "    f1 = f1_score(y_true_classes, y_pred_classes, average='weighted')\n",
    "\n",
    "    return {\n",
    "        'val_loss': val_loss,\n",
    "        'val_accuracy': val_accuracy,\n",
    "        'train_loss': history.history['loss'],\n",
    "        'train_accuracy': history.history['accuracy'],\n",
    "        'test_loss': scores_test[0],\n",
    "        'test_accuracy': scores_test[1],\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1_score': f1,\n",
    "        'model': model,\n",
    "        'history': history\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fbb8832d",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_directory2 = \"saved_files/assignment1_saved_results\"\n",
    "\n",
    "def save_model_params(output, base_filename):\n",
    "    # If the output contains a Keras model\n",
    "    if 'best_model' in output and hasattr(output['best_model'], 'save_weights'):\n",
    "        print(\"Saving Keras model architecture and weights...\")\n",
    "\n",
    "        # Save the entire model (architecture + weights)\n",
    "        output['best_model'].save(base_filename + '_full_model.h5')\n",
    "        output['best_model'] = \"MODEL_SAVED_SEPARATELY\"\n",
    "    else:\n",
    "        print(\"No Keras model detected or model does not support weight saving.\")\n",
    "\n",
    "    # Serialize the modified dictionary\n",
    "    with open(base_filename + '.pkl', 'wb') as file:\n",
    "        pickle.dump(output, file)\n",
    "    print(f\"Data saved to {base_filename}.pkl\")\n",
    "\n",
    "def load_model_params(base_filename, model_architecture_func=None):\n",
    "    # Deserialize the dictionary\n",
    "    with open(base_filename + '.pkl', 'rb') as file:\n",
    "        output = pickle.load(file)\n",
    "\n",
    "    # If there's a placeholder for the Keras model\n",
    "    if 'best_model' in output and output['best_model'] == \"MODEL_SAVED_SEPARATELY\":\n",
    "        print(\"Loading Keras model architecture and weights...\")\n",
    "\n",
    "        # Load the entire model (architecture + weights)\n",
    "        model = tf.keras.models.load_model(base_filename + '_full_model.h5')\n",
    "        output['best_model'] = model\n",
    "    else:\n",
    "        print(\"No placeholder for Keras model detected in the loaded data.\")\n",
    "\n",
    "    return output\n",
    "\n",
    "def save_results(results, save_directory):\n",
    "    if not os.path.exists(save_directory):\n",
    "        os.makedirs(save_directory)\n",
    "\n",
    "    # Save Accuracy Plot\n",
    "    if 'fig_acc' in results:\n",
    "        results['fig_acc'].savefig(os.path.join(save_directory, 'fig_acc.png'))\n",
    "        del results['fig_acc']\n",
    "\n",
    "    # Save Confusion Matrix\n",
    "    if 'fig_cm' in results:\n",
    "        results['fig_cm'].savefig(os.path.join(save_directory, 'fig_cm.png'))\n",
    "        del results['fig_cm']\n",
    "\n",
    "    # Save the rest of the results\n",
    "    with open(os.path.join(save_directory, 'results.pkl'), 'wb') as f:\n",
    "        pickle.dump(results, f)\n",
    "\n",
    "def load_results(load_directory):\n",
    "    results = {}\n",
    "\n",
    "    # Load Accuracy Plot\n",
    "    if os.path.exists(os.path.join(load_directory, 'fig_acc.png')):\n",
    "        results['fig_acc'] = os.path.join(load_directory, 'fig_acc.png')\n",
    "\n",
    "    # Load Confusion Matrix\n",
    "    if os.path.exists(os.path.join(load_directory, 'fig_cm.png')):\n",
    "        results['fig_cm'] = os.path.join(load_directory, 'fig_cm.png')\n",
    "\n",
    "    # Load the rest of the results\n",
    "    if os.path.exists(os.path.join(load_directory, 'results.pkl')):\n",
    "        with open(os.path.join(load_directory, 'results.pkl'), 'rb') as f:\n",
    "            data = pickle.load(f)\n",
    "        results.update(data)\n",
    "\n",
    "    return results\n",
    "\n",
    "# Helper function to style DataFrames\n",
    "def style_df(df):\n",
    "    numeric_cols = df.select_dtypes(include=['number']).columns\n",
    "    format_dict = {col: \"{:.3f}\" for col in numeric_cols}\n",
    "    return df.style.set_table_styles({\n",
    "        '': [{'selector': '',\n",
    "              'props': [('border', '1px solid black')]}]\n",
    "    }).format(format_dict).hide()\n",
    "\n",
    "# Mapping for column names\n",
    "column_mapping = {\n",
    "    'train_accuracy': 'Training Accuracy',\n",
    "    'val_accuracy': 'Validation Accuracy',\n",
    "    'test_accuracy': 'Test Accuracy',\n",
    "    'Hyperparameter Value': 'Hyperparameter Value'\n",
    "}\n",
    "\n",
    "def display_accuracy_plot(loaded_results):\n",
    "    if 'fig_acc' in loaded_results:\n",
    "        display(Image(filename=loaded_results['fig_acc']))\n",
    "\n",
    "def display_results_table(loaded_results):\n",
    "    if 'results_table' in loaded_results:\n",
    "        df = loaded_results['results_table'].rename(columns=column_mapping)\n",
    "        display(style_df(df))\n",
    "\n",
    "def display_table_nn(loaded_results):\n",
    "    if 'table_nn' in loaded_results:\n",
    "        df = loaded_results['table_nn'].rename(columns=column_mapping)\n",
    "        display(style_df(df))\n",
    "\n",
    "def display_table_others(loaded_results):\n",
    "    if 'table_others' in loaded_results:\n",
    "        table = loaded_results['table_others'].rename(columns=column_mapping)\n",
    "        # Extracting best row based on highest validation accuracy\n",
    "        best_row = table[table['Validation Accuracy'] == table['Validation Accuracy'].max()]\n",
    "        \n",
    "        # Creating a dataframe for neat display\n",
    "        best_df = pd.DataFrame({\n",
    "            'Hyperparameter Value': best_row.iloc[0, 0],\n",
    "            'Training Accuracy': best_row['Training Accuracy'].values,\n",
    "            'Validation Accuracy': best_row['Validation Accuracy'].values,\n",
    "            'Test Accuracy': best_row['Test Accuracy'].values\n",
    "        })\n",
    "        \n",
    "        display(style_df(best_df))\n",
    "        display(style_df(table))\n",
    "\n",
    "def display_confusion_matrix(loaded_results):\n",
    "    if 'fig_cm' in loaded_results:\n",
    "        display(Image(filename=loaded_results['fig_cm']))\n",
    "\n",
    "def display_test_classification_report(loaded_results):\n",
    "     # Extracting and formatting values from the loaded_results dictionary\n",
    "    precision_val = \"{:.3f}\".format(loaded_results['test_report']['precision']).rstrip('0').rstrip('.')\n",
    "    recall_val = \"{:.3f}\".format(loaded_results['test_report']['recall']).rstrip('0').rstrip('.')\n",
    "    f1_score_val = \"{:.3f}\".format(loaded_results['test_report']['f1_score']).rstrip('0').rstrip('.')\n",
    "\n",
    "    # Creating a 2D array with headers and values\n",
    "    data = [\n",
    "        ['Metric', 'Value'],\n",
    "        ['Precision', precision_val],\n",
    "        ['Recall', recall_val],\n",
    "        ['F1 Score', f1_score_val]\n",
    "    ]\n",
    "\n",
    "    # Creating the DataFrame without index and displaying it\n",
    "    manual_df = pd.DataFrame(data[1:], columns=data[0])\n",
    "    display(manual_df.style.hide())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ffd425e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The data prep function is called after preparing the data according to BoW, TF-IDF and token embeddings.\n",
    "def data_prep(x,y):\n",
    "    # First, split into train and a temporary set (test + validation) using stratification\n",
    "    X_train, X_temp, y_train, y_temp = train_test_split(x, y, test_size=0.3, stratify=y, random_state=seed)\n",
    "\n",
    "    # Split the temporary set into test and validation sets, again using stratification\n",
    "    X_test, X_val, y_test, y_val = train_test_split(X_temp, y_temp, test_size=0.5, stratify=y_temp, random_state=seed)\n",
    "\n",
    "    X_train = X_train.values\n",
    "    X_val = X_val.values\n",
    "    X_test = X_test.values\n",
    "\n",
    "    label_encoder = LabelEncoder()\n",
    "\n",
    "    # Fit and transform the labels to integer labels\n",
    "    y_train_encoded = label_encoder.fit_transform(y_train)\n",
    "    y_val_encoded = label_encoder.transform(y_val)  # Use transform, not fit_transform\n",
    "    y_test_encoded = label_encoder.transform(y_test)  # Use transform, not fit_transform\n",
    "\n",
    "    # Convert to one-hot encoded vector\n",
    "    y_train = to_categorical(y_train_encoded)\n",
    "    y_val = to_categorical(y_val_encoded)\n",
    "    y_test = to_categorical(y_test_encoded)\n",
    "\n",
    "    # dimensions\n",
    "    inp_dim = X_test.shape[1]\n",
    "    count_classes = y_test.shape[1]\n",
    "\n",
    "    return {\n",
    "        'X_train': X_train,\n",
    "        'X_val': X_val,\n",
    "        'X_test': X_test,\n",
    "        'y_train': y_train,\n",
    "        'y_val': y_val,\n",
    "        'y_test': y_test,\n",
    "        'inp_dim': inp_dim,\n",
    "        'count_classes': count_classes\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3e34f25d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(data_preparation_func, x, y):\n",
    "    data = data_preparation_func(x, y)\n",
    "    return data\n",
    "\n",
    "\n",
    "def hyperparameter_search_nn(seed, neural_net_func, X_train, y_train, X_val, y_val, X_test, y_test):\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    neurons_space = [[800, 300], [800, 300, 100]]\n",
    "    l2_reg_space = [0, 0.001, 0.01]\n",
    "    dropout_space = [0.2, 0.4]\n",
    "    num_epochs_space = [20]\n",
    "\n",
    "    best_val_accuracy = 0\n",
    "    best_params = {}\n",
    "    best_model = None\n",
    "    best_train_accuracies = []\n",
    "    best_val_accuracies = []\n",
    "\n",
    "    results_list = []\n",
    "\n",
    "    for neurons in neurons_space:\n",
    "        for l2_reg in l2_reg_space:\n",
    "            for dropout_rate in dropout_space:\n",
    "                for epochs in num_epochs_space:\n",
    "                    print(f\"Training with: neurons={neurons}, l2_reg={l2_reg}, dropout={dropout_rate}, epochs={epochs}\")\n",
    "\n",
    "                    results = neural_net_func(neurons_per_layer=neurons,\n",
    "                                              l2_reg_value=l2_reg,\n",
    "                                              epochs=epochs,\n",
    "                                              dropout_rate=dropout_rate)\n",
    "\n",
    "                    results_list.append({\n",
    "                        'neurons': str(neurons),\n",
    "                        'l2_reg': l2_reg,\n",
    "                        'dropout_rate': dropout_rate,\n",
    "                        'epochs': epochs,\n",
    "                        'train_accuracy': results['train_accuracy'][-1],\n",
    "                        'val_accuracy': results['val_accuracy'][-1],\n",
    "                        'test_accuracy': results['test_accuracy']\n",
    "                    })\n",
    "\n",
    "                    if results['val_accuracy'][-1] > best_val_accuracy:\n",
    "                        best_val_accuracy = results['val_accuracy'][-1]\n",
    "                        best_model = results['model']\n",
    "                        history = results['history']\n",
    "                        best_params = {\n",
    "                            'neurons': neurons,\n",
    "                            'l2_reg': l2_reg,\n",
    "                            'dropout_rate': dropout_rate,\n",
    "                            'epochs': epochs,\n",
    "                            'val_accuracy': best_val_accuracy\n",
    "                        }\n",
    "                        best_train_accuracies = results['train_accuracy']\n",
    "                        best_val_accuracies = results['val_accuracy']\n",
    "\n",
    "    results_df = pd.DataFrame(results_list)\n",
    "\n",
    "    output = {\n",
    "        'best_train_accuracies': best_train_accuracies,\n",
    "        'best_val_accuracies': best_val_accuracies,\n",
    "        'best_model': best_model,\n",
    "        'results_df': results_df,\n",
    "        'history': history\n",
    "    }\n",
    "\n",
    "    return output\n",
    "\n",
    "def train_evaluate_svm(X_train, y_train, X_val, y_val, X_test, y_test, seed):\n",
    "    scaler = cuml.preprocessing.StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_val_scaled = scaler.transform(X_val)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "    y_train_int = np.argmax(y_train, axis=1)\n",
    "    y_val_int = np.argmax(y_val, axis=1)\n",
    "    y_test_int = np.argmax(y_test, axis=1)\n",
    "\n",
    "    best_C = 1\n",
    "    best_val_accuracy = 0\n",
    "    C_values = [0.001, 0.01, 0.1, 0.5, 1, 10, 100]\n",
    "    results_list = []\n",
    "\n",
    "    for C in C_values:\n",
    "        svm_clf = cuml.svm.SVC(C=C, class_weight='balanced', random_state=seed, max_iter=1000)\n",
    "        svm_clf.fit(X_train_scaled, y_train_int)\n",
    "        y_val_pred = svm_clf.predict(X_val_scaled)\n",
    "        val_accuracy = cuml.metrics.accuracy_score(y_val_int, y_val_pred)\n",
    "\n",
    "        results_list.append({\n",
    "            'C': C,\n",
    "            'train_accuracy': cuml.metrics.accuracy_score(y_train_int, svm_clf.predict(X_train_scaled)),\n",
    "            'val_accuracy': val_accuracy,\n",
    "            'test_accuracy': cuml.metrics.accuracy_score(y_test_int, svm_clf.predict(X_test_scaled))\n",
    "        })\n",
    "\n",
    "        if val_accuracy > best_val_accuracy:\n",
    "            best_val_accuracy = val_accuracy\n",
    "            best_C = C\n",
    "\n",
    "    best_svm_clf = cuml.svm.SVC(C=best_C, class_weight='balanced', random_state=seed, max_iter=1000)\n",
    "    best_svm_clf.fit(X_train_scaled, y_train_int)\n",
    "\n",
    "    results_df = pd.DataFrame(results_list)\n",
    "\n",
    "    output = {\n",
    "        'best_model': best_svm_clf,\n",
    "        'results_df': results_df\n",
    "    }\n",
    "\n",
    "    return output\n",
    "\n",
    "def train_evaluate_nb(X_train, y_train, X_val, y_val, X_test, y_test):\n",
    "    param_grid = {'alpha': [0.001, 0.01, 0.1, 1.0, 10.0]}\n",
    "    results_list = []\n",
    "\n",
    "    for alpha in param_grid['alpha']:\n",
    "        nb_clf = MultinomialNB(alpha=alpha)\n",
    "        nb_clf.fit(X_train, np.argmax(y_train, axis=1))\n",
    "        \n",
    "        results_list.append({\n",
    "            'alpha': alpha,\n",
    "            'train_accuracy': accuracy_score(np.argmax(y_train, axis=1), nb_clf.predict(X_train)),\n",
    "            'val_accuracy': accuracy_score(np.argmax(y_val, axis=1), nb_clf.predict(X_val)),\n",
    "            'test_accuracy': accuracy_score(np.argmax(y_test, axis=1), nb_clf.predict(X_test))\n",
    "        })\n",
    "\n",
    "    results_df = pd.DataFrame(results_list)\n",
    "    best_alpha = results_df.loc[results_df['val_accuracy'].idxmax(), 'alpha']\n",
    "    best_nb_clf = MultinomialNB(alpha=best_alpha)\n",
    "    best_nb_clf.fit(X_train, np.argmax(y_train, axis=1))\n",
    "\n",
    "    output = {\n",
    "        'best_model': best_nb_clf,\n",
    "        'results_df': results_df\n",
    "    }\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0514bdea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_accuracies(title, x_label, x_data, train_accuracies, val_accuracies):\n",
    "    \"\"\"\n",
    "    Plots training and validation accuracies.\n",
    "\n",
    "    Parameters:\n",
    "        - title (str): Title for the plot.\n",
    "        - x_label (str): X-axis label.\n",
    "        - x_data (list): X-axis data.\n",
    "        - train_accuracies (list): Training accuracies.\n",
    "        - val_accuracies (list): Validation accuracies.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(x_data, train_accuracies, '-o', label='Training Accuracy', color='blue')\n",
    "    plt.plot(x_data, val_accuracies, '-o', label='Validation Accuracy', color='red')\n",
    "    plt.title(title)\n",
    "    plt.xlabel(x_label)\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    return plt.gcf()\n",
    "\n",
    "def display_results_table(df):\n",
    "    \"\"\"\n",
    "    Displays the results DataFrame as a table.\n",
    "\n",
    "    Parameters:\n",
    "        - df (DataFrame): Results DataFrame.\n",
    "    \"\"\"\n",
    "    display(df)\n",
    "\n",
    "\n",
    "def plot_confusion_matrix(y_true, y_pred, title, class_labels=None):\n",
    "    \"\"\"\n",
    "    Plots a confusion matrix using seaborn's heatmap() with adjustments for label formats.\n",
    "\n",
    "    Parameters:\n",
    "        - y_true (array-like): True labels.\n",
    "        - y_pred (array-like): Predicted labels.\n",
    "        - title (str): Title for the heatmap.\n",
    "        - class_labels (list, optional): List of class labels.\n",
    "    \"\"\"\n",
    "    # Convert one-hot encoded labels to label encoded format if necessary\n",
    "    if len(y_true.shape) > 1 and y_true.shape[1] > 1:\n",
    "        y_true = np.argmax(y_true, axis=1)\n",
    "    if len(y_pred.shape) > 1 and y_pred.shape[1] > 1:\n",
    "        y_pred = np.argmax(y_pred, axis=1)\n",
    "\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=class_labels)\n",
    "    plt.figure(figsize=(10, 7))\n",
    "    sns.heatmap(cm, annot=True, fmt='g', cmap='Blues', cbar=False)\n",
    "    plt.xlabel('Predicted Labels')\n",
    "    plt.ylabel('True Labels')\n",
    "    plt.title(title)\n",
    "    return plt.gcf()\n",
    "\n",
    "\n",
    "def visualize_and_verify_results(output, X_val, y_val, X_test, y_test, classifier_type, class_labels=None):\n",
    "    \"\"\"\n",
    "    Extended visualization and verification function to handle SVM and NB predictions.\n",
    "\n",
    "    Parameters:\n",
    "        - output (dict): Output from the tuning methods.\n",
    "        - X_val (array-like): Validation features.\n",
    "        - y_val (array-like): Validation targets.\n",
    "        - X_test (array-like): Test features.\n",
    "        - y_test (array-like): Test targets.\n",
    "        - classifier_type (str): One of 'nn', 'svm', or 'nb'.\n",
    "        - class_labels (list, optional): List of class labels.\n",
    "\n",
    "    Returns:\n",
    "        results (dict): Dictionary containing plots, tables, and reports.\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "\n",
    "    # 1. Plot accuracies\n",
    "    if classifier_type == 'nn':\n",
    "        fig_acc = plot_accuracies(\"Neural Network Accuracies\",\n",
    "                                  \"Epochs\",\n",
    "                                  list(range(1, len(output['best_train_accuracies']) + 1)),\n",
    "                                  output['best_train_accuracies'],\n",
    "                                  output['best_val_accuracies'])\n",
    "        results['fig_acc'] = fig_acc\n",
    "    elif classifier_type in ['svm', 'nb']:\n",
    "        param_key = 'C' if classifier_type == 'svm' else 'alpha'\n",
    "        fig_acc = plot_accuracies(f\"{classifier_type.upper()} Accuracies\",\n",
    "                                  param_key,\n",
    "                                  output['results_df'][param_key],\n",
    "                                  output['results_df']['train_accuracy'],\n",
    "                                  output['results_df']['val_accuracy'])\n",
    "        results['fig_acc'] = fig_acc\n",
    "\n",
    "    # 2. Display results table\n",
    "    if classifier_type == 'nn':\n",
    "        metrics = {\n",
    "            'train_accuracy': output['best_train_accuracies'][-1],\n",
    "            'val_accuracy': output['best_val_accuracies'][-1]\n",
    "        }\n",
    "        results['table_nn'] = pd.DataFrame([metrics])\n",
    "    else:\n",
    "        results['table_others'] = output['results_df']\n",
    "\n",
    "    # 3. Plot confusion matrices and generate reports\n",
    "    if 'best_model' in output:\n",
    "        if classifier_type == 'nn':\n",
    "            y_pred = output['best_model'].predict(X_test)\n",
    "            y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "        else:\n",
    "            y_pred_classes = output['best_model'].predict(X_test)\n",
    "\n",
    "        y_true_classes = np.argmax(y_test, axis=1) if len(y_test.shape) > 1 and y_test.shape[1] > 1 else y_test\n",
    "        fig_cm = plot_confusion_matrix(y_true_classes,\n",
    "                                       y_pred_classes,\n",
    "                                       f\"Test Confusion Matrix - {classifier_type.upper()}\",\n",
    "                                       class_labels)\n",
    "        results['fig_cm'] = fig_cm\n",
    "\n",
    "        # 4. Classification reports\n",
    "        precision = precision_score(y_true_classes, y_pred_classes, average='weighted')\n",
    "        recall = recall_score(y_true_classes, y_pred_classes, average='weighted')\n",
    "        f1 = f1_score(y_true_classes, y_pred_classes, average='weighted')\n",
    "\n",
    "        results['test_report'] = {\n",
    "            'precision': precision,\n",
    "            'recall': recall,\n",
    "            'f1_score': f1\n",
    "        }\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1e3f633b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bow_x(data):\n",
    "    # Extract relevant columns\n",
    "    text_data = data['sentence']\n",
    "    y = data['president']\n",
    "\n",
    "    # Initialize a CountVectorizer for BOW representation\n",
    "    vectorizer = CountVectorizer(lowercase=True, token_pattern=r\"(?u)\\b\\w+\\b\", stop_words='english')\n",
    "\n",
    "    # Fit and transform the text data\n",
    "    X = vectorizer.fit_transform(text_data)\n",
    "\n",
    "    # Create a DataFrame from the BOW representation\n",
    "    bow_df = pd.DataFrame(X.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "\n",
    "    return bow_df\n",
    "\n",
    "\n",
    "def tf_idf(df):\n",
    "    sentences = df['sentence'].tolist()\n",
    "\n",
    "    # Create a TfidfVectorizer with stop words removal\n",
    "    tfidf_vectorizer = TfidfVectorizer(stop_words='english')\n",
    "\n",
    "    # Fit and transform the sentences to compute TF-IDF values\n",
    "    tfidf_matrix = tfidf_vectorizer.fit_transform(sentences)\n",
    "\n",
    "    # Create a new dataframe with TF-IDF values\n",
    "    tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=tfidf_vectorizer.get_feature_names_out())\n",
    "\n",
    "    return tfidf_df\n",
    "\n",
    "def tokenize_text(text_data, labels, max_features=10000, maxlen=100):\n",
    "    # Tokenization\n",
    "    tokenizer = Tokenizer(num_words=max_features)\n",
    "    tokenizer.fit_on_texts(text_data)\n",
    "    sequences = tokenizer.texts_to_sequences(text_data)\n",
    "\n",
    "    # Filter out sequences that have length 0\n",
    "    seq_ok = [i for i, s in enumerate(sequences) if len(s) > 0]\n",
    "    valid_labels = [labels.iloc[i] for i in seq_ok]\n",
    "\n",
    "    # Padding sequences\n",
    "    filtered_sequences = [sequences[i] for i in seq_ok]\n",
    "    x_pad = pad_sequences(filtered_sequences, maxlen=maxlen)\n",
    "\n",
    "\n",
    "    return x_pad, valid_labels, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9e62dd8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"data/sentences.csv\"\n",
    "sentence_data = pd.read_csv(data_path)\n",
    "\n",
    "# Drop rows where 'sentence' is NaN\n",
    "sentence_data = sentence_data.dropna(subset=['sentence'])\n",
    "\n",
    "# Get the names of the two presidents with the lowest sentence counts\n",
    "remove_presidents = sentence_data['president'].value_counts().tail(2).index.tolist()\n",
    "\n",
    "# Filter the data to exclude sentences from these two presidents\n",
    "sentence_data = sentence_data[~sentence_data['president'].isin(remove_presidents)]\n",
    "\n",
    "# Set the seed\n",
    "seed = 1\n",
    "\n",
    "# Set the save directory of the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5fafac6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_directory = \"saved_files/assignment1_savedvals\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "725a7159",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = bow_x(sentence_data)\n",
    "y = sentence_data['president']\n",
    "\n",
    "# Prepare data\n",
    "data = prepare_data(data_prep, x, y)\n",
    "X_train, y_train, X_val, y_val, X_test, y_test, inp_dim, count_classes = data['X_train'], data['y_train'], data['X_val'], data['y_val'], data['X_test'], data['y_test'], data['inp_dim'], data['count_classes']\n",
    "\n",
    "# Scaled data for SVM\n",
    "# scaler = cuml.preprocessing.StandardScaler()\n",
    "# X_train_scaled = scaler.fit_transform(X_train)\n",
    "# X_val_scaled = scaler.transform(X_val)\n",
    "# X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# y_train_int = np.argmax(y_train, axis=1)\n",
    "# y_val_int = np.argmax(y_val, axis=1)\n",
    "# y_test_int = np.argmax(y_test, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6f288086",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with tf.device('/device:GPU:0'):\n",
    "#  nn_bow_tune = hyperparameter_search_nn(seed,\n",
    "#                                        neural_net,\n",
    "#                                        X_train, y_train,\n",
    "#                                        X_val, y_val,\n",
    "#                                        X_test, y_test)\n",
    "\n",
    "# # Saving the model parameters\n",
    "# save_model_params(nn_bow_tune, os.path.join(save_directory, 'nn_bow_tune'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b94dbcfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the tuned output\n",
    "# nn_bow_tuned = load_model_params(os.path.join(save_directory, 'nn_bow_tune'))\n",
    "\n",
    "# visualize_and_verify_results(nn_bow_tuned, X_val, y_val, X_test, y_test, 'nn', class_labels=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "abcbe7ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = save_directory2 + \"/bow_nn_results\"\n",
    "# save_results(bow_nn_results, save_path)\n",
    "loaded_results = load_results(save_path)\n",
    "\n",
    "#display_loaded_results(loaded_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "nn-bow-accuracy-plot",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| label: nn bow accuracy plot\n",
    "#| fig-cap: An accuracy plot for the neural network model trained on the Bag of Words representation.\n",
    "\n",
    "display_accuracy_plot(loaded_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "nn-bow-nn-accuracies-for-the-best-fitted-model",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| label: nn bow nn accuracies for the best fitted model\n",
    "#| tbl-cap: A table of accuracies for the neural network model trained on the Bag of Words representation.\n",
    "\n",
    "display_table_nn(loaded_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "nn-bow-table",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| label: nn bow table\n",
    "#| tbl-cap: A table of results for the neural network model trained on the Bag of Words representation.\n",
    "\n",
    "display_table_others(loaded_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "nn-bow-confusion-matrix",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| label: nn bow confusion matrix\n",
    "#| fig-cap: A confusion matrix for the neural network model trained on the Bag of Words representation.\n",
    "\n",
    "display_confusion_matrix(loaded_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "nn-bow-test-classification-report",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| label: nn bow test classification report\n",
    "#| tbl-cap: A table of test classification results for the neural network model trained on the Bag of Words representation.\n",
    "\n",
    "display_test_classification_report(loaded_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ad0f73d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# svm_results_bow = train_evaluate_svm(X_train, y_train, X_val, y_val, X_test, y_test, seed)\n",
    "\n",
    "# # Save the model parameters\n",
    "# save_model_params(svm_results_bow, os.path.join(save_directory, 'svm_bow_results'))\n",
    "\n",
    "# # Load the saved model parameters\n",
    "# svm_results_bow = load_model_params(os.path.join(save_directory, 'svm_bow_results'))\n",
    "\n",
    "# results = visualize_and_verify_results(svm_results_bow, X_val_scaled, y_val_int, X_test_scaled, y_test_int, 'svm', class_labels=None)\n",
    "\n",
    "# Save and load the results for later\n",
    "\n",
    "save_path = save_directory2 + \"/bow_svm_results\"\n",
    "# save_results(results, save_path)\n",
    "loaded_results = load_results(save_path)\n",
    "#display_loaded_results(loaded_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "svm-bow-accuracy-plot",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| label: svm bow accuracy plot\n",
    "#| fig-cap: An accuracy plot for the SVM model trained on the Bag of Words representation.\n",
    "\n",
    "display_accuracy_plot(loaded_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "svm-bow-tuning-table",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| label: svm bow tuning table\n",
    "#| tbl-cap: 'A table of hyperparameter tuning results for the SVM model trained on the Bag of Words representation, with the best hyperparameters highlighted above.'\n",
    "\n",
    "display_table_others(loaded_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "svm-bow-confusion-matrix",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| label: svm bow confusion matrix\n",
    "#| fig-cap: A confusion matrix for the SVM model trained on the Bag of Words representation.\n",
    "\n",
    "display_confusion_matrix(loaded_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "svm-bow-test-classification-report",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| label: svm bow test classification report\n",
    "#| tbl-cap: A table of test classification results for the SVM model trained on the Bag of Words representation.\n",
    "\n",
    "display_test_classification_report(loaded_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c901a841",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nb_results_bow = train_evaluate_nb(X_train, y_train, X_val, y_val, X_test, y_test)\n",
    "\n",
    "# # Save the model parameters\n",
    "# save_model_params(nb_results_bow, os.path.join(save_directory, 'nb_bow_results'))\n",
    "\n",
    "# # Load the saved model parameters\n",
    "# nb_results_bow = load_model_params(os.path.join(save_directory, 'nb_bow_results'))\n",
    "\n",
    "# results = visualize_and_verify_results(nb_results_bow, X_val, y_val, X_test, y_test, 'nb', class_labels=None)\n",
    "\n",
    "# Save and load the results for later\n",
    "\n",
    "save_path = save_directory2 + \"/bow_nb_results\"\n",
    "# save_results(results, save_path)\n",
    "loaded_results = load_results(save_path)\n",
    "#display_loaded_results(loaded_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "nb-bow-accuracy-plot",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| label: nb bow accuracy plot\n",
    "#| fig-cap: An accuracy plot for the NB model trained on the Bag of Words representation.\n",
    "\n",
    "display_accuracy_plot(loaded_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "nb-bow-tuning-table",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| label: nb bow tuning table\n",
    "#| tbl-cap: 'A table of hyperparameter tuning results for the NB model trained on the Bag of Words representation, with the best hyperparameters highlighted above.'\n",
    "\n",
    "display_table_others(loaded_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "nb-bow-confusion-matrix",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| label: nb bow confusion matrix\n",
    "#| fig-cap: A confusion matrix for the NB model trained on the Bag of Words representation.\n",
    "\n",
    "display_confusion_matrix(loaded_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "nb-bow-test-classification-report",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| label: nb bow test classification report\n",
    "#| tbl-cap: A table of test classification results for the NB model trained on the Bag of Words representation.\n",
    "\n",
    "display_test_classification_report(loaded_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "37cbc864",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import idf data\n",
    "x = tf_idf(sentence_data)\n",
    "y = sentence_data['president']\n",
    "\n",
    "# Prepare the data\n",
    "data = prepare_data(data_prep, x, y)\n",
    "X_train, y_train, X_val, y_val, X_test, y_test, inp_dim, count_classes = data['X_train'], data['y_train'], data['X_val'], data['y_val'], data['X_test'], data['y_test'], data['inp_dim'], data['count_classes']\n",
    "\n",
    "\n",
    "# Scaled data for SVM\n",
    "# scaler = cuml.preprocessing.StandardScaler()\n",
    "# X_train_scaled = scaler.fit_transform(X_train)\n",
    "# X_val_scaled = scaler.transform(X_val)\n",
    "# X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# y_train_int = np.argmax(y_train, axis=1)\n",
    "# y_val_int = np.argmax(y_val, axis=1)\n",
    "# y_test_int = np.argmax(y_test, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ee30ec5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with tf.device('/device:GPU:0'):\n",
    "#   nn_tf_tune = hyperparameter_search_nn(seed,\n",
    "#                                         neural_net,\n",
    "#                                         X_train, y_train,\n",
    "#                                         X_val, y_val,\n",
    "#                                         X_test, y_test)\n",
    "\n",
    "# # Saving the model parameters\n",
    "# save_model_params(nn_tf_tune, os.path.join(save_directory, 'nn_tf_results'))\n",
    "\n",
    "# # Load the tuned output\n",
    "# nn_tf_tune = load_model_params(os.path.join(save_directory, 'nn_tf_results'))\n",
    "\n",
    "# results = visualize_and_verify_results(nn_tf_tune, X_val, y_val, X_test, y_test, 'nn', class_labels=None)\n",
    "\n",
    "# Save and load the results for later\n",
    "\n",
    "save_path = save_directory2 + \"/tfidf_nn_results\"\n",
    "# save_results(results, save_path)\n",
    "loaded_results = load_results(save_path)\n",
    "#display_loaded_results(loaded_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "nn-tf-accuracy-plot",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| label: nn tf accuracy plot\n",
    "#| fig-cap: An accuracy plot for the neural network model trained on the TF-IDF representation.\n",
    "\n",
    "display_accuracy_plot(loaded_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "nn-tf-accuracies-for-the-best-fitted-model",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| label: nn tf accuracies for the best fitted model\n",
    "#| tbl-cap: A table of accuracies for the neural network model trained on the TF-IDF representation.\n",
    "\n",
    "display_table_nn(loaded_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "nn-tf-table",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| label: nn tf table\n",
    "#| tbl-cap: A table of results for the neural network model trained on the TF-IDF representation.\n",
    "\n",
    "display_table_others(loaded_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "nn-tf-confusion-matrix",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| label: nn tf confusion matrix\n",
    "#| fig-cap: A confusion matrix for the neural network model trained on the TF-IDF representation.\n",
    "\n",
    "display_confusion_matrix(loaded_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "nn-tf-test-classification-report",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| label: nn tf test classification report\n",
    "#| tbl-cap: A table of test classification results for the neural network model trained on the TF-IDF representation.\n",
    "\n",
    "display_test_classification_report(loaded_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c2f754df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# svm_results_tf = train_evaluate_svm(X_train, y_train, X_val, y_val, X_test, y_test, seed)\n",
    "\n",
    "# save_model_params(svm_results_tf, os.path.join(save_directory, 'svm_tf_results'))\n",
    "\n",
    "# svm_results_tf = load_model_params(os.path.join(save_directory, 'svm_tf_results'))\n",
    "\n",
    "# results = visualize_and_verify_results(svm_results_tf, X_val_scaled, y_val_int, X_test_scaled, y_test_int, 'svm', class_labels=None)\n",
    "\n",
    "# Save and load the results for later\n",
    "\n",
    "save_path = save_directory2 + \"/tfidf_svm_results\"\n",
    "# save_results(results, save_path)\n",
    "loaded_results = load_results(save_path)\n",
    "#display_loaded_results(loaded_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "svm-tf-accuracy-plot",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| label: svm tf accuracy plot\n",
    "#| fig-cap: An accuracy plot for the SVM model trained on the TF-IDF representation.\n",
    "\n",
    "display_accuracy_plot(loaded_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "svm-tf-tuning-table",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| label: svm tf tuning table\n",
    "#| tbl-cap: 'A table of hyperparameter tuning results for the SVM model trained on the TF-IDF representation, with the best hyperparameters highlighted above.'\n",
    "\n",
    "display_table_others(loaded_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "svm-tf-confusion-matrix",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| label: svm tf confusion matrix\n",
    "#| fig-cap: A confusion matrix for the SVM model trained on the TF-IDF representation.\n",
    "\n",
    "display_confusion_matrix(loaded_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "svm-tf-test-classification-report",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| label: svm tf test classification report\n",
    "#| tbl-cap: A table of test classification results for the SVM model trained on the TF-IDF representation.\n",
    "\n",
    "display_test_classification_report(loaded_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "07f1543e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nb_results_tf = train_evaluate_nb(X_train, y_train, X_val, y_val, X_test, y_test)\n",
    "\n",
    "# save_model_params(nb_results_tf, os.path.join(save_directory, 'nb_tf_results'))\n",
    "\n",
    "# nb_results_tf = load_model_params(os.path.join(save_directory, 'nb_tf_results'))\n",
    "\n",
    "# results = visualize_and_verify_results(nb_results_tf, X_val, y_val, X_test, y_test, 'nb', class_labels=None)\n",
    "\n",
    "# Save and load the results for later\n",
    "\n",
    "save_path = save_directory2 + \"/tfidf_nb_results\"\n",
    "# save_results(results, save_path)\n",
    "loaded_results = load_results(save_path)\n",
    "#display_loaded_results(loaded_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "nb-tf-accuracy-plot",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| label: nb tf accuracy plot\n",
    "#| fig-cap: An accuracy plot for the NB model trained on the TF-IDF representation.\n",
    "\n",
    "display_accuracy_plot(loaded_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "nb-tf-tuning-table",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| label: nb tf tuning table\n",
    "#| tbl-cap: 'A table of hyperparameter tuning results for the NB model trained on the TF-IDF representation, with the best hyperparameters highlighted above.'\n",
    "\n",
    "display_table_others(loaded_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "nb-tf-confusion-matrix",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| label: nb tf confusion matrix\n",
    "#| fig-cap: A confusion matrix for the NB model trained on the TF-IDF representation.\n",
    "\n",
    "display_confusion_matrix(loaded_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "nb-tf-test-classification-report",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| label: nb tf test classification report\n",
    "#| tbl-cap: A table of test classification results for the NB model trained on the TF-IDF representation.\n",
    "\n",
    "display_test_classification_report(loaded_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "70236c4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y, _ = tokenize_text(sentence_data['sentence'], sentence_data['president'])\n",
    "x_df = pd.DataFrame(x)\n",
    "\n",
    "data = prepare_data(data_prep, x_df, y)\n",
    "X_train, y_train, X_val, y_val, X_test, y_test, inp_dim, count_classes = data['X_train'], data['y_train'], data['X_val'], data['y_val'], data['X_test'], data['y_test'], data['inp_dim'], data['count_classes']\n",
    "\n",
    "# Scaled data for SVM\n",
    "# scaler = cuml.preprocessing.StandardScaler()\n",
    "# X_train_scaled = scaler.fit_transform(X_train)\n",
    "# X_val_scaled = scaler.transform(X_val)\n",
    "# X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# y_train_int = np.argmax(y_train, axis=1)\n",
    "# y_val_int = np.argmax(y_val, axis=1)\n",
    "# y_test_int = np.argmax(y_test, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "1efedfca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with tf.device('/device:GPU:0'):\n",
    "#   nn_emb_tune = hyperparameter_search_nn(seed,\n",
    "#                                         neural_net,\n",
    "#                                         X_train, y_train,\n",
    "#                                         X_val, y_val,\n",
    "#                                         X_test, y_test)\n",
    "\n",
    "# # Saving the model parameters\n",
    "# save_model_params(nn_emb_tune, os.path.join(save_directory, 'nn_emb_results'))\n",
    "\n",
    "# # Load the saved model parameters\n",
    "# nn_emb_tune = load_model_params(os.path.join(save_directory, 'nn_emb_results'))\n",
    "\n",
    "# results = visualize_and_verify_results(nn_emb_tune, X_val, y_val, X_test, y_test, 'nn', class_labels=None)\n",
    "\n",
    "# Save and load the results for later\n",
    "\n",
    "save_path = save_directory2 + \"/emb_nn_results\"\n",
    "# save_results(results, save_path)\n",
    "loaded_results = load_results(save_path)\n",
    "#display_loaded_results(loaded_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "nn-emb-accuracy-plot",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| label: nn emb accuracy plot\n",
    "#| fig-cap: An accuracy plot for the neural network model trained on the text embeddings representation.\n",
    "\n",
    "display_accuracy_plot(loaded_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "nn-emb-accuracies-for-the-best-fitted-model",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| label: nn emb accuracies for the best fitted model\n",
    "#| tbl-cap: A table of accuracies for the neural network model trained on the text embeddings representation.\n",
    "\n",
    "display_table_nn(loaded_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "nn-emb-table",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| label: nn emb table\n",
    "#| tbl-cap: A table of results for the neural network model trained on the text embeddings representation.\n",
    "\n",
    "display_table_others(loaded_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "nn-emb-confusion-matrix",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| label: nn emb confusion matrix\n",
    "#| fig-cap: A confusion matrix for the neural network model trained on the text embeddings representation.\n",
    "\n",
    "display_confusion_matrix(loaded_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "nn-emb-test-classification-report",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| label: nn emb test classification report\n",
    "#| tbl-cap: A table of test classification results for the neural network model trained on the text embeddings representation.\n",
    "\n",
    "display_test_classification_report(loaded_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "b3d2adf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# svm_results_emb = train_evaluate_svm(X_train, y_train, X_val, y_val, X_test, y_test, seed)\n",
    "\n",
    "# save_model_params(svm_results_emb, os.path.join(save_directory, 'svm_emb_results'))\n",
    "\n",
    "# svm_results_emb = load_model_params(os.path.join(save_directory, 'svm_emb_results'))\n",
    "\n",
    "# results = visualize_and_verify_results(svm_results_emb, X_val_scaled, y_val_int, X_test_scaled, y_test_int, 'svm', class_labels=None)\n",
    "\n",
    "# Save and load the results for later\n",
    "\n",
    "save_path = save_directory2 + \"/emb_svm_results\"\n",
    "# save_results(results, save_path)\n",
    "loaded_results = load_results(save_path)\n",
    "#display_loaded_results(loaded_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "svm-emb-accuracy-plot",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| label: svm emb accuracy plot\n",
    "#| fig-cap: An accuracy plot for the SVM model trained on the text embedding representation.\n",
    "\n",
    "display_accuracy_plot(loaded_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "svm-emb-tuning-table",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| label: svm emb tuning table\n",
    "#| tbl-cap: 'A table of hyperparameter tuning results for the SVM model trained on the text embedding representation, with the best hyperparameters highlighted below.'\n",
    "\n",
    "display_table_others(loaded_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "svm-emb-confusion-matrix",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| label: svm emb confusion matrix\n",
    "#| fig-cap: A confusion matrix for the SVM model trained on the text embedding representation.\n",
    "\n",
    "display_confusion_matrix(loaded_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "svm-emb-test-classification-report",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| label: svm emb test classification report\n",
    "#| tbl-cap: A table of test classification results for the SVM model trained on the text embedding representation.\n",
    "\n",
    "display_test_classification_report(loaded_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "1bf09bfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nb_results_emb = train_evaluate_nb(X_train, y_train, X_val, y_val, X_test, y_test)\n",
    "\n",
    "# save_model_params(nb_results_emb, os.path.join(save_directory, 'nb_emb_results'))\n",
    "\n",
    "# nb_results_emb = load_model_params(os.path.join(save_directory, 'nb_emb_results'))\n",
    "\n",
    "# results = visualize_and_verify_results(nb_results_emb, X_val, y_val, X_test, y_test, 'nb', class_labels=None)\n",
    "\n",
    "# Save and load the results for later\n",
    "\n",
    "save_path = save_directory2 + \"/emb_nb_results\"\n",
    "# save_results(results, save_path)\n",
    "loaded_results = load_results(save_path)\n",
    "#display_loaded_results(loaded_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "nb-emb-accuracy-plot",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| label: nb emb accuracy plot\n",
    "#| fig-cap: An accuracy plot for the NB model trained on the text embedding representation.\n",
    "\n",
    "display_accuracy_plot(loaded_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "nb-emb-tuning-table",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| label: nb emb tuning table\n",
    "#| tbl-cap: 'A table of hyperparameter tuning results for the NB model trained on the text embedding representation, with the best hyperparameters highlighted above.'\n",
    "\n",
    "display_table_others(loaded_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "nb-emb-confusion-matrix",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| label: nb emb confusion matrix\n",
    "#| fig-cap: A confusion matrix for the NB model trained on the text embedding representation.\n",
    "\n",
    "display_confusion_matrix(loaded_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "nb-emb-test-classification-report",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| label: nb emb test classification report\n",
    "#| tbl-cap: A table of test classification results for the NB model trained on the text embedding representation.\n",
    "\n",
    "display_test_classification_report(loaded_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "317835e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = sentence_data['president'].values\n",
    "\n",
    "# Split dataset into train and a temporary set (70% - 30% split)\n",
    "train_df, temp_df = train_test_split(sentence_data, test_size=0.3, stratify=y, random_state=seed)\n",
    "\n",
    "# Split the temporary set into validation and test sets (50% - 50% split of the 30%)\n",
    "val_df, test_df = train_test_split(temp_df, test_size=0.5, stratify=temp_df['president'], random_state=seed)\n",
    "\n",
    "# Assuming you have a list of unique presidents\n",
    "unique_presidents = sentence_data['president'].unique()\n",
    "\n",
    "# Create a mapping of president names to integer labels\n",
    "label_map = {name: idx for idx, name in enumerate(unique_presidents)}\n",
    "\n",
    "# Integer-encode the labels in the dataframes\n",
    "train_df['president'] = train_df['president'].map(label_map)\n",
    "val_df['president'] = val_df['president'].map(label_map)\n",
    "test_df['president'] = test_df['president'].map(label_map)\n",
    "\n",
    "# Convert pandas DataFrames to TensorFlow datasets\n",
    "def df_to_tfdata(df, shuffle=True, batch_size=32):\n",
    "    ds = tf.data.Dataset.from_tensor_slices((df[\"sentence\"].values, df[\"president\"].values))\n",
    "    if shuffle:\n",
    "        ds = ds.shuffle(buffer_size=len(df))\n",
    "    ds = ds.batch(batch_size)\n",
    "    ds = ds.cache().prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "    return ds\n",
    "\n",
    "train_ds = df_to_tfdata(train_df)\n",
    "val_ds = df_to_tfdata(val_df, shuffle=False)\n",
    "test_ds = df_to_tfdata(test_df, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "7fc8d51b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the saved model\n",
    "# import tensorflow_text as text\n",
    "# saved_model_path = \"saved_files/jared_sentences_bert\"\n",
    "# bert_model = tf.saved_model.load(saved_model_path)\n",
    "\n",
    "# # Define a function to evaluate the model\n",
    "# def evaluate_model(model, dataset):\n",
    "#     predictions = []\n",
    "#     true_labels = []\n",
    "\n",
    "#     for inputs, labels in dataset:\n",
    "#         logits = model(inputs, training=False)  # Forward pass\n",
    "#         predicted_labels = np.argmax(logits, axis=1)\n",
    "#         true_labels.extend(labels.numpy())\n",
    "#         predictions.extend(predicted_labels)\n",
    "\n",
    "#     return true_labels, predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "66206fae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_output(train_true, train_pred, val_true, val_pred, test_true, test_pred, filename):\n",
    "    \"\"\"\n",
    "    Save the evaluation outputs to a file.\n",
    "    \n",
    "    Parameters:\n",
    "    - train_true, train_pred, val_true, val_pred, test_true, test_pred: Outputs to save.\n",
    "    - filename: Path to the file where the outputs will be saved.\n",
    "    \"\"\"\n",
    "    with open(filename, 'wb') as f:\n",
    "        pickle.dump((train_true, train_pred, val_true, val_pred, test_true, test_pred), f)\n",
    "\n",
    "def load_output(filename):\n",
    "    \"\"\"\n",
    "    Load the evaluation outputs from a file.\n",
    "    \n",
    "    Parameters:\n",
    "    - filename: Path to the file from where the outputs will be loaded.\n",
    "    \n",
    "    Returns:\n",
    "    - train_true, train_pred, val_true, val_pred, test_true, test_pred: Loaded outputs.\n",
    "    \"\"\"\n",
    "    with open(filename, 'rb') as f:\n",
    "        train_true, train_pred, val_true, val_pred, test_true, test_pred = pickle.load(f)\n",
    "    return train_true, train_pred, val_true, val_pred, test_true, test_pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "676a144a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Evaluate the model on the training, validation, and test datasets\n",
    "# train_true, train_pred = evaluate_model(bert_model, train_ds)\n",
    "# val_true, val_pred = evaluate_model(bert_model, val_ds)\n",
    "# test_true, test_pred = evaluate_model(bert_model, test_ds)\n",
    "\n",
    "# # Save the outputs\n",
    "# save_output(train_true, train_pred, val_true, val_pred, test_true, test_pred, 'bert_evaluation.pkl')\n",
    "\n",
    "train_true, train_pred, val_true, val_pred, test_true, test_pred = load_output('bert_evaluation.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "36a85a8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the saved image\n",
    "display(Image(filename='saved_files/train_val_curves_bert.png'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "a417b63e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_metrics(y_true, y_pred):\n",
    "    accuracy = np.mean(np.array(y_true) == np.array(y_pred))\n",
    "    \n",
    "    precision = precision_score(y_true, y_pred, average='macro', zero_division=1)\n",
    "    recall = recall_score(y_true, y_pred, average='macro', zero_division=1)\n",
    "    f1 = f1_score(y_true, y_pred, average='macro', zero_division=1)\n",
    "    \n",
    "    metrics = {\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1_score': f1\n",
    "    }\n",
    "    return metrics\n",
    "\n",
    "train_metrics = extract_metrics(train_true, train_pred)\n",
    "val_metrics = extract_metrics(val_true, val_pred)\n",
    "test_metrics = extract_metrics(test_true, test_pred)\n",
    "\n",
    "# Create a DataFrame for a neat table\n",
    "df = pd.DataFrame([train_metrics, val_metrics, test_metrics], \n",
    "                  index=['Training', 'Validation', 'Test'])\n",
    "\n",
    "# Round the values to 3 decimal places for better presentation\n",
    "df = df.round(3)\n",
    "\n",
    "# Display the table\n",
    "display(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "d01b4dc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to plot confusion matrix\n",
    "def plot_confusion_matrix(true, pred, title):\n",
    "    matrix = confusion_matrix(true, pred)\n",
    "    plt.figure(figsize=(10, 7))\n",
    "    sns.heatmap(matrix, annot=True, fmt=\"d\", cmap=\"Blues\",\n",
    "                xticklabels=set(true), yticklabels=set(true))\n",
    "    plt.title(title)\n",
    "    plt.ylabel('True Label')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.show()\n",
    "\n",
    "# Plot confusion matrices\n",
    "plot_confusion_matrix(test_true, test_pred, title=\"Test Data Confusion Matrix\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (scientificProject)",
   "language": "python",
   "name": "scientificproject"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}