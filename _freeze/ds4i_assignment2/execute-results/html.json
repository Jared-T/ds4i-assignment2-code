{
  "hash": "4b9c0afeb95c450202f9e5a7a3f7267d",
  "result": {
    "markdown": "---\ntitle: \"Sentiments and Topics in South African SONA Speeches\"\ntitle-size: small\nsubtitle: \"STA5073Z Data Science for Industry Assignment 2\"\nformat: html\nexecute: \n  echo: false\n  cache: true\n  freeze: auto\nauthor: \n    - \"Jared Tavares (TVRJAR001)\"\n    - \"Heiletjé van Zyl (VZYHEI003)\"\n---\n\n<div style=\"text-align: justify\"> \n\n<h4> Abstract </h4>\n<hr> \n\n<h4> Introduction </h4>\n<hr>\nThe field of Natural Language Processing (NLP) is faceted by techniques tailored for theme tracking and opinion mining which merge part of text analysis. Though, of particular prominence, is the extraction of latent thematic patterns and the establishment of the extent of emotionality expressed in political-based texts. \n\nGiven such political context, it is of specific interest to analyse the  annual State of the Nation Address (SONA) speeches delivered by six different South African presidents (F.W. de Klerk, N.R. Mandela, T.M. Mbeki, K.P. Motlanthe, J.G. Zuma, and M.C. Ramaphosa) ranging over twenty-nine years (from 1994 to 2023). This analysis, descriptive and data-driven in nature, endeavours to examine the content of the SONA speeches in terms of themes via topic modelling (TM) and emotions via sentiment analysis (SentA). Applying a double-bifurcated approach, SentA will be executed within a macro and micro context both at the text (all-presidents versus by-president SONA speeches, respectively) and token (sentences versus words, respectively) level, as shown in @fig-SentA_MacroMicroScheme. This underlying framework is also utilized for TM, with an exception of only employing it within a micro-context at the token level, as seen in @fig-TM_MacroMicroScheme.\n\n![Illustration of how sentA will be implemented within a different-scales-within-different-levels framework for the presidential-SONA-speeech text analysis.](SentA_MacroMicroScheme.png){width=400 height=250 #fig-SentA_MacroMicroScheme}\n\n![Depiction of how TM will be done using a similar approach to sentA, though tokens will only be defined in terms of words (and not also as sentences).](TM_MacroMicroScheme.png){width=400 height=250 #fig-TM_MacroMicroScheme}\n\nThrough such a multi-layered lens, the identification of any trends, both in terms of topics and sentiments, over time at both a large (presidents as a collective) as well as at a small (each president as an individual) scale is attainable. This explicates not only an aggregated perspective of the general political discourse prevailing within South Africa, but also a more niche outlook of the specific rhetoric employed by each of the country's serving presidents during different date periods. \n\nTo achieve all of the above-mentioned, it is first relevant to revise foundational terms and review related literature in context of politics and NLP. All pertinent pre-processing of the political text data is then considered, followed by a discussion delving into the details of each SentA and TM approach applied. Specifically, two different lexicons are leveraged to describe sentiments, whilst five different topic models are tackled to uncover themes within South-African-presidents’ SONA speeches. Ensuing the implementation of these methodologies, the results thereof are detailed in terms insights and interpretations. Thereafter, an overall evaluation of the techniques in terms of efficacy and inadequacy is overviewed. Finally, focal findings are highlighted and potential improvements as part of future research are recommended.  \n\n\n<h4> Literature Review </h4>\n<hr>\n<b><u> SONA  </b></u>\n\nSONA, a pivotal event in the political programme of Parliament, serves as a presidential summary for the South African public. Specifically, the country’s current domestic affairs and international relations are reflected upon, past governmental work is perused, and future plans in terms of policies and civil projects are proposed. Through this address, accountability on the part of government is re-instilled and transparency with the public is re-affirmed on an annual basis, either once (non-election year) or twice (pre-and-post election) [@Muthambi2017].\nThe text analysis of such SONA speeches, via the implementation of TM and SentA, has been previously done for Philippine presidents [@MirandaBringula2021]. Though, it is now of interest to extend such an application to another country, South Africa. \n\n\n<b><u> Topic modelling (TM) </b></u>\n\nTM, an unsupervised learning approach, implicates the identification of underlying abstract themes in some body of text, in the absence of pre-specified labels [@Cho2019]. In general, there are two topic-model assumptions: each document comprises of a mixture of topics and each topic consists of a collection of words [@Zhang2018]. Different types of topic models exist, each with varying complexity in terms of the way in which topics are generated. The simplest one, Latent Semantic Analysis (LSA), has previously been implemented to discover patterns of lexical cohesion in political speech, specifically that of the former Prime Minister of the United Kingdom, Margaret Thatcher [@Klebanov2008]. Improving on LSA methodology, Probabilistic LSA (pLSA) has been implemented in healthcare [@Zhu2014] and educational [@Ming2014] contexts, albeit no application thereof in political science was found. A further sophisticated model, Latent Dirichlet Allocation (LDA), has been used to determine trending topics in  news on governmental YouTube channels [@Subhan2023]. \n\n<b><u> Sentiment analysis (SentA) </b></u>\n\nSentA involves deciphering the intent of words to infer certain emotional dimensions labelled either in polarized (negative/positive) or higher-dimensional terms (niche feelings like joy/sadness). Various unigram lexicons have been derived to such extents. For example, the R-based $\\texttt{nrc}$ lexicon dichotomously classifies words with yes/no labels in categories such as positive, negative, anticipation, anger, and so forth. In contrast, the Python-based $\\texttt{TextBlob}$ lexicon processes textual data in the form of a tuple where a polarity score (ranges between -1 and +1 which relates to negative and positive sentiment, respectively) and a subjectivity score (ranges between 0 and 1 which refers to being very objective or very subjective, respectively) is produced. Using such pre-defined lexicons has been previously utilized to analyze political communication, specifically in terms of campaign polarization, via SentA [@Haselmayer2017]. \n\n\n<h4> Data </h4>\n<hr>\n\n<b><u> Tokenization </b></u>\n\nThe process of tokenization entails breaking up given text into units, referred to as tokens (or terms), which are meaningful for analysis [@Zhang2018]. In this case, these tokens take on different structures, based on either a macro-context (i.e., sentences) or micro-context (i.e., words). At both scales, the way in which these tokens are valued will be varied. The value will either be defined by a bag-of-words (BoW) or term-frequency, inverse-document-frequency (tf-idf) approach. The former way implicates accounting for the number of occurrences of some token in some document. On the other hand, the latter way not only regards the frequency of some token, but also the significance thereof. Thus, tf-idf involves the assignment of some weight to each token in a document which in turn reflects its importance relative to the entire collection of documents (corpus). It then follows that the tf-idf value of a token *t* in a document *d* within a corpus *D* is calculated as the product of two constituents. The first being tf(*t*,*d*) defined as the quotient of the frequency of token *t* in document *d* and the total number of tokens in document *d*, whereas the second is idf(*t*, *D*) denoted by the quotient of the natural logarithm of the total number of documents in corpus *D* and the number of documents containing the token *t* [@SilgeRobinson2017]. \n\n\n<b><u> Number of topics </b></u>\n\nIn order to determine the optimal number of topics, a coherence score is calculated.  This metric measures the ability of a topic model to distinguish well between topics that are semantically interpretable by humans and are not simply statistical-inference artifacts. Hence, the number of topics as well as any other topic-model hyperparameters (like $\\alpha$ and $\\beta$ for LDA) are tuned to values that yield the maximum coherence score, allowing for the most understandable themes. \n\n<!-- Two approaches can be applied to determine the optimum topic number. Within the first approach, each topic can be viewed as a cluster and a metric showing how similar each word is to its own topic (cohesion) compared to other topics (separation) can be computed. The Silhouette Coefficient is such a measure ranging from a value of negative one to positive one. Here, higher positive values are indicative of words being well-compacted within the topic to which it belongs. The second approach involves the calculation of a coherence score. This measures the ability of the topic model to distinguish well between topics that are semantically interpretable by humans and are not simply statistical-inference artifacts. Hence, the number of topics as well as any other topic-model hyperparameters are tuned to values that yield the maximum coherence score, allowing for the most understandable themes. This latter approach will largely be applied in this SONA-speech analysis.  -->\n\n<h4> Methods </h4>\n<hr>\n<h5> Topic modelling </h5>\n\n<b><u> Latent Semantic Analysis (LSA) </b></u> \n\n![Schematic representation of LSA outlining the factorization of the *DTM* matrix.](LSA.png){#fig-LSA}\n\nLSA [@Deerwester1990] is a non-probabilistic, non-generative model where a form of matrix factorization is utilized to uncover few latent topics, capturing meaningful relationships among documents/tokens. As depicted in @fig-LSA, in the first step, a document-term matrix *DTM* is generated from the raw text data by tokenizing *d* documents into *w* words (or sentences), forming the columns and rows respectively. Each row-column entry is either valued via the BoW or tf-idf approach. This *DTM*-matrix, which is often sparse and high-dimensional, is then decomposed via a dimensionality-reduction-technique, namely truncated Singular Value Decomposition (SVD). Consequently, in the second step the *DTM*-matrix becomes the product of three matrices: the topic-word matrix $A_{t*}$ (for the tokens), the topic-prevalence matrix $B_{t*}$ (for the latent semantic factors), and the transposed document-topic matrix $C^{T}_{t*}$ (for the document). Here, *t\\**, the optimal number of topics, is a hyperparameter which is refined at a value (via the coherence-measure approach) that retains the most significant dimensions in the transformed space. In the final step, the text data is then encoded using this top-topic number. \n\nGiven LSA only implicates a *DTM*-matrix, the implementation thereof is generally efficient. Though, with the involvement of truncated SVD, some computational intensity and a lack of quick updates with new, incoming text-data can arise. Additional LSA drawbacks include: the lack of interpretability, the underlying linear-model framework (which results in poor performance on text-data with non-linear dependencies), and the underlying Gaussian assumption for tokens in documents (which may not be an appropriate distribution). \n\n<b><u> Probabilistic Latent Semantic Analysis (pLSA) </b></u>\n\n![Schematic representation of pLSA, where the different-shade-of-blue colours highlight similarities shared with LSA-related matrices shown in Figure 3.](pLSA.png){#fig-pLSA height=250}\n\nInstead of implementing truncated SVD, pLSA [@Hofmann1999] rather utilizes a generative, probabilistic model. Within this framework, a document *d* is first selected with probability P(d). Then given this, a latent topic *t* is present in this selected document *d* and so chosen with probability of P(t|d). Finally, given this chosen topic *t*, a word *w* (or sentence) is generated from it with probability P(w|t), as shown in @fig-pLSA. It is noted that the values of P(d) is determined directly from the corpus *D* which is defined in terms of a *DTM* matrix. In contrast, the probabilities P(t|d) and P(w|t) are parameters modelled as multinomial distributions and iteratively updated via the Expectation-Maximization (EM) algorithm. Direct parallelism between LSA and pLSA can be drawn via the methods’ parameterization, as conveyed via matching colours of the topic-word matrix and P(w|t), the document-topic matrix and P(d|t) as well as the topic-prevalence matrix and P(t) displayed in @fig-LSA and @fig-pLSA, respectively. \n\nDespite pLSA implicitly addressing LSA-related disadvantages, this method still involves two main drawbacks. There is no probability model for the document-topic probabilities P(t|d), resulting in the inability to assign topic mixtures to new, unseen documents not trained on. Model parameters also then increase linearly with the number of documents added, making this method more susceptible to overfitting. \n\n<b><u>  Latent Dirichlet Allocation </b></u>\n\n![Schematic representation of LDA where the dark-blue-shaded block represents observed words.](LDA.png){height=370 #fig-LDA}\n\nLDA is another generative, probabilistic model which can be deemed as a hierarchical Bayesian version of pLSA. Via explicitly defining a generative model for the document-topic probabilities, both the above-mentioned pitfalls of pLSA are improved upon. The number of parameters to estimate drastically decrease and the ability to apply and generalize to new, unseen documents is attainable. As presented in @fig-LDA, the initial steps first involve randomly sampling a document-topic probability distribution $\\theta$ from a Dirichlet (Dir) distribution $\\eta$, followed by randomly sampling a topic-word probability distribution $\\phi$ from another Dirichlet distribution $\\tau$. From the $\\theta$ distribution, a topic *t* is selected by drawing from a multinomial (Mult) distribution (third step) and from the $\\phi$ distribution given said topic *t*, a word *w* (or sentences) is sampled from another multinomial distribution (fourth step). The associated LDA-parameters are then estimated via a variational expectation maximization algorithm or collapsed Gibbs sampling. \n\n\n<b><u>  Correlated Topic Model (CTM) </b></u>\n\n![Schematic representation of CTM where the dark-blue-shaded block represents observed words, whilst the light-grey colour outlines the distinctions from the LDA topic model presented in Figure 5.](CTM.png){height=300 #fig-CTM}\n\nFollowing closely to LDA, the CTM [@LaffertyBlei2005] additionally allows for the ability to model the presence of any correlated topics.  Such topic correlations are introduced via the inclusion of the multivariate normal (MultNorm) distribution with *t* length-vector of means $\\mu$ and *t* $\\times$ *t* covariance matrix $\\Sigma$ where the resulting values are then mapped into probabilities by passing through a logistic (log) transformation. Comparing @fig-LDA and @fig-CTM, the nuance between LDA and CTM is highlighted using a light-grey colour, where the discrepancy in the models come about from replacing the Dirichlet distribution (which involves the implicit assumption of independence across topics) with the logit-normal distribution (which now explicitly enables for topic dependency via a covariance structure) for generating document-topic probabilities. The other generative processes previously outlined for LDA is retained and repeated for CTM. Given this additional model complexity, the more convoluted mean-field variational inference algorithm is employed for CTM-parameter estimation which necessitates many iterations for optimization purposes. CTM is consequently computationally more expensive than LDA. Though, this snag is far outweighed by the procurement of richer topics with overt relationships acknowledged between these. \n\n\n<b><u>  Author Topic Model (ATM) </b></u>\n\n![Schematic representation of ATM where the dark-blue-shaded blocks represents observed words and authors, whilst the light-grey colour highlights the differences compared to the LDA topic model presented in Figure 5.](ATM.png){#fig-ATM height=450}\n\nATM [@RosenZvi2012] extends LDA via the inclusion of authorship information with topics. Again, inspecting @fig-LDA and @fig-ATM, the slight discrepancies between these two models are accentuated with the light-grey colour. Here, for each word *w* in the document *d* an author *a* is sampled uniformly (Uni) at random. Each author is associated with a distribution over topics ($\\Psi$) sampled from a Dirichlet prior $\\alpha$. The resultant mixture weights corresponding to the chosen author are used to select a topic *t*, then a word *w* (or sentence) is generated according to the topic-word distribution $\\phi$ (drawn from another Dirichlet prior $\\beta$) corresponding to that said chosen topic *t*. Therefore, through the estimation of the $\\psi$ and $\\phi$ parameters, not only is information obtained about which topics authors generally relate to, but also a representation of these document contents in terms of these topics, respectively. \n\n<h5> Sentiment analysis </h5>\n\n<b><u>  AFINN </b></u>\n\nThe R-based $\\texttt{AFINN}$ lexicon scores words across a range spanning from the value of -5 to +5. Intuitively, words scored closer to the lower-boundary value relate to more negative sentiment, and in contrast higher positive sentiment is revealed if rather closer to the upper-boundary value [@SilgeRobinson2017]. \n\n<b><u>  Bing </b></u>\n\nUnlike $\\texttt{AFINN}$ , the R-based $\\texttt{bing}$ lexicon does not provide sentiments via some scoring system. Instead, it simply assigns a binary label of a word being interpreted as either positive or negative [@SilgeRobinson2017].  \n\n<h4> Exploratory Data Analysis</h4>\n<hr>\n\n![Most frequent words used across all SONA speeches, irrespective of president.](saved_plots/overall_top_words.png){#fig-EDA-speeches}\n\nFrom @fig-EDA-speeches, it is evident that the word \"government\" is mainly referenced to across all SONA speeches. This word dominance draws upon the importance of this authority body that is integral to the governance of South Africa. The frequent usage of the words \"people\" and \"public\" suggests a sense of inclusivity, where the idea of togetherness is implicitly emphasized. Other words, such as \"development\" and \"new\", are indicative of ideas of growth and renewal. Lastly, a sense of security and safety is provided with the recurring use of the word \"ensure\".\n\n::: {#fig-EDA-presidents layout-ncol=2}\n\n![de Klerk](saved_plots/deKlerk_top_words.png){#fig-deKlerk}\n\n![Mandela](saved_plots/Mandela_top_words.png){#fig-Mandela}\n\n![Mbeki](saved_plots/Mbeki_top_words.png){#fig-Mbeki}\n\n![Motlanthe](saved_plots/ Motlanthe_top_words.png){#fig-Motlanthe}\n\n![Zuma](saved_plots/Zuma_top_words.png){#fig-Zuma}\n\n![Ramaphosa](saved_plots/Ramaphosa_top_words.png){#fig-Ramaphosa}\n\nMost frequent words used in SONA speeches, faceted by president.\n:::\n\nThe two words, \"government\" and \"people\", \n\n\n\n\n\n\n\n\n\n\n\n\n\n# Exploratory Data Analysis\n\n\n\n\n\n\n\n\n\n# Sentiment analysis\n\n![$\\texttt{AFINN}$: All speeches](sentiment_plots/speech_afinn_all.png){#fig-sentA-speeches-afinn}\n\n![$\\texttt{bing}$: All speeches](sentiment_plots/speech_bing_all.png){#fig-sentA-speeches-bing}\n\n    \n::: {#fig-SentA layout-ncol=2}\n\n![$\\texttt{AFINN}$: de Klerk](sentiment_plots/sent_afinn_deKlerk.png){#fig-sentA-deKlerk-afinn}\n\n![$\\texttt{bing}$: de Klerk](sentiment_plots/sent_bing_deKlerk.png){#fig-sentA-deKlerk-bing}\n\n![$\\texttt{AFINN}$: Mandela](sentiment_plots/sent_afinn_Mandela.png){#fig-sentA-Mandela-afinn}\n\n![$\\texttt{bing}$: Mandela](sentiment_plots/sent_bing_Mandela.png){#fig-sentA-Mandela-bing}\n\n![$\\texttt{AFINN}$: Mbeki](sentiment_plots/sent_afinn_Mbeki.png){#fig-sentA-Mbeki-afinn}\n\n![$\\texttt{bing}$: Mbeki](sentiment_plots/sent_bing_Mbeki.png){#fig-sentA-Mbeki-bing}\n\n![$\\texttt{AFINN}$: Motlanthe](sentiment_plots/sent_afinn_ Motlanthe.png){#fig-sentA-Motlanthe-afinn}\n\n![$\\texttt{bing}$: Motlanthe](sentiment_plots/sent_bing_ Motlanthe.png){#fig-sentA-Motlanthe-bing}\n\n![$\\texttt{AFINN}$: Zuma](sentiment_plots/sent_afinn_Zuma.png){#fig-sentA-Zuma-afinn}\n\n![$\\texttt{bing}$: Zuma](sentiment_plots/sent_bing_Zuma.png){#fig-sentA-Zuma-bing}\n\n![$\\texttt{AFINN}$: Ramaphosa](sentiment_plots/sent_afinn_Ramaphosa.png){#fig-sentA-Ramaphosa-afinn}\n\n![$\\texttt{bing}$: Ramaphosa](sentiment_plots/sent_bing_Ramaphosa.png){#fig-sentA-Ramaphosa-bing}\n\nMost frequent words used in SONA speeches, faceted by president.\n:::\n\n\n::: {#fig-SentA-Contr layout-ncol=2}\n\n![$\\texttt{AFINN}$: All speeches](sentiment_plots/word_contr_AFINN_all.png){#fig-sentA-speeches-afinn}\n\n![$\\texttt{bing}$: All speeches](sentiment_plots/word_contr_bing_all.png){#fig-sentA-speeches-bing}\n\n\n![$\\texttt{AFINN}$: de Klerk](sentiment_plots/word_contr_AFINN_deKlerk.png){#fig-sentA-deKlerk-afinn}\n\n![$\\texttt{bing}$: de Klerk](sentiment_plots/word_contr_bing_deKlerk.png){#fig-sentA-deKlerk-bing}\n\n![$\\texttt{AFINN}$: Mandela](sentiment_plots/word_contr_AFINN_Mandela.png){#fig-sentA-Mandela-afinn}\n\n![$\\texttt{bing}$: Mandela](sentiment_plots/word_contr_bing_Mandela.png){#fig-sentA-Mandela-bing}\n\n![$\\texttt{AFINN}$: Mbeki](sentiment_plots/word_contr_AFINN_Mbeki.png){#fig-sentA-Mbeki-afinn}\n\n![$\\texttt{bing}$: Mbeki](sentiment_plots/word_contr_bing_Mbeki.png){#fig-sentA-Mbeki-bing}\n\n![$\\texttt{AFINN}$: Motlanthe](sentiment_plots/word_contr_AFINN_ Motlanthe.png){#fig-sentA-Motlanthe-afinn}\n\n![$\\texttt{bing}$: Motlanthe](sentiment_plots/word_contr_bing_ Motlanthe.png){#fig-sentA-Motlanthe-bing}\n\n![$\\texttt{AFINN}$: Zuma](sentiment_plots/word_contr_AFINN_Zuma.png){#fig-sentA-Zuma-afinn}\n\n![$\\texttt{bing}$: Zuma](sentiment_plots/word_contr_bing_Zuma.png){#fig-sentA-Zuma-bing}\n\n![$\\texttt{AFINN}$: Ramaphosa](sentiment_plots/word_contr_AFINN_Ramaphosa.png){#fig-sentA-Ramaphosa-afinn}\n\n![$\\texttt{bing}$: Ramaphosa](sentiment_plots/word_contr_bing_Ramaphosa.png){#fig-sentA-Ramaphosa-bing}\n\nContribution to sentiment score, faceted by president.\n:::\n\n\n\n\n\n\n\n\n\n# Topic modelling\n\n## LSA\n\n\n\n\n\n\n\n\n\n\n\n## pLSA (Probabilistic Latent Semantic Analysis)\n\n\n\n::: {.cell execution_count=22}\n\n::: {.cell-output .cell-output-stdout}\n```\nPLSA:\n====\nNumber of topics:     5\nNumber of documents:  36\nNumber of words:      5742\nNumber of iterations: 0\nPLSA:\n====\nNumber of topics:     5\nNumber of documents:  36\nNumber of words:      5742\nNumber of iterations: 63\n[0.24343712 0.22242129 0.21591094 0.16692839 0.15130226]\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](ds4i_assignment2_files/figure-html/cell-22-output-2.png){width=661 height=468}\n:::\n\n::: {.cell-output .cell-output-display}\n![](ds4i_assignment2_files/figure-html/cell-22-output-3.png){width=893 height=865}\n:::\n\n::: {.cell-output .cell-output-display}\n![](ds4i_assignment2_files/figure-html/cell-22-output-4.png){width=661 height=468}\n:::\n:::\n\n\n## LDA (Latent Dirichlet Allocation)\n\n\n\n\n\n::: {.cell execution_count=25}\n\n::: {.cell-output .cell-output-display execution_count=24}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Validation_Set</th>\n      <th>Topics</th>\n      <th>Alpha</th>\n      <th>Beta</th>\n      <th>Coherence</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>364</th>\n      <td>BoW Corpus</td>\n      <td>2</td>\n      <td>0.1</td>\n      <td>0.9</td>\n      <td>-0.367989</td>\n    </tr>\n    <tr>\n      <th>369</th>\n      <td>BoW Corpus</td>\n      <td>2</td>\n      <td>0.2</td>\n      <td>0.9</td>\n      <td>-0.367989</td>\n    </tr>\n    <tr>\n      <th>379</th>\n      <td>BoW Corpus</td>\n      <td>2</td>\n      <td>0.4</td>\n      <td>0.9</td>\n      <td>-0.377293</td>\n    </tr>\n    <tr>\n      <th>394</th>\n      <td>BoW Corpus</td>\n      <td>2</td>\n      <td>0.7</td>\n      <td>0.9</td>\n      <td>-0.377293</td>\n    </tr>\n    <tr>\n      <th>389</th>\n      <td>BoW Corpus</td>\n      <td>2</td>\n      <td>0.6</td>\n      <td>0.9</td>\n      <td>-0.377293</td>\n    </tr>\n    <tr>\n      <th>384</th>\n      <td>BoW Corpus</td>\n      <td>2</td>\n      <td>0.5</td>\n      <td>0.9</td>\n      <td>-0.377293</td>\n    </tr>\n    <tr>\n      <th>374</th>\n      <td>BoW Corpus</td>\n      <td>2</td>\n      <td>0.3</td>\n      <td>0.9</td>\n      <td>-0.377293</td>\n    </tr>\n    <tr>\n      <th>382</th>\n      <td>BoW Corpus</td>\n      <td>2</td>\n      <td>0.5</td>\n      <td>0.5</td>\n      <td>-0.378270</td>\n    </tr>\n    <tr>\n      <th>387</th>\n      <td>BoW Corpus</td>\n      <td>2</td>\n      <td>0.6</td>\n      <td>0.5</td>\n      <td>-0.378270</td>\n    </tr>\n    <tr>\n      <th>377</th>\n      <td>BoW Corpus</td>\n      <td>2</td>\n      <td>0.4</td>\n      <td>0.5</td>\n      <td>-0.378270</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\n\n\n::: {.cell execution_count=27}\n\n::: {.cell-output .cell-output-display execution_count=26}\n```{=html}\n\n<link rel=\"stylesheet\" type=\"text/css\" href=\"https://cdn.jsdelivr.net/gh/bmabey/pyLDAvis@3.4.0/pyLDAvis/js/ldavis.v1.0.0.css\">\n\n\n<div id=\"ldavis_el6090360617691048589451134\" style=\"background-color:white;\"></div>\n<script type=\"text/javascript\">\n\nvar ldavis_el6090360617691048589451134_data = {\"mdsDat\": {\"x\": [0.05715602387222515, -0.05715602387222515], \"y\": [0.0, 0.0], \"topics\": [1, 2], \"cluster\": [1, 1], \"Freq\": [57.88687207877439, 42.11312792122562]}, \"tinfo\": {\"Term\": [\"shall\", \"plan\", \"businesses\", \"eskom\", \"key\", \"mining\", \"question\", \"say\", \"compatriots\", \"capture\", \"farmers\", \"grow\", \"fact\", \"several\", \"context\", \"pandemic\", \"recovery\", \"sona\", \"fellow\", \"presidential\", \"rand\", \"evening\", \"reforms\", \"funding\", \"rail\", \"covid\", \"procurement\", \"underway\", \"around\", \"essential\", \"question\", \"confident\", \"context\", \"say\", \"soccer\", \"questions\", \"instance\", \"shall\", \"discharge\", \"session\", \"iraq\", \"patriotism\", \"fact\", \"weapons\", \"jobcreation\", \"instances\", \"asgisa\", \"mention\", \"tribute\", \"presence\", \"elaborate\", \"deficit\", \"constitute\", \"mentioned\", \"nepad\", \"offensive\", \"mind\", \"dealing\", \"appreciation\", \"privileged\", \"accordingly\", \"matters\", \"reconciliation\", \"masses\", \"humanity\", \"negotiations\", \"elections\", \"objectives\", \"cup\", \"importance\", \"sense\", \"occasion\", \"convinced\", \"relations\", \"reconstruction\", \"necessary\", \"rates\", \"distinguished\", \"child\", \"possible\", \"second\", \"spheres\", \"general\", \"objective\", \"movement\", \"intensify\", \"indeed\", \"improvement\", \"per\", \"rest\", \"structures\", \"terms\", \"parties\", \"governance\", \"particularly\", \"crimes\", \"number\", \"decade\", \"pleased\", \"republic\", \"restructuring\", \"increased\", \"pandemic\", \"covid\", \"sona\", \"model\", \"loadshedding\", \"genderbased\", \"capabilities\", \"capture\", \"nhi\", \"stimulus\", \"visa\", \"npa\", \"na\", \"several\", \"damage\", \"trillion\", \"mogoeng\", \"entities\", \"ndp\", \"sugar\", \"essential\", \"mandates\", \"phakisa\", \"prosecuting\", \"hydrogen\", \"zondo\", \"fix\", \"underway\", \"tvet\", \"siu\", \"evening\", \"master\", \"reforms\", \"grow\", \"brics\", \"compatriots\", \"businesses\", \"renewable\", \"eskom\", \"recovery\", \"farmers\", \"plan\", \"key\", \"signed\", \"mining\", \"digital\", \"funding\", \"fellow\", \"presidential\", \"products\", \"rail\", \"rand\", \"procurement\", \"around\", \"eastern\", \"crisis\", \"reform\", \"law\", \"phase\", \"municipalities\", \"inclusive\", \"manufacturing\", \"number\", \"households\", \"potential\", \"students\", \"supply\", \"want\", \"project\"], \"Freq\": [151.0, 149.0, 73.0, 70.0, 88.0, 93.0, 65.0, 69.0, 46.0, 35.0, 49.0, 42.0, 71.0, 35.0, 59.0, 29.0, 42.0, 29.0, 63.0, 58.0, 65.0, 31.0, 31.0, 43.0, 47.0, 23.0, 51.0, 27.0, 62.0, 25.0, 64.63786229792939, 35.524015442162586, 58.142684256879946, 67.32714723279724, 30.96745680380658, 23.33113473990958, 22.488098071390638, 145.52232995563003, 18.169186151473383, 21.471735849926183, 17.30886416542615, 18.939126544743278, 67.92890080110614, 17.250174482655673, 17.247412462675534, 16.403564416053772, 15.571088630101416, 20.515304460384915, 23.822907673596813, 16.374095459753264, 15.546386057039603, 16.37018676615293, 18.007106891820577, 15.52531523459554, 18.824988986690748, 14.696528625565012, 16.337274084069, 33.62983658898192, 14.661973736430516, 13.822692007213883, 31.896347649387597, 54.46130990255954, 34.94844826851449, 28.48841327310102, 36.42436356215406, 35.33106306045142, 60.07841011483229, 62.99019419109154, 44.947108793533694, 34.25832858545535, 30.31984740545854, 27.21942347767844, 24.187457261746804, 63.158864508483674, 47.51172307470377, 85.82933925932751, 44.0649740742429, 29.99652805117707, 52.07734395433599, 63.79464408819109, 57.239842723814505, 42.15115912998948, 42.73445106776713, 36.082236167055456, 33.958744819373436, 40.81585677304506, 51.69532195005409, 41.27108064287707, 49.905440379852514, 44.385197965488274, 41.16723165556209, 42.64499200077793, 45.46242280697117, 44.7772862451371, 38.58297016855072, 43.608849908159826, 58.95978646549147, 44.120647949818434, 42.402189882201874, 42.884389295260426, 41.694760799615956, 42.10656297371726, 29.183239936757438, 22.973599574464956, 28.930908532621306, 21.18208892223146, 18.537136033895116, 17.651589379207337, 17.65022283229922, 33.61913206126314, 16.58699273525095, 13.215672996472502, 14.047913540308617, 12.32839726911893, 13.156670244811565, 33.13870322389987, 12.315699433844513, 14.799535902642317, 12.300217292936793, 12.28062742522835, 11.423617881762796, 10.554690809393435, 23.74709746876367, 10.5539199367786, 13.0116019915273, 10.53685831561811, 9.667092312824387, 9.666738546982788, 10.484473343633995, 25.225151370150503, 9.661248774960889, 9.651195086662526, 29.241168587939317, 19.39504313183878, 28.88349717145021, 38.19241740488967, 15.307611035146682, 41.76587557635864, 64.00643189665092, 23.910964501950595, 61.24540877260616, 37.051808947608315, 42.6533116775604, 114.93806641951089, 70.41768779202806, 19.631033923291582, 70.94374635840018, 21.659261766172627, 35.357474065943414, 49.06263500068288, 45.2859683390825, 25.5469071307212, 37.751966129146055, 48.888094346675175, 39.78311035024126, 45.34393472528748, 40.290686841368895, 50.36363617366681, 44.528448531709444, 43.67383111917279, 31.561885985615287, 40.798363060771884, 31.821149852567345, 34.333868438512894, 47.06460865307882, 39.851189722638054, 35.97959521970255, 34.08363402351782, 34.785656393340396, 33.68639473994561, 34.826782177805924], \"Total\": [151.0, 149.0, 73.0, 70.0, 88.0, 93.0, 65.0, 69.0, 46.0, 35.0, 49.0, 42.0, 71.0, 35.0, 59.0, 29.0, 42.0, 29.0, 63.0, 58.0, 65.0, 31.0, 31.0, 43.0, 47.0, 23.0, 51.0, 27.0, 62.0, 25.0, 65.98807229947958, 36.39009394143742, 59.90315128267353, 69.48373244136502, 32.04165953804175, 24.206452423747482, 23.335675712139416, 151.35933898629185, 18.983730064463337, 22.467983501376505, 18.113288016907465, 19.855785473290233, 71.24581674416442, 18.114138894107114, 18.114211296587836, 17.243635145643843, 16.372701727741273, 21.598826313417355, 25.08318879784355, 17.24390281945847, 16.373127142408155, 17.244093261202707, 18.986441587993838, 16.373357066666472, 19.857212097332702, 15.502506941000298, 17.24469155384391, 35.5381529729545, 15.503109553893685, 14.632298275584718, 33.7972506174596, 58.200113174593625, 37.286900958589314, 30.314978077198674, 39.032279188394206, 38.16670011736705, 66.07561038312691, 70.45492904924313, 49.5119903453631, 37.300236397011226, 32.94070749230267, 29.451702278773663, 25.962537512157173, 73.11320525092975, 53.899070463746185, 104.6122785383618, 50.417200264371175, 32.946442593283834, 60.907600136581976, 76.65005003765671, 68.79249012636306, 48.67999533583889, 49.55573437651047, 40.81322897518692, 38.19290214116787, 47.818847694827035, 64.46438612362007, 48.69714855762216, 62.72459914426816, 53.96000927931239, 49.586492790635155, 53.1066531752663, 59.26276629445171, 60.16318936714773, 45.200809143268266, 57.52412618993808, 106.02439511857028, 60.17602698422464, 55.77304277047105, 58.42515156137135, 53.12530305811829, 61.10254475943968, 29.971848799454214, 23.760950963192737, 29.966641132723307, 21.98597839186145, 19.324576109213474, 18.437351008698563, 18.437308798038888, 35.27584469521486, 17.54592911575006, 14.001002597226476, 14.886970253351901, 13.113731964141813, 13.999608617096303, 35.26714133825155, 13.11339085700373, 15.771296410450287, 13.113089926896054, 13.112532653062743, 12.225994990310017, 11.339212679078015, 25.51354887534264, 11.33919552670424, 13.9967259244756, 11.338730396870169, 10.451939405267359, 10.451932049711667, 11.33753912251258, 27.283030618285864, 10.451774703671713, 10.45150704806078, 31.712146270055996, 21.079044374997363, 31.703780073560228, 42.32538943580858, 16.650942886124575, 46.745864289161574, 73.27898033941304, 26.38764313409335, 70.61541984193106, 42.30328236541419, 49.37263801378214, 149.09849241881574, 88.19683263372389, 21.95431948738909, 93.42856742553536, 24.60313441074072, 43.14111127196021, 63.41997793988735, 58.12580081587875, 29.898664117880482, 47.538284558750455, 65.15739976871112, 51.92865140085612, 62.47811548354582, 54.549139832484975, 74.75790963840672, 72.0353147433103, 71.14860158417679, 42.19797570723816, 65.87202860447637, 43.073040357461814, 50.08331761170471, 106.02439511857028, 75.42669988740823, 61.428182283058575, 53.55932231420843, 61.405271387203, 55.2923191903556, 65.75745836007953], \"Category\": [\"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\"], \"logprob\": [30.0, 29.0, 28.0, 27.0, 26.0, 25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, -6.1992, -6.7978, -6.3051, -6.1584, -6.9351, -7.2182, -7.255, -5.3877, -7.4683, -7.3013, -7.5168, -7.4268, -6.1495, -7.5202, -7.5203, -7.5705, -7.6226, -7.3468, -7.1974, -7.5723, -7.6242, -7.5725, -7.4772, -7.6255, -7.4328, -7.6804, -7.5746, -6.8526, -7.6828, -7.7417, -6.9055, -6.3705, -6.8141, -7.0185, -6.7728, -6.8032, -6.2724, -6.225, -6.5625, -6.8341, -6.9562, -7.0641, -7.1822, -6.2224, -6.507, -5.9156, -6.5823, -6.9669, -6.4153, -6.2123, -6.3208, -6.6267, -6.613, -6.7822, -6.8429, -6.6589, -6.4226, -6.6478, -6.4579, -6.5751, -6.6504, -6.6151, -6.5511, -6.5663, -6.7152, -6.5927, -6.2912, -6.5811, -6.6208, -6.6095, -6.6376, -6.6278, -6.6763, -6.9155, -6.685, -6.9967, -7.1301, -7.1791, -7.1791, -6.5348, -7.2413, -7.4685, -7.4074, -7.538, -7.4729, -6.5492, -7.539, -7.3553, -7.5403, -7.5419, -7.6142, -7.6933, -6.8824, -7.6934, -7.484, -7.695, -7.7812, -7.7812, -7.7, -6.822, -7.7818, -7.7828, -6.6743, -7.0849, -6.6866, -6.4072, -7.3215, -6.3178, -5.8909, -6.8755, -5.935, -6.4376, -6.2968, -5.3055, -5.7954, -7.0728, -5.788, -6.9744, -6.4844, -6.1568, -6.2369, -6.8094, -6.4188, -6.1603, -6.3664, -6.2356, -6.3538, -6.1306, -6.2537, -6.2731, -6.5979, -6.3412, -6.5897, -6.5137, -6.1984, -6.3647, -6.4669, -6.5211, -6.5007, -6.5328, -6.4995], \"loglift\": [30.0, 29.0, 28.0, 27.0, 26.0, 25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, 0.526, 0.5226, 0.5169, 0.5152, 0.5126, 0.5098, 0.5097, 0.5074, 0.5028, 0.5013, 0.5013, 0.4994, 0.499, 0.4978, 0.4976, 0.4967, 0.4965, 0.4952, 0.4951, 0.4949, 0.4949, 0.4947, 0.4937, 0.4935, 0.4933, 0.4933, 0.4926, 0.4915, 0.4909, 0.4898, 0.4888, 0.4803, 0.4819, 0.4845, 0.4775, 0.4695, 0.4515, 0.4347, 0.45, 0.4616, 0.4638, 0.4679, 0.4759, 0.4003, 0.4205, 0.3488, 0.412, 0.4529, 0.3901, 0.3631, 0.3628, 0.4027, 0.3986, 0.4235, 0.4292, 0.3883, 0.3259, 0.3812, 0.3181, 0.3513, 0.3606, 0.3273, 0.2816, 0.2513, 0.3884, 0.2697, -0.0401, 0.2363, 0.2726, 0.2374, 0.3044, 0.1743, 0.8381, 0.8311, 0.8296, 0.8276, 0.8232, 0.8213, 0.8212, 0.8167, 0.8086, 0.8071, 0.8068, 0.8031, 0.8027, 0.8026, 0.8021, 0.8012, 0.8008, 0.7993, 0.7969, 0.7931, 0.7931, 0.793, 0.7918, 0.7915, 0.7868, 0.7867, 0.7866, 0.7864, 0.7862, 0.7851, 0.7837, 0.7815, 0.7716, 0.7621, 0.7807, 0.7522, 0.7295, 0.7663, 0.7225, 0.7323, 0.7185, 0.6046, 0.6397, 0.753, 0.5895, 0.7374, 0.6658, 0.6081, 0.6152, 0.7075, 0.6343, 0.5775, 0.5984, 0.5443, 0.5618, 0.4698, 0.3838, 0.3768, 0.5744, 0.3857, 0.562, 0.4873, 0.0527, 0.2268, 0.3299, 0.4128, 0.2965, 0.3693, 0.2292]}, \"token.table\": {\"Topic\": [1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2], \"Freq\": [0.946822579215034, 0.059176411200939624, 0.9675478295406017, 0.06450318863604011, 0.27209527477628714, 0.7202521979372306, 0.9772363942164913, 0.06107727463853071, 0.060056659063632466, 0.900849885954487, 0.12281830285183919, 0.873374598057523, 0.0542378505970658, 0.9762813107471844, 0.0566960201032776, 0.9638323417557192, 0.85375223918514, 0.1477648106281973, 0.10696133392829989, 0.898475204997719, 0.9892802161471416, 0.027480006004087267, 0.9480449465255449, 0.05266916369586361, 0.9682295298006468, 0.03338722516553955, 0.9244088713887, 0.07703407261572501, 0.042085857655658025, 0.9679747260801346, 0.764896451529173, 0.2433761436683732, 0.32103626380251343, 0.6688255495885697, 0.9088707540559282, 0.10098563933954757, 0.07625792679441948, 0.9150951215330336, 0.9567182634920539, 0.056277544911297286, 0.7311881858125123, 0.26588661302273175, 0.927853947299057, 0.05799087170619106, 0.12193568306850866, 0.8941950091690636, 0.9481803596488746, 0.0526766866471597, 0.9105687181570107, 0.09105687181570107, 0.25664932651536976, 0.7332837900439136, 0.97721100317839, 0.06107568769864938, 0.9080506355083421, 0.0908050635508342, 0.07626291780988825, 0.915155013718659, 0.12745091681315543, 0.863833991733609, 0.07838972186001468, 0.9406766623201762, 0.06306731758135488, 0.9144761049296457, 0.9544420024572141, 0.04210773540252415, 0.14177893427622773, 0.8709277391253989, 0.2207506286626259, 0.7726272003191906, 0.0882025622310165, 0.882025622310165, 0.18543796773264024, 0.8112911088303011, 0.054237726424376784, 0.9762790756387821, 0.8677098733579074, 0.14125509566291516, 0.747965665938787, 0.24932188864626234, 0.0945059231189466, 0.8978062696299927, 0.47728456970460476, 0.530316188560672, 0.9223135504396622, 0.07685946253663853, 0.09567602348479365, 0.9567602348479365, 0.9115223731591239, 0.08042844469051093, 0.8419384135291965, 0.1437455827976677, 0.2553801614353513, 0.7429241059937493, 0.6873690803771549, 0.31095267921823677, 0.8066469430156714, 0.20166173575391785, 0.942762501132779, 0.04285284096058086, 0.9278785978049405, 0.05799241236280878, 0.8574025091875084, 0.1463857942515258, 0.9385374971198884, 0.05520808806587579, 0.9384896599501564, 0.05520527411471508, 0.20408896172896493, 0.7936792956126414, 0.37948743051619876, 0.6184239608412129, 0.05174757750692524, 0.9832039726315795, 0.08818967779900802, 0.9700864557890883, 0.31946765436043567, 0.6788687655159258, 0.923635832052939, 0.06597398800378135, 0.09488096160432559, 0.9013691352410931, 0.9278332472998159, 0.06872838868887525, 0.9722750530640936, 0.04629881205067112, 0.9771972806098166, 0.06107483003811354, 0.9278217560483728, 0.0579888597530233, 0.2354740162053163, 0.7599388704807934, 0.0454835341951473, 0.9551542180980933, 0.07625967682482797, 0.9151161218979356, 0.890217765445785, 0.10473150181715118, 0.37952376038258384, 0.6224189670274375, 0.07143056833594628, 0.9285973883673018, 0.08179293389148058, 0.8997222728062864, 0.8220832315440237, 0.18162303952716805, 0.9170297639662565, 0.0786025511971077, 0.9568311959840603, 0.05035953663074001, 0.05699327709595911, 0.9688857106313049, 0.07625594321543248, 0.9150713185851899, 0.556475704803772, 0.4432942055216489, 0.8820669401552815, 0.1225092972437891, 0.8941886799143229, 0.0993542977682581, 0.9167551588167233, 0.0679077895419795, 0.9675854400251038, 0.0645056960016736, 0.03336464182410429, 0.9675746128990244, 0.8628164127855718, 0.15486448434612826, 0.759330061921409, 0.23623601926443832, 0.9568999436239163, 0.050363154927574545, 0.7971354250506845, 0.20725521051317797, 0.07144527980299549, 0.9287886374389415, 0.260676011482541, 0.7583302152219374, 0.22803718165368458, 0.7713022320639332, 0.7530519748195778, 0.23308751601558358, 0.8349635775652856, 0.16960197669294863, 0.4069793223703253, 0.5860502242132684, 0.9278641945224363, 0.05799151215765227, 0.2236528326066292, 0.7741828820998703, 0.9567874940986022, 0.06834196386418587, 0.23108630161348967, 0.7702876720449656, 0.13378524151545138, 0.8696040698504339, 0.4714294130750602, 0.5322590147621648, 0.08819329545713779, 0.9701262500285157, 0.9850265015320446, 0.015154253869723765, 0.9501598828846186, 0.04131129925585298, 0.2103567701868048, 0.7993557267098582, 0.2455592159416293, 0.7520250988212397, 0.872718036092415, 0.11900700492169296, 0.9386674435311978, 0.05363813963035416, 0.8905533914965372, 0.11131917393706715, 0.11819413814772539, 0.8746366222931679, 0.388698239186916, 0.6246935986932578, 0.09462594028343921, 0.9147174227399123, 0.8616774464172305, 0.13677419784400485, 0.07579305168849888, 0.9095166202619867, 0.7359843980007762, 0.27385465972121903, 0.8154186885373473, 0.1853224292130335, 0.7905837253116961, 0.20705764234353946, 0.9642544757730026, 0.028783715694716494, 0.8285788157297147, 0.17443764541678206, 0.9107272515931896, 0.09107272515931895, 0.9346633176365574, 0.04450777703031226, 0.05671001175904082, 0.9357151940241736, 0.9645919503732953, 0.03964076508383405, 0.09109824611729969, 0.9109824611729969, 0.0956799814037866, 0.9567998140378661, 0.9674904623212468, 0.031209369752298283, 0.033370440002633756, 0.9677427600763789, 0.8627774039468532, 0.1437962339911422, 0.07142345650290034, 0.9285049345377046, 0.8268380700589336, 0.16133425757247485, 0.35474683358642134, 0.6348101232599118, 0.08818954439800748, 0.9700849883780823, 0.4397016638807147, 0.5699836383638894, 0.8096913932438631, 0.18830032401020072, 0.956816144606914, 0.03986733935862142, 0.06340632843203592, 0.9510949264805387, 0.09567753117073022, 0.9567753117073022, 0.07330563924447393, 0.9163204905559241, 0.06717283523656153, 0.9404196933118615, 0.3978852817560484, 0.6149136172593476, 0.9384934111071895, 0.05520549477101115, 0.09567609081687309, 0.9567609081687309], \"Term\": [\"accordingly\", \"accordingly\", \"appreciation\", \"appreciation\", \"around\", \"around\", \"asgisa\", \"asgisa\", \"brics\", \"brics\", \"businesses\", \"businesses\", \"capabilities\", \"capabilities\", \"capture\", \"capture\", \"child\", \"child\", \"compatriots\", \"compatriots\", \"confident\", \"confident\", \"constitute\", \"constitute\", \"context\", \"context\", \"convinced\", \"convinced\", \"covid\", \"covid\", \"crimes\", \"crimes\", \"crisis\", \"crisis\", \"cup\", \"cup\", \"damage\", \"damage\", \"dealing\", \"dealing\", \"decade\", \"decade\", \"deficit\", \"deficit\", \"digital\", \"digital\", \"discharge\", \"discharge\", \"distinguished\", \"distinguished\", \"eastern\", \"eastern\", \"elaborate\", \"elaborate\", \"elections\", \"elections\", \"entities\", \"entities\", \"eskom\", \"eskom\", \"essential\", \"essential\", \"evening\", \"evening\", \"fact\", \"fact\", \"farmers\", \"farmers\", \"fellow\", \"fellow\", \"fix\", \"fix\", \"funding\", \"funding\", \"genderbased\", \"genderbased\", \"general\", \"general\", \"governance\", \"governance\", \"grow\", \"grow\", \"households\", \"households\", \"humanity\", \"humanity\", \"hydrogen\", \"hydrogen\", \"importance\", \"importance\", \"improvement\", \"improvement\", \"inclusive\", \"inclusive\", \"increased\", \"increased\", \"indeed\", \"indeed\", \"instance\", \"instance\", \"instances\", \"instances\", \"intensify\", \"intensify\", \"iraq\", \"iraq\", \"jobcreation\", \"jobcreation\", \"key\", \"key\", \"law\", \"law\", \"loadshedding\", \"loadshedding\", \"mandates\", \"mandates\", \"manufacturing\", \"manufacturing\", \"masses\", \"masses\", \"master\", \"master\", \"matters\", \"matters\", \"mention\", \"mention\", \"mentioned\", \"mentioned\", \"mind\", \"mind\", \"mining\", \"mining\", \"model\", \"model\", \"mogoeng\", \"mogoeng\", \"movement\", \"movement\", \"municipalities\", \"municipalities\", \"na\", \"na\", \"ndp\", \"ndp\", \"necessary\", \"necessary\", \"negotiations\", \"negotiations\", \"nepad\", \"nepad\", \"nhi\", \"nhi\", \"npa\", \"npa\", \"number\", \"number\", \"objective\", \"objective\", \"objectives\", \"objectives\", \"occasion\", \"occasion\", \"offensive\", \"offensive\", \"pandemic\", \"pandemic\", \"particularly\", \"particularly\", \"parties\", \"parties\", \"patriotism\", \"patriotism\", \"per\", \"per\", \"phakisa\", \"phakisa\", \"phase\", \"phase\", \"plan\", \"plan\", \"pleased\", \"pleased\", \"possible\", \"possible\", \"potential\", \"potential\", \"presence\", \"presence\", \"presidential\", \"presidential\", \"privileged\", \"privileged\", \"procurement\", \"procurement\", \"products\", \"products\", \"project\", \"project\", \"prosecuting\", \"prosecuting\", \"question\", \"question\", \"questions\", \"questions\", \"rail\", \"rail\", \"rand\", \"rand\", \"rates\", \"rates\", \"reconciliation\", \"reconciliation\", \"reconstruction\", \"reconstruction\", \"recovery\", \"recovery\", \"reform\", \"reform\", \"reforms\", \"reforms\", \"relations\", \"relations\", \"renewable\", \"renewable\", \"republic\", \"republic\", \"rest\", \"rest\", \"restructuring\", \"restructuring\", \"say\", \"say\", \"second\", \"second\", \"sense\", \"sense\", \"session\", \"session\", \"several\", \"several\", \"shall\", \"shall\", \"signed\", \"signed\", \"siu\", \"siu\", \"soccer\", \"soccer\", \"sona\", \"sona\", \"spheres\", \"spheres\", \"stimulus\", \"stimulus\", \"structures\", \"structures\", \"students\", \"students\", \"sugar\", \"sugar\", \"supply\", \"supply\", \"terms\", \"terms\", \"tribute\", \"tribute\", \"trillion\", \"trillion\", \"tvet\", \"tvet\", \"underway\", \"underway\", \"visa\", \"visa\", \"want\", \"want\", \"weapons\", \"weapons\", \"zondo\", \"zondo\"]}, \"R\": 30, \"lambda.step\": 0.01, \"plot.opts\": {\"xlab\": \"PC1\", \"ylab\": \"PC2\"}, \"topic.order\": [2, 1]};\n\nfunction LDAvis_load_lib(url, callback){\n  var s = document.createElement('script');\n  s.src = url;\n  s.async = true;\n  s.onreadystatechange = s.onload = callback;\n  s.onerror = function(){console.warn(\"failed to load library \" + url);};\n  document.getElementsByTagName(\"head\")[0].appendChild(s);\n}\n\nif(typeof(LDAvis) !== \"undefined\"){\n   // already loaded: just create the visualization\n   !function(LDAvis){\n       new LDAvis(\"#\" + \"ldavis_el6090360617691048589451134\", ldavis_el6090360617691048589451134_data);\n   }(LDAvis);\n}else if(typeof define === \"function\" && define.amd){\n   // require.js is available: use it to load d3/LDAvis\n   require.config({paths: {d3: \"https://d3js.org/d3.v5\"}});\n   require([\"d3\"], function(d3){\n      window.d3 = d3;\n      LDAvis_load_lib(\"https://cdn.jsdelivr.net/gh/bmabey/pyLDAvis@3.4.0/pyLDAvis/js/ldavis.v3.0.0.js\", function(){\n        new LDAvis(\"#\" + \"ldavis_el6090360617691048589451134\", ldavis_el6090360617691048589451134_data);\n      });\n    });\n}else{\n    // require.js not available: dynamically load d3 & LDAvis\n    LDAvis_load_lib(\"https://d3js.org/d3.v5.js\", function(){\n         LDAvis_load_lib(\"https://cdn.jsdelivr.net/gh/bmabey/pyLDAvis@3.4.0/pyLDAvis/js/ldavis.v3.0.0.js\", function(){\n                 new LDAvis(\"#\" + \"ldavis_el6090360617691048589451134\", ldavis_el6090360617691048589451134_data);\n            })\n         });\n}\n</script>\n```\n:::\n:::\n\n\n## CTM (Correlated Topic Model)\n\n::: {.cell execution_count=28}\n\n::: {.cell-output .cell-output-stdout}\n```\nTopics: 2, Coherence Score: -0.012629277750513099\nTopics: 4, Coherence Score: -0.039088255043751505\nTopics: 6, Coherence Score: -0.04782304793703118\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](ds4i_assignment2_files/figure-html/cell-28-output-2.png){width=989 height=523}\n:::\n:::\n\n\n## ATM (Author-Topic Model)\n\n::: {.cell execution_count=29}\n\n::: {.cell-output .cell-output-stdout}\n```\nNum Topics: 2, Coherence Score: -0.4103558110777859\nNum Topics: 4, Coherence Score: -0.44784230652190293\nNum Topics: 6, Coherence Score: -0.44298005893897024\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](ds4i_assignment2_files/figure-html/cell-29-output-2.png){width=989 height=523}\n:::\n:::\n\n\n</div> \n\n",
    "supporting": [
      "ds4i_assignment2_files"
    ],
    "filters": [],
    "includes": {
      "include-in-header": [
        "<script src=\"https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js\" integrity=\"sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==\" crossorigin=\"anonymous\"></script>\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js\" integrity=\"sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==\" crossorigin=\"anonymous\"></script>\n<script type=\"application/javascript\">define('jquery', [],function() {return window.jQuery;})</script>\n"
      ]
    }
  }
}