{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {},
      "source": [
        "---\n",
        "title: \"Sentiments and Topics in South African SONA Speeches\"\n",
        "title-size: small\n",
        "subtitle: \"~ STA5073Z Data Science for Industry Assignment 2\"\n",
        "format: html\n",
        "author:   \n",
        "  - Jared Tavares (TVRJAR001)\n",
        "  - Heiletjé van Zyl (VZYHEI003)\n",
        "---"
      ],
      "id": "a5c0e835"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<!-- <div style=\"text-align: justify\">  -->\n",
        "\n",
        "<h4> Abstract </h4>"
      ],
      "id": "2da1ebb6"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# TODO AT THE END!\n",
        "a = 1"
      ],
      "id": "9e0a4435",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<h4> Introduction </h4>\n",
        "The field of Natural Language Processing (NLP) is faceted by techniques tailored for theme tracking and opinion mining which merge part of text analysis. Though, of particular prominence, is the extraction of latent thematic patterns and the establishment of the extent of emotionality expressed in political-based texts. \n",
        "\n",
        "Given such political context, it is of specific interest to analyse the  annual State of the Nation Address (SONA) speeches delivered by six different South African presidents (F.W. de Klerk, N.R. Mandela, T.M. Mbeki, K.P. Motlanthe, J.G. Zuma, and M.C. Ramaphosa) ranging over twenty-nine years (from 1994 to 2023). This analysis, descriptive and data-driven in nature, endeavours to examine the content of the SONA speeches in terms of themes via topic modelling (TM) and emotions via sentiment analysis (SentA). In general, as illustrated in @fig-MacroMicroScheme, this exploration will be double-bifurcated, executing the aforementioned techniques within a macro and micro context both at the text (all-presidents versus by-president SONA speeches, respectively) and token (sentences versus words, respectively) level.  \n",
        "\n",
        "![Illustration of how the NLP techniques, sentiment analysis and topic modelling, will be implemented within a different-scales-within-different-levels framework for the presidential-SONA-speeech text analysis.](MacroMicroScheme.png){width=400 height=250 #fig-MacroMicroScheme}\n",
        "\n",
        "Through such a multi-layered lens, the identification of any trends, both in terms of topics and sentiments, over time at both a large (presidents as a collective) as well as at a small (each president as an individual) scale is attainable. This explicates not only an aggregated perspective of the general political discourse prevailing within South Africa, but also a more niche outlook of the specific rhetoric employed by each of the country\\'s serving presidents during different date periods. \n",
        "\n",
        "To achieve all of the above-mentioned, it is first relevant to revise foundational terms and review related literature in context of politics and NLP. All pertinent pre-processing of the political text data is then considered, followed by a discussion delving into the details of each SentA and TM approach applied as part of the analysis. Specifically, three different lexicons are leveraged to describe sentiments, whilst five different topic models are tackled to uncover themes   within South-African-presidents’ SONA speeches. Ensuing the implementation of these methodologies, the results thereof are detailed in terms insights and interpretations. Thereafter, an overall evaluation of the techniques in terms of efficacy and inadequacy is overviewed. Finally, focal findings are highlighted and potential improvements as part of future research are recommended.  \n",
        "\n",
        "\n",
        "<h4> Literature Review </h4>\n",
        "\n",
        "<b><u> SONA and analysis thereof </b></u>\n",
        "\n",
        "SONA, a pivotal event in the political programme of Parliament, serves as a presidential summary for the South African public. Specifically, the country’s current domestic affairs and international relations are reflected upon, past governmental work is perused, and future plans in terms of policies and civil projects are proposed. Through this address, accountability on the part of government is re-instilled and transparency with the public is re-affirmed on an annual basis, either once (non-election year) or twice (pre-and-post election) [@Muthambi2017].\n",
        "The text analysis of such SONA speeches, via the implementation of TM and SentA, has been previously done for Philippine presidents [@MirandaBringula2021]. Though, it is now of interest to extend such an application to another country, South Africa. \n",
        "\n",
        "\n",
        "<b><u> Tokenization process </b></u>\n",
        "\n",
        "The process of tokenization entails breaking up given text into units, referred to as tokens (or terms), which are meaningful for analysis [@Zhang2018]. In this case, these tokens take on different structures, based on either a macro-context (i.e., sentences) or micro-context (i.e., words, as a unigram or bigram). At both scales, the way in which these tokens are valued will be varied. The value will either be defined by a bag-of-words (BoW) or term-frequency, inverse-document-frequency (tf-idf) approach. The former way implicates accounting for the number of occurrences of some token in some document. On the other hand, the latter way not only regards the frequency of some token, but also the significance thereof. Thus, tf-idf involves the assignment of some weight to each token in a document which in turn reflects its importance relative to the entire collection of documents (corpus). It then follows that the tf-idf value of a token t in a document *d* within a corpus *D* is calculated as the product of two constituents. The first being tf(*t*,*d*) defined as the quotient of the frequency of token *t* in document *d* and the total number of tokens in document *d*, whereas the second is idf(*t*, *D*) denoted by the quotient of the natural logarithm of the total number of documents in corpus *D* and the number of documents containing the token *t* [@SilgeRobinson2017]. \n",
        "\n",
        "\n",
        "<b><u> Topic modelling (TM) </b></u>\n",
        "\n",
        "TM, an unsupervised learning approach, implicates the identification of underlying abstract themes in some body of text, in the absence of pre-specified labels [@Cho2019]. In general, there are two topic-model assumptions: each document comprises of a mixture of topics and each topic consists of a collection of words [@Zhang2018]. \n",
        "\n",
        "\n",
        "<b><u> Sentiment analysis (SentA) </b></u>\n",
        "SentA involves deciphering the intent of words to infer certain emotional dimensions labelled either in polarized (negative/positive) or higher-dimensional terms (niche feelings like joy/sadness). Various unigram lexicons have been derived to such extents. For example, the R-based nrc lexicon dichotomously classifies words with yes/no labels in categories such as positive, negative, anticipation, anger, and so forth. \n",
        "\n",
        "<b><u> Optimal number of topics </b></u>\n",
        "Two approaches can be applied to determine the optimum topic number. Within the first approach, each topic can be viewed as a cluster and a metric showing how similar each word is to its own topic (cohesion) compared to other topics (separation) can be computed. The Silhouette Coefficient is such a measure ranging from a value of negative one to positive one. Here, higher positive values are indicative of words being well-compacted within the topic to which it belongs. The second approach involves the calculation of a coherence score. This measures the ability of the topic model to distinguish well between topics that are semantically interpretable by humans and are not simply statistical-inference artifacts. Hence, the number of topics as well as any other topic-model hyperparameters are tuned to values that yield the maximum coherence score, allowing for the most understandable themes. This latter approach will largely be applied in this SONA-speech analysis. \n",
        "\n",
        "<h4> Methods </h4>\n",
        "<h5> Topic modelling </h5>\n",
        "\n",
        "<b><u> Latent Semantic Analysis (LSA) </b></u> \n",
        "\n",
        "![](LSA.png){fig-alt=\"Schematic representation of LSA.\"}\n",
        "\n",
        "LSA [@Deerwester1990] is a non-probabilistic, non-generative model where a form of matrix factorization is utilized to uncover few latent topics, capturing meaningful relationships among documents/tokens. As depicted in Figure, in the first step, a document-term matrix DTM is generated from the raw text data by tokenizing d documents into w words (or sentences), forming the columns and rows respectively. Each row-column entry is either valued via the BoW or tf-idf approach. This DTM-matrix, which is often sparse and high-dimensional, is then decomposed via a dimensionality-reduction-technique, namely truncated Singular Value Decomposition (SVD). Consequently, in the second step the DTM-matrix becomes the product of three matrices: the topic-word matrix At* (for the tokens), the topic-prevalence matrix Bt* (for the latent semantic factors), and the transposed document-topic matrix CTt* (for the document). Here, t*, the optimal number of topics, is a hyperparameter which is refined at a value (either via the Silhouette-Coefficient or the coherence-measure approach) that retains the most significant dimensions in the transformed space. In the final step, the text data is then encoded using this top-topic number. \n",
        "\n",
        "Given LSA only implicates a DTM-matrix, the implementation thereof is generally efficient. Though, with the involvement of truncated SVD, some computational intensity and a lack of quick updates with new, incoming text-data can arise. Additional LSA drawbacks include: the lack of interpretability, the underlying linear-model framework (which results in poor performance on text-data with non-linear dependencies), and the underlying Gaussian assumption for tokens in documents (which may not be an appropriate distribution). \n",
        "\n",
        "<b><u> Probabilistic Latent Semantic Analysis (pLSA) </b></u>\n",
        "\n",
        "![](pLSA.png){fig-alt=\"Schematic representation of pLSA.\"}\n",
        "\n",
        "Instead of implementing truncated SVD, pLSA [@Hofmann1999] rather utilizes a generative, probabilistic model. Within this framework, a document d is first selected with probability P(d). Then given this, a latent topic t is present in this selected document d and so chosen with probability of P(t|d). Finally, given this chosen topic t, a word w (or sentence) is generated from it with probability P(w|t), as shown in Figure. It is noted that the values of P(d) is determined directly from the corpus D which is defined in terms of a DTM matrix. In contrast, the probabilities P(t|d) and P(w|t) are parameters modelled as multinomial distributions and iteratively updated via the Expectation-Maximization (EM) algorithm. Direct parallelism between LSA and pLSA can be drawn via the methods’ parameterization, as conveyed via matching colours of the topic-word matrix and P(w|t), the document-topic matrix and P(d|t) as well as the topic-prevalence matrix and P(t) displayed in Figure and Figure, respectively. \n",
        "\n",
        "Despite pLSA implicitly addressing LSA-related disadvantages, this method still involves two main drawbacks. There is no probability model for the document-topic probabilities P(t|d), resulting in the inability to assign topic mixtures to new, unseen documents not trained on. Model parameters also then increase linearly with the number of documents added, making this method more susceptible to overfitting. \n",
        "\n",
        "<b><u>  Latent Dirichlet Allocation </b></u>\n",
        "\n",
        "![](LDA.png){fig-alt=\"Schematic representation of LDA.\"}\n",
        "\n",
        "LDA is another generative, probabilistic model which can be deemed as a hierarchical Bayesian version of pLSA. Via explicitly defining a generative model for the document-topic probabilities, both the above-mentioned pitfalls of pLSA are improved upon. The number of parameters to estimate drastically decrease and the ability to apply and generalize to new, unseen documents is attainable. As presented in Figure, the initial steps first involve randomly sampling a document-topic probability distribution ($\\theta$) from a Dirichlet (Dir) distribution ($\\eta$), followed by randomly sampling a topic-word probability distribution ($\\phi$) from another Dirichlet distribution ($\\tau$). From the $\\theta$ distribution, a topic t is selected by drawing from a multinomial (Mult) distribution (third step) and from the $\\phi$ distribution given said topic t, a word w (or sentences) is sampled from another multinomial distribution (fourth step). The associated LDA-parameters are then estimated via a variational expectation maximization algorithm or collapsed Gibbs sampling. \n",
        "\n",
        "\n",
        "<b><u>  Correlated Topic Model (CTM) </b></u>\n",
        "\n",
        "![](CTM.png){fig-alt=\"Schematic representation of CTM.\"}\n",
        "\n",
        "Following closely to LDA, the CTM [@LaffertyBlei2005] additionally allows for the ability to model the presence of any correlated topics.  Such topic correlations are introduced via the inclusion of the multivariate normal (MultNorm) distribution with t length-vector of means ($\\mu$) and t $\\times$ t covariance matrix ($\\Sigma$) where the resulting values are then mapped into probabilities by passing through a logistic (log) transformation. Comparing Figure and Figure, the nuance between LDA and CTM is highlighted in light-blue, where the discrepancy in the models come about from replacing the Dirichlet distribution (which involves the implicit assumption of independence across topics) with the logit-normal distribution (which now explicitly enables for topic dependency via a covariance structure) for generating document-topic probabilities. The other generative processes previously outlined for LDA is retained and repeated for CTM. Given this additional model complexity, the more convoluted mean-field variational inference algorithm is employed for CTM-parameter estimation which necessitate many iterations for optimization purposes. CTM is consequently computationally more expensive than LDA. Though, this snag is far outweighed by the procurement of richer topics with overt relationships acknowledged between these. \n",
        "\n",
        "\n",
        "<b><u>  Author Topic Model (ATM) </b></u>\n",
        "\n",
        "![](ATM.png){fig-alt=\"Schematic representation of ATM.\"}\n",
        "\n",
        "ATM [@RosenZvi2012] extends LDA via the inclusion of authorship information with topics. Again, inspecting Figure and Figure, the slight differences between these two models are accentuated with light-blue colour. Here, for each word w in the document d an author a is sampled uniformly (Uni) at random. Each author is associated with a distribution over topics ($\\Psi$) sampled from a Dirichlet prior $\\alpha$. The resultant mixture weights corresponding to the chosen author are used to select a topic t, then a word w (or sentence) is generated according to the topic-word distribution $\\phi$ (drawn from another Dirichlet prior $\\beta$) corresponding to that said chosen topic t. Therefore, through the estimation of the $\\psi$ and $\\phi$ parameters, not only is information obtained about which topics authors generally relate to, but also a representation of these document contents in terms of these topics, respectively. \n",
        "\n",
        "# Read in the data\n"
      ],
      "id": "815f5897"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Loading in the necessary libraries\n",
        "import zipfile\n",
        "import os\n",
        "import pandas as pd\n",
        "import re\n",
        "import matplotlib.pyplot as plt\n",
        "from wordcloud import WordCloud, STOPWORDS\n",
        "from itertools import cycle\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "from collections import Counter\n",
        "from nltk.tokenize import word_tokenize"
      ],
      "id": "868f323c",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Unzip the file and get the list of filenames\n",
        "with zipfile.ZipFile(\"data/speeches.zip\", 'r') as zip_ref:\n",
        "    zip_ref.extractall(\"data\")\n",
        "\n",
        "filenames = os.listdir(\"data\")\n",
        "filenames = [filename for filename in filenames if filename.endswith('.txt')]\n",
        "\n",
        "# Read the content of each speech file and extract the date from the first line\n",
        "speeches = []\n",
        "dates = []\n",
        "for filename in filenames:\n",
        "    with open(os.path.join(\"data\", filename), 'r', encoding='utf-8') as file:\n",
        "        # Extract date from the first line\n",
        "        date = file.readline().strip()\n",
        "        dates.append(date)\n",
        "        \n",
        "        # Read the rest of the file\n",
        "        speeches.append(file.read())\n",
        "\n",
        "# Create DataFrame\n",
        "sona = pd.DataFrame({'filename': filenames, 'speech': speeches, 'date': dates})\n",
        "\n",
        "# Extract year and president for each speech\n",
        "sona['year'] = sona['filename'].str[:4]\n",
        "sona['president'] = sona['filename'].str.split('_').str[-1].str.split('.').str[0]\n",
        "\n",
        "# Clean the sona dataset by removing unnecessary text\n",
        "replace_reg = r'(http.*?(\\s|.$))|(www.*?(\\s|.$))|&amp;|&lt;|&gt;|\\n'\n",
        "sona['speech'] = sona['speech'].str.replace(replace_reg, ' ')\n",
        "\n",
        "# Split speeches into sentences\n",
        "# sona_sentences = sona['speech'].str.split(r'(?<!\\w\\.\\w.)(?<![A-Z][a-z]\\.)(?<=\\.|\\?)\\s', expand=True).stack().reset_index(level=-1, drop=True)\n",
        "# sona_sentences.name = 'sentence'\n",
        "\n",
        "# # Remove newline characters from the sentences\n",
        "# sona_sentences = sona_sentences.str.replace('\\n', '').str.strip()\n",
        "\n",
        "# # Merge with the president, date, and year columns to associate each sentence with the respective details\n",
        "# df_sentences = sona[['president', 'date', 'year']].join(sona_sentences)\n",
        "\n",
        "# Make a csv of the sentences\n",
        "sona.to_csv('data/sona_speeches.csv', index=False)"
      ],
      "id": "e1438f63",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from matplotlib.colors import LinearSegmentedColormap\n",
        "\n",
        "cmap = plt.cm.cividis\n",
        "\n",
        "norm = plt.Normalize(0, 100)\n",
        "\n",
        "# Define a colour map based on cividis\n",
        "# Define a new colormap using a smaller slice of the cividis colormap, this time stopping well before the yellows\n",
        "cividis_modified = cmap(np.linspace(0, 0.4, cmap.N))  # Using only 40% of the colormap range\n",
        "\n",
        "# Create a new colormap from the data\n",
        "cividis_no_yellow_light = LinearSegmentedColormap.from_list('cividis_no_yellow_light', cividis_modified)\n",
        "\n",
        "# Let's pick three colors from the modified colormap\n",
        "colors = [cividis_no_yellow_light(norm(0)), \n",
        "          cividis_no_yellow_light(norm(50)), \n",
        "          cividis_no_yellow_light(norm(100))]"
      ],
      "id": "285823d3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import nltk\n",
        "# Make sure to download the necessary NLTK corpus if you haven't already\n",
        "# nltk.download('wordnet')\n",
        "# nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "# Read in the sona speeches dataset\n",
        "sona_speeches_df = pd.read_csv('data/sona_speeches.csv')\n",
        "\n",
        "# Initialize the WordNetLemmatizer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# Function to convert NLTK's part-of-speech tags to WordNet's part-of-speech tags\n",
        "def get_wordnet_pos(word):\n",
        "    \"\"\"Map NLTK part of speech tags to WordNet part of speech tags.\"\"\"\n",
        "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
        "    tag_dict = {\"J\": nltk.corpus.wordnet.ADJ,\n",
        "                \"N\": nltk.corpus.wordnet.NOUN,\n",
        "                \"V\": nltk.corpus.wordnet.VERB,\n",
        "                \"R\": nltk.corpus.wordnet.ADV}\n",
        "\n",
        "    return tag_dict.get(tag, nltk.corpus.wordnet.NOUN)\n",
        "\n",
        "# Clean the text, convert to lowercase, and lemmatize each word\n",
        "def clean_text(text):\n",
        "    # Remove special characters: keep only letters, numbers, and basic punctuation\n",
        "    text = re.sub(r'[.;]', ' ', text)  # Replaces periods with spaces\n",
        "    text = re.sub(r'[^a-zA-Z0-9 ]', '', text)\n",
        "    text = text.lower()  # Convert to lowercase\n",
        "    \n",
        "    # Tokenize the text\n",
        "    words = word_tokenize(text)\n",
        "    \n",
        "    # Lemmatize each word with its POS tag\n",
        "    lemmatized_words = [lemmatizer.lemmatize(w, get_wordnet_pos(w)) for w in words]\n",
        "    \n",
        "    # Join the lemmatized words back into one string\n",
        "    text = ' '.join(lemmatized_words)\n",
        "    return text\n",
        "\n",
        "# Apply the cleaning function to the speech column\n",
        "sona_speeches_df['speech'] = sona_speeches_df['speech'].apply(clean_text)\n",
        "\n",
        "# Make a csv of the sentences\n",
        "sona_speeches_df.to_csv('data/sona_speeches_adapted.csv', index=False)"
      ],
      "id": "89b4c2ef",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Exploratory Data Analysis\n"
      ],
      "id": "7a0f2f19"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
        "from collections import Counter\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Function to count words in speeches excluding stopwords\n",
        "def get_word_frequencies(speeches, stopwords):\n",
        "    word_counts = Counter()\n",
        "    for speech in speeches:\n",
        "        words = speech.lower().split()\n",
        "        # Remove stopwords from the count\n",
        "        words = [word.strip('.,!?\"\\'-()') for word in words if word.strip('.,!?\"\\'-()') not in stopwords]\n",
        "        word_counts.update(words)\n",
        "    return word_counts\n",
        "\n",
        "# Get the word frequencies excluding stopwords\n",
        "word_frequencies = get_word_frequencies(sona_speeches_df['speech'], ENGLISH_STOP_WORDS)\n",
        "\n",
        "# Get the top 10 most frequent words across all speeches\n",
        "top_10_words = word_frequencies.most_common(10)\n",
        "\n",
        "cividis_no_yellow_light(norm(100))\n",
        "\n",
        "# Plotting\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.bar([word for word, count in top_10_words], [count for word, count in top_10_words], color=colors[0])\n",
        "plt.xlabel('Words')\n",
        "plt.ylabel('Frequency')\n",
        "plt.xticks(rotation=45)\n",
        "plt.show()\n",
        "\n",
        "# Define the directory name\n",
        "directory = \"saved_plots\"\n",
        "\n",
        "# Check if the directory exists\n",
        "if not os.path.exists(directory):\n",
        "    # If it does not exist, create it\n",
        "    os.makedirs(directory)\n",
        "\n",
        "# Now you can safely save your file within 'saved_plots' directory\n",
        "pdf_filename = f'{directory}/top_10_words_across_speeches_chart.pdf'\n",
        "plt.savefig(pdf_filename, format='pdf', bbox_inches='tight')\n",
        "\n",
        "# Closing the figure to prevent it from displaying again in the output\n",
        "plt.close()\n",
        "\n",
        "pdf_filename"
      ],
      "id": "ca3020d3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Function to get top N frequent words for each president\n",
        "def get_top_words_by_president(speeches_df, n, stopwords):\n",
        "    presidents = speeches_df['president'].unique()\n",
        "    top_words_by_president = {}\n",
        "    for president in presidents:\n",
        "        president_speeches = speeches_df[speeches_df['president'] == president]['speech']\n",
        "        word_frequencies = get_word_frequencies(president_speeches, stopwords)\n",
        "        top_words_by_president[president] = word_frequencies.most_common(n)\n",
        "    return top_words_by_president\n",
        "\n",
        "# Get the top 10 most frequent words for each president\n",
        "top_10_words_by_president = get_top_words_by_president(sona_speeches_df, 10, ENGLISH_STOP_WORDS)\n",
        "\n",
        "# Plotting the bar graph for the top 10 most frequent words faceted by president\n",
        "fig, axes = plt.subplots(nrows=len(top_10_words_by_president), ncols=1, figsize=(10, 6 * len(top_10_words_by_president)))\n",
        "fig.suptitle('Top 10 Most Frequent Words by President (Excluding Stopwords)', y=1.02)\n",
        "\n",
        "for i, (president, top_words) in enumerate(top_10_words_by_president.items()):\n",
        "    axes[i].bar([word for word, count in top_words], [count for word, count in top_words])\n",
        "    axes[i].set_title(president)\n",
        "    axes[i].set_xlabel('Words')\n",
        "    axes[i].set_ylabel('Frequency')\n",
        "    axes[i].tick_params(axis='x', rotation=45)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "id": "85bb5b68",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# We will first identify the overall top words to track over time.\n",
        "overall_top_words = [word for word, count in top_10_words]\n",
        "\n",
        "# Function to get yearly frequency of specific words\n",
        "def get_yearly_word_trends(speeches_df, words):\n",
        "    yearly_word_trends = {word: [] for word in words}\n",
        "    years = sorted(speeches_df['year'].unique())\n",
        "    \n",
        "    for year in years:\n",
        "        year_speeches = speeches_df[speeches_df['year'] == year]['speech']\n",
        "        word_counts = get_word_frequencies(year_speeches, ENGLISH_STOP_WORDS)\n",
        "        for word in words:\n",
        "            yearly_word_trends[word].append(word_counts[word])\n",
        "    \n",
        "    return years, yearly_word_trends\n",
        "\n",
        "# Get the yearly trends for the overall top words\n",
        "years, yearly_word_trends = get_yearly_word_trends(sona_speeches_df, overall_top_words)\n",
        "\n",
        "# Plotting the line graph for how the most frequent words trend over time across speeches\n",
        "plt.figure(figsize=(14, 7))\n",
        "\n",
        "for word in overall_top_words:\n",
        "    plt.plot(years, yearly_word_trends[word], label=word, marker='o')\n",
        "\n",
        "plt.title('Trends of Most Frequent Words Over Time in SONA Speeches (Excluding Stopwords)')\n",
        "plt.xlabel('Year')\n",
        "plt.ylabel('Frequency')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.xticks(years, rotation=45)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "id": "6f204327",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Function to get yearly frequency of specific words for each president\n",
        "def get_presidential_yearly_word_trends(speeches_df, words):\n",
        "    presidents = speeches_df['president'].unique()\n",
        "    presidential_word_trends = {president: {word: [] for word in words} for president in presidents}\n",
        "    presidential_years = {president: sorted(speeches_df[speeches_df['president'] == president]['year'].unique()) for president in presidents}\n",
        "    \n",
        "    for president in presidents:\n",
        "        for year in presidential_years[president]:\n",
        "            year_speeches = speeches_df[(speeches_df['year'] == year) & (speeches_df['president'] == president)]['speech']\n",
        "            word_counts = get_word_frequencies(year_speeches, ENGLISH_STOP_WORDS)\n",
        "            for word in words:\n",
        "                presidential_word_trends[president][word].append(word_counts[word])\n",
        "    \n",
        "    return presidential_years, presidential_word_trends\n",
        "\n",
        "# Get the presidential yearly trends for the overall top words\n",
        "presidential_years, presidential_word_trends = get_presidential_yearly_word_trends(sona_speeches_df, overall_top_words)\n",
        "\n",
        "# Plotting the line graphs for the most frequent words for each president over time\n",
        "for president in presidential_years:\n",
        "    plt.figure(figsize=(14, 7))\n",
        "\n",
        "    for word in overall_top_words:\n",
        "        plt.plot(presidential_years[president], presidential_word_trends[president][word], label=word, marker='o')\n",
        "\n",
        "    plt.title(f'Trends of Most Frequent Words Over Time for President {president} (Excluding Stopwords)')\n",
        "    plt.xlabel('Year')\n",
        "    plt.ylabel('Frequency')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.xticks(presidential_years[president], rotation=45)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n"
      ],
      "id": "8e973544",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from itertools import chain\n",
        "\n",
        "# Define the list of negation words\n",
        "negation_words = ['not', 'no', 'never', 'without', 'nor']\n",
        "\n",
        "# Function to get top N frequent bigrams for the given speeches\n",
        "def get_top_negation_bigrams(speeches, n, negation_words):\n",
        "    vectorizer = CountVectorizer(ngram_range=(2, 2), stop_words='english')\n",
        "    X = vectorizer.fit_transform(speeches)\n",
        "    feature_names = vectorizer.get_feature_names_out()\n",
        "    bigram_frequencies = zip(feature_names, X.toarray().sum(axis=0))\n",
        "    # TODO filter actual negation words\n",
        "    # Filter bigrams to only keep those with negation words\n",
        "    negation_bigrams = [(bigram, freq) for bigram, freq in bigram_frequencies if any(neg_word in bigram for neg_word in negation_words)]\n",
        "    negation_bigrams = sorted(negation_bigrams, key=lambda x: x[1], reverse=True)[:n]\n",
        "    \n",
        "    return negation_bigrams\n",
        "\n",
        "# Get the top 10 most frequent negation bigrams across all speeches\n",
        "top_10_negation_bigrams = get_top_negation_bigrams(sona_speeches_df['speech'], 10, negation_words)\n",
        "\n",
        "# Plotting the bar graph for the top 10 most frequent negation bigrams across speeches\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.bar([bigram for bigram, count in top_10_negation_bigrams], [count for bigram, count in top_10_negation_bigrams])\n",
        "plt.title('Top 10 Most Frequent Negation Bigrams Across SONA Speeches')\n",
        "plt.xlabel('Bigrams')\n",
        "plt.ylabel('Frequency')\n",
        "plt.xticks(rotation=45)\n",
        "plt.show()\n"
      ],
      "id": "e4028e8e",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Function to get top N frequent negation bigrams for each president\n",
        "def get_top_negation_bigrams_by_president(speeches_df, n, negation_words):\n",
        "    presidents = speeches_df['president'].unique()\n",
        "    top_negation_bigrams_by_president = {}\n",
        "    for president in presidents:\n",
        "        president_speeches = speeches_df[speeches_df['president'] == president]['speech']\n",
        "        negation_bigrams = get_top_negation_bigrams(president_speeches, n, negation_words)\n",
        "        top_negation_bigrams_by_president[president] = negation_bigrams\n",
        "    return top_negation_bigrams_by_president\n",
        "\n",
        "# Get the top 10 most frequent negation bigrams for each president\n",
        "top_10_negation_bigrams_by_president = get_top_negation_bigrams_by_president(sona_speeches_df, 10, negation_words)\n",
        "\n",
        "# Plotting the bar graph for the top 10 most frequent negation bigrams faceted by president\n",
        "fig, axes = plt.subplots(nrows=len(top_10_negation_bigrams_by_president), ncols=1, figsize=(12, 6 * len(top_10_negation_bigrams_by_president)))\n",
        "fig.suptitle('Top 10 Most Frequent Negation Bigrams by President', y=1.02)\n",
        "\n",
        "for i, (president, negation_bigrams) in enumerate(top_10_negation_bigrams_by_president.items()):\n",
        "    axes[i].bar([bigram for bigram, count in negation_bigrams], [count for bigram, count in negation_bigrams])\n",
        "    axes[i].set_title(president)\n",
        "    axes[i].set_xlabel('Bigrams')\n",
        "    axes[i].set_ylabel('Frequency')\n",
        "    axes[i].tick_params(axis='x', rotation=45)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "id": "f162a86e",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# We will first identify the overall top negation bigrams to track over time.\n",
        "overall_top_negation_bigrams = [bigram for bigram, count in top_10_negation_bigrams]\n",
        "\n",
        "# Function to get yearly frequency of specific bigrams\n",
        "def get_yearly_bigram_trends(speeches_df, bigrams):\n",
        "    yearly_bigram_trends = {bigram: [] for bigram in bigrams}\n",
        "    years = sorted(speeches_df['year'].unique())\n",
        "    \n",
        "    for year in years:\n",
        "        year_speeches = speeches_df[speeches_df['year'] == year]['speech']\n",
        "        negation_bigrams = get_top_negation_bigrams(year_speeches, None, negation_words)  # None for n to get all\n",
        "        negation_bigrams_dict = dict(negation_bigrams)\n",
        "        \n",
        "        for bigram in bigrams:\n",
        "            yearly_bigram_trends[bigram].append(negation_bigrams_dict.get(bigram, 0))\n",
        "    \n",
        "    return years, yearly_bigram_trends\n",
        "\n",
        "# Get the yearly trends for the overall top negation bigrams\n",
        "years, yearly_negation_bigram_trends = get_yearly_bigram_trends(sona_speeches_df, overall_top_negation_bigrams)\n",
        "\n",
        "# Plotting the line graph for how the most frequent negation bigrams trend over time across speeches\n",
        "plt.figure(figsize=(14, 7))\n",
        "\n",
        "for bigram in overall_top_negation_bigrams:\n",
        "    plt.plot(years, yearly_negation_bigram_trends[bigram], label=bigram, marker='o')\n",
        "\n",
        "plt.title('Trends of Most Frequent Negation Bigrams Over Time in SONA Speeches')\n",
        "plt.xlabel('Year')\n",
        "plt.ylabel('Frequency')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.xticks(years, rotation=45)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "id": "19106783",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Sentiment analysis\n"
      ],
      "id": "b90f4caa"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from textblob import TextBlob\n",
        "from afinn import Afinn\n",
        "from nltk.corpus import opinion_lexicon\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# Make sure to download the necessary NLTK corpora if you haven't already\n",
        "import nltk\n",
        "# nltk.download('opinion_lexicon')\n",
        "# nltk.download('punkt')\n",
        "\n",
        "# Initialize Afinn and prepare Bing lexicon\n",
        "afinn = Afinn()\n",
        "positive_words = set(opinion_lexicon.positive())\n",
        "negative_words = set(opinion_lexicon.negative())\n",
        "\n",
        "# Function to calculate sentiment scores using Afinn\n",
        "def afinn_sentiment(text):\n",
        "    return afinn.score(text)\n",
        "\n",
        "# Function to calculate sentiment scores using Bing lexicon\n",
        "def bing_sentiment(word):\n",
        "    if word in positive_words:\n",
        "        return 1  # Positive sentiment\n",
        "    elif word in negative_words:\n",
        "        return -1  # Negative sentiment\n",
        "    else:\n",
        "        return 0  # Neutral sentiment\n",
        "\n",
        "# Function to analyze the sentiment of each word in the speech\n",
        "def analyze_sentiment(speech):\n",
        "    # Tokenize words\n",
        "    words = word_tokenize(speech)\n",
        "    \n",
        "    # TextBlob\n",
        "    tb_sentiments = [TextBlob(word).sentiment.polarity for word in words]\n",
        "    \n",
        "    # Afinn\n",
        "    afinn_sentiments = [afinn_sentiment(word) for word in words]\n",
        "    \n",
        "    # Bing\n",
        "    bing_sentiments = [bing_sentiment(word) for word in words]\n",
        "    \n",
        "    # Aggregate results\n",
        "    sentiment_data = pd.DataFrame({\n",
        "        'word': words,\n",
        "        'textblob': tb_sentiments,\n",
        "        'afinn': afinn_sentiments,\n",
        "        'bing': bing_sentiments\n",
        "    })\n",
        "    \n",
        "    return sentiment_data\n",
        "\n",
        "# Apply the sentiment analysis\n",
        "all_sentiments = pd.concat([analyze_sentiment(speech) for speech in sona_speeches_df['speech']])\n",
        "\n",
        "# Group by word and calculate mean sentiment\n",
        "aggregated_sentiments = all_sentiments.groupby('word').agg('mean').reset_index()\n",
        "\n",
        "# Function to plot bar graphs for the words that contribute most to sentiment\n",
        "def plot_sentiment_words(sentiment_df, lexicon_name):\n",
        "    top_positive = sentiment_df[sentiment_df[lexicon_name] > 0].sort_values(by=lexicon_name, ascending=False).head(10)\n",
        "    top_negative = sentiment_df[sentiment_df[lexicon_name] < 0].sort_values(by=lexicon_name).head(10)\n",
        "    \n",
        "    fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
        "    axes[0].barh(top_positive['word'], top_positive[lexicon_name])\n",
        "    axes[0].set_title('Top Positive Words - ' + lexicon_name)\n",
        "    axes[0].invert_yaxis()  # labels read top-to-bottom\n",
        "    axes[0].set_xlabel('Sentiment Score')\n",
        "\n",
        "    axes[1].barh(top_negative['word'], top_negative[lexicon_name])\n",
        "    axes[1].set_title('Top Negative Words - ' + lexicon_name)\n",
        "    axes[1].invert_yaxis()  # labels read top-to-bottom\n",
        "    axes[1].set_xlabel('Sentiment Score')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Plot for each lexicon\n",
        "plot_sentiment_words(aggregated_sentiments, 'textblob')\n",
        "plot_sentiment_words(aggregated_sentiments, 'afinn')\n",
        "plot_sentiment_words(aggregated_sentiments, 'bing')\n"
      ],
      "id": "7413bb89",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import re\n",
        "import nltk\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import opinion_lexicon\n",
        "from textblob import TextBlob\n",
        "from afinn import Afinn\n",
        "from nltk.util import bigrams\n",
        "from itertools import chain\n",
        "\n",
        "# Initialize the WordNetLemmatizer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# Function to convert NLTK's part-of-speech tags to WordNet's part-of-speech tags\n",
        "def get_wordnet_pos(word):\n",
        "    \"\"\"Map NLTK part of speech tags to WordNet part of speech tags.\"\"\"\n",
        "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
        "    tag_dict = {\"J\": nltk.corpus.wordnet.ADJ,\n",
        "                \"N\": nltk.corpus.wordnet.NOUN,\n",
        "                \"V\": nltk.corpus.wordnet.VERB,\n",
        "                \"R\": nltk.corpus.wordnet.ADV}\n",
        "    return tag_dict.get(tag, nltk.corpus.wordnet.NOUN)\n",
        "\n",
        "# Clean the text, convert to lowercase, and lemmatize each word\n",
        "def clean_text_bi(text):\n",
        "    # Remove special characters: keep only letters, numbers, and basic punctuation\n",
        "    text = re.sub(r'[.]', ' ', text)  # Replaces periods with spaces\n",
        "    text = re.sub(r'[^a-zA-Z0-9 ]', '', text)\n",
        "    text = text.lower()  # Convert to lowercase\n",
        "    \n",
        "    # Tokenize the text\n",
        "    words = word_tokenize(text)\n",
        "    \n",
        "    # Lemmatize each word with its POS tag\n",
        "    lemmatized_words = [lemmatizer.lemmatize(w, get_wordnet_pos(w)) for w in words]\n",
        "    \n",
        "    return lemmatized_words\n",
        "\n",
        "# Apply the cleaning and lemmatization to the dataset\n",
        "sona_speeches_df['bigram_words'] = sona_speeches_df['speech'].apply(clean_text_bi)\n",
        "\n",
        "# Initialize Afinn and prepare Bing lexicon\n",
        "afinn = Afinn()\n",
        "positive_words = set(opinion_lexicon.positive())\n",
        "negative_words = set(opinion_lexicon.negative())\n",
        "\n",
        "# Function to calculate sentiment scores using Afinn\n",
        "def afinn_sentiment(text):\n",
        "    return afinn.score(text)\n",
        "\n",
        "# Function to calculate sentiment scores using Bing lexicon\n",
        "def bing_sentiment(word):\n",
        "    if word in positive_words:\n",
        "        return 1  # Positive sentiment\n",
        "    elif word in negative_words:\n",
        "        return -1  # Negative sentiment\n",
        "    else:\n",
        "        return 0  # Neutral sentiment\n",
        "\n",
        "# Function to get sentiment score for a bigram\n",
        "def bigram_sentiment(bigram, lexicon_score_func):\n",
        "    return lexicon_score_func(' '.join(bigram))\n",
        "\n",
        "# Function to analyze the sentiment of bigrams\n",
        "def analyze_bigram_sentiment(cleaned_words_list):\n",
        "    bigram_list = list(bigrams(cleaned_words_list))\n",
        "    \n",
        "    # Calculate sentiment for each bigram\n",
        "    tb_sentiments = [TextBlob(' '.join(bigram)).sentiment.polarity for bigram in bigram_list]\n",
        "    afinn_sentiments = [afinn_sentiment(' '.join(bigram)) for bigram in bigram_list]\n",
        "    bing_sentiments = [bigram_sentiment(bigram, bing_sentiment) for bigram in bigram_list]\n",
        "    \n",
        "    sentiment_data = pd.DataFrame({\n",
        "        'bigram': [' '.join(bigram) for bigram in bigram_list],\n",
        "        'textblob': tb_sentiments,\n",
        "        'afinn': afinn_sentiments,\n",
        "        'bing': bing_sentiments\n",
        "    })\n",
        "    \n",
        "    return sentiment_data\n",
        "\n",
        "# Apply the sentiment analysis for bigrams\n",
        "all_bigram_sentiments = pd.concat([analyze_bigram_sentiment(words) for words in sona_speeches_df['bigram_words']])\n",
        "\n",
        "# Group by bigram and calculate mean sentiment\n",
        "aggregated_bigram_sentiments = all_bigram_sentiments.groupby('bigram').agg('mean').reset_index()\n",
        "\n",
        "# Function to plot bar graphs for the bigrams that contribute most to sentiment\n",
        "def plot_bigram_sentiment_words(sentiment_df, lexicon_name):\n",
        "    top_positive_bigrams = sentiment_df[sentiment_df[lexicon_name] > 0].sort_values(by=lexicon_name, ascending=False).head(10)\n",
        "    top_negative_bigrams = sentiment_df[sentiment_df[lexicon_name] < 0].sort_values(by=lexicon_name).head(10)\n",
        "    \n",
        "    fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
        "    axes[0].barh(top_positive_bigrams['bigram'], top_positive_bigrams[lexicon_name])\n",
        "    axes[0].set_title(f'Top Positive Bigrams - {lexicon_name}')\n",
        "    axes[0].invert_yaxis()  # labels read top-to-bottom\n",
        "    axes[0].set_xlabel('Sentiment Score')\n",
        "\n",
        "    axes[1].barh(top_negative_bigrams['bigram'], top_negative_bigrams[lexicon_name])\n",
        "    axes[1].set_title(f'Top Negative Bigrams - {lexicon_name}')\n",
        "    axes[1].invert_yaxis()  # labels read top-to-bottom\n",
        "    axes[1].set_xlabel('Sentiment Score')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Plot for each lexicon\n",
        "plot_bigram_sentiment_words(aggregated_bigram_sentiments, 'textblob')\n",
        "plot_bigram_sentiment_words(aggregated_bigram_sentiments, 'afinn')\n",
        "\n",
        "# TODO: Fix Bing lexicon for bigrams\n",
        "plot_bigram_sentiment_words(aggregated_bigram_sentiments, 'bing')\n"
      ],
      "id": "985b7424",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Function to calculate overall sentiment for a speech using TextBlob\n",
        "def textblob_sentiment(text):\n",
        "    return TextBlob(text).sentiment.polarity\n",
        "\n",
        "# Function to calculate overall sentiment for a speech using Afinn (normalized)\n",
        "def afinn_sentiment(text):\n",
        "    afinn = Afinn()\n",
        "    words = text.split()  # Split text into words\n",
        "    if len(words) > 0:   # Avoid division by zero\n",
        "        return afinn.score(text) / len(words)  # Normalized score\n",
        "    else:\n",
        "        return 0\n",
        "\n",
        "\n",
        "# Calculate sentiment scores\n",
        "sona_speeches_df['textblob_sentiment'] = sona_speeches_df['speech'].apply(textblob_sentiment)\n",
        "sona_speeches_df['afinn_sentiment'] = sona_speeches_df['speech'].apply(afinn_sentiment)\n",
        "\n",
        "# Ensure the 'year' column is of type int if it's not already\n",
        "sona_speeches_df['year'] = sona_speeches_df['year'].astype(int)\n",
        "\n",
        "# Sort by year\n",
        "sona_speeches_df.sort_values('year', inplace=True)\n",
        "\n",
        "# Plotting the sentiment trend over time\n",
        "plt.figure(figsize=(14, 7))\n",
        "\n",
        "# TextBlob sentiment trend\n",
        "plt.plot(sona_speeches_df['year'], sona_speeches_df['textblob_sentiment'], label='TextBlob Sentiment')\n",
        "\n",
        "# Afinn sentiment trend\n",
        "plt.plot(sona_speeches_df['year'], sona_speeches_df['afinn_sentiment'], label='Afinn Sentiment', alpha=0.7)\n",
        "\n",
        "plt.title('Sentiment Trend of SONA Speeches Over Time')\n",
        "plt.xlabel('Year')\n",
        "plt.ylabel('Sentiment Score')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ],
      "id": "6af4ffbc",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Topic modelling\n"
      ],
      "id": "57182bd5"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import re\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from scipy import linalg, spatial\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.decomposition import PCA, SparsePCA, TruncatedSVD\n",
        "from sklearn.feature_extraction.text import (CountVectorizer, TfidfTransformer, TfidfVectorizer)\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "from sklearn.utils.extmath import randomized_svd\n",
        "\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# Load dataset - already lemmatized and cleaned, but still has stopwords\n",
        "df = pd.read_csv('data/sona_speeches_adapted.csv')\n",
        "\n",
        "# Define the stop words\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "# Function to clean and remove stopwords\n",
        "def remove_stopwords(text):\n",
        "    return ' '.join([word.strip('.,!?\"\\'-()') for word in text.split() if word.strip('.,!?\"\\'-()').lower() not in stop_words])\n",
        "\n",
        "# Remove numeric values from the text\n",
        "df['speech'] = df['speech'].str.replace(r'\\d+', '')\n",
        "\n",
        "# Apply the function to each speech\n",
        "df['speech'] = df['speech'].apply(remove_stopwords)\n",
        "\n",
        "# Save the csv of the cleaned speeches without stopwords\n",
        "df.to_csv('data/sona_speeches_no_stopwords.csv', index=False)"
      ],
      "id": "d917d824",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## LSA\n"
      ],
      "id": "4229bc11"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Old code here\n",
        "\n",
        "# import pandas as pd\n",
        "# from gensim import corpora, models, matutils\n",
        "# from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# # Using TF-IDF\n",
        "# tfidf_vectorizer = TfidfVectorizer()\n",
        "# tfidf_matrix = tfidf_vectorizer.fit_transform(df['speech']) # Ensure 'speech_text' is the correct column name\n",
        "\n",
        "# # Convert to Gensim format\n",
        "# corpus = matutils.Sparse2Corpus(tfidf_matrix, documents_columns=False)\n",
        "# dictionary = corpora.Dictionary.from_corpus(corpus, id2word=dict((id, word) for word, id in tfidf_vectorizer.vocabulary_.items()))\n",
        "\n",
        "# # LSA model\n",
        "# num_topics = 5  # or however many topics you want\n",
        "# lsi_model = models.LsiModel(corpus=corpus, id2word=dictionary, num_topics=num_topics)\n",
        "\n",
        "# # Now you can print topics or save the model\n",
        "# lsi_topics = lsi_model.print_topics()\n",
        "# for topic in lsi_topics:\n",
        "#     print(topic)\n"
      ],
      "id": "3cdad9e6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### K-means version\n"
      ],
      "id": "49c252c4"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Read in the cleaned csv file for the analysis\n",
        "df = pd.read_csv('data/sona_speeches_no_stopwords.csv')\n",
        "\n",
        "filtered_document= []\n",
        "filtered_text = []\n",
        "\n",
        "# Catch any stopwords that might have been missed.\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "for document in df['speech']:\n",
        "    \n",
        "    clean_document = \" \".join(re.sub(r\"[^A-Za-z \\—]+\", \" \", document).split())\n",
        "    \n",
        "    document_tokens = word_tokenize(clean_document)\n",
        "\n",
        "    for word in document_tokens:\n",
        "        if word not in stop_words:\n",
        "            filtered_document.append(word)\n",
        "\n",
        "    filtered_text.append(' '.join(filtered_document))"
      ],
      "id": "cf9c038f",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "vectorizer = CountVectorizer()\n",
        "\n",
        "counts_matrix = vectorizer.fit_transform(filtered_text)\n",
        "\n",
        "# Get the feature names from the vectorizer\n",
        "feature_names = vectorizer.get_feature_names_out()\n",
        "\n",
        "count_matrix_df = pd.DataFrame(counts_matrix.toarray(), columns=feature_names)\n",
        "count_matrix_df.index = df['president']\n",
        "\n",
        "print(\"Word frequency matrix: \\n\", count_matrix_df)"
      ],
      "id": "d089e8fd",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Convert the set of stop words to a list\n",
        "stop_words_list = list(stop_words)\n",
        "\n",
        "vectorizer = TfidfVectorizer(stop_words=stop_words_list, \n",
        "max_features=10000, max_df=0.5, use_idf=True, ngram_range=(1,1))\n",
        "\n",
        "X = vectorizer.fit_transform(filtered_text)\n",
        "print(X.shape)\n",
        "\n",
        "# Retrieve feature names using the correct attribute\n",
        "feature_names = vectorizer.get_feature_names_out()\n",
        "print(feature_names)\n",
        "\n",
        "num_clusters = 4\n",
        "\n",
        "km = KMeans(n_clusters=num_clusters)\n",
        "km.fit(X)\n",
        "\n",
        "clusters = km.labels_.tolist()\n",
        "print(clusters)\n"
      ],
      "id": "2aeb9af8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "U, Sigma, VT = randomized_svd(X, n_components=10, n_iter=100, random_state=122)\n",
        "\n",
        "svd_model = TruncatedSVD(n_components=2, algorithm='randomized', n_iter=100, random_state=122)\n",
        "\n",
        "svd_model.fit(X)\n",
        "    \n",
        "print(U.shape)\n",
        "\n",
        "for i, comp in enumerate(VT):\n",
        "    terms_comp = zip(feature_names, comp)\n",
        "    sorted_terms = sorted(terms_comp, key= lambda x:x[1], reverse=True)[:7]\n",
        "    print(\"Cluster \"+str(i)+\": \")\n",
        "    for t in sorted_terms:\n",
        "        print(t[0])\n",
        "    print(\" \")"
      ],
      "id": "1ca51fcf",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Coherence version\n"
      ],
      "id": "41f4927f"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#import modules\n",
        "import os.path\n",
        "from gensim import corpora\n",
        "from gensim.models import LsiModel\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "from gensim.models.coherencemodel import CoherenceModel\n",
        "import matplotlib.pyplot as plt\n"
      ],
      "id": "67680f9a",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Read in the cleaned csv file for the analysis\n",
        "df = pd.read_csv('data/sona_speeches_no_stopwords.csv')"
      ],
      "id": "64bd81ff",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def preprocess_data(doc_set):\n",
        "    \"\"\"\n",
        "    Input  : docuemnt list\n",
        "    Purpose: preprocess text (tokenize, removing stopwords, and stemming)\n",
        "    Output : preprocessed text\n",
        "    \"\"\"\n",
        "    # initialize regex tokenizer\n",
        "    tokenizer = RegexpTokenizer(r'\\w+')\n",
        "    # create English stop words list\n",
        "    en_stop = set(stopwords.words('english'))\n",
        "    # Create p_stemmer of class PorterStemmer\n",
        "    p_stemmer = PorterStemmer()\n",
        "    # list for tokenized documents in loop\n",
        "    texts = []\n",
        "    # loop through document list\n",
        "    for i in doc_set:\n",
        "        # clean and tokenize document string\n",
        "        raw = i.lower()\n",
        "        tokens = tokenizer.tokenize(raw)\n",
        "        # remove stop words from tokens\n",
        "        stopped_tokens = [i for i in tokens if not i in en_stop]\n",
        "        # stem tokens\n",
        "        stemmed_tokens = [p_stemmer.stem(i) for i in stopped_tokens]\n",
        "        # add tokens to list\n",
        "        texts.append(stemmed_tokens)\n",
        "    return texts\n",
        "\n",
        "def prepare_corpus(doc_clean):\n",
        "    \"\"\"\n",
        "    Input  : clean document\n",
        "    Purpose: create term dictionary of our courpus and Converting list of documents (corpus) into Document Term Matrix\n",
        "    Output : term dictionary and Document Term Matrix\n",
        "    \"\"\"\n",
        "    # Creating the term dictionary of our courpus, where every unique term is assigned an index. dictionary = corpora.Dictionary(doc_clean)\n",
        "    dictionary = corpora.Dictionary(doc_clean)\n",
        "    # Converting list of documents (corpus) into Document Term Matrix using dictionary prepared above.\n",
        "    doc_term_matrix = [dictionary.doc2bow(doc) for doc in doc_clean]\n",
        "    # generate LDA model\n",
        "    return dictionary,doc_term_matrix\n",
        "\n",
        "def create_gensim_lsa_model(doc_clean,number_of_topics,words):\n",
        "    \"\"\"\n",
        "    Input  : clean document, number of topics and number of words associated with each topic\n",
        "    Purpose: create LSA model using gensim\n",
        "    Output : return LSA model\n",
        "    \"\"\"\n",
        "    dictionary,doc_term_matrix=prepare_corpus(doc_clean)\n",
        "    # generate LSA model\n",
        "    lsamodel = LsiModel(doc_term_matrix, num_topics=number_of_topics, id2word = dictionary)  # train model\n",
        "    print(lsamodel.print_topics(num_topics=number_of_topics, num_words=words))\n",
        "    return lsamodel\n",
        "\n",
        "\n",
        "def compute_coherence_values(dictionary, doc_term_matrix, doc_clean, stop, start=2, step=3):\n",
        "    \"\"\"\n",
        "    Input   : dictionary : Gensim dictionary\n",
        "              corpus : Gensim corpus\n",
        "              texts : List of input texts\n",
        "              stop : Max num of topics\n",
        "    purpose : Compute c_v coherence for various number of topics\n",
        "    Output  : model_list : List of LSA topic models\n",
        "              coherence_values : Coherence values corresponding to the LDA model with respective number of topics\n",
        "    \"\"\"\n",
        "    coherence_values = []\n",
        "    model_list = []\n",
        "    for num_topics in range(start, stop, step):\n",
        "        # generate LSA model\n",
        "        model = LsiModel(doc_term_matrix, num_topics=num_topics, id2word = dictionary)  # train model\n",
        "        model_list.append(model)\n",
        "        coherencemodel = CoherenceModel(model=model, texts=doc_clean, dictionary=dictionary, coherence='c_v')\n",
        "        coherence_values.append(coherencemodel.get_coherence())\n",
        "    return model_list, coherence_values"
      ],
      "id": "5adebdfb",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def plot_graph(doc_clean,start, stop, step):\n",
        "    dictionary,doc_term_matrix=prepare_corpus(doc_clean)\n",
        "    model_list, coherence_values = compute_coherence_values(dictionary, doc_term_matrix,doc_clean, stop, start, step)\n",
        "    # Show graph\n",
        "    x = range(start, stop, step)\n",
        "    plt.plot(x, coherence_values)\n",
        "    plt.xlabel(\"Number of Topics\")\n",
        "    plt.ylabel(\"Coherence score\")\n",
        "    plt.legend((\"coherence_values\"), loc='best')\n",
        "    plt.show()\n",
        "\n",
        "clean_text=preprocess_data(df['speech'].str.replace(r'[^a-zA-Z\\s]', '', regex=True)) # clean data\n",
        "start,stop,step=2,3,1\n",
        "plot_graph(clean_text,start,stop,step)"
      ],
      "id": "088fc110",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## pLSA (Probabilistic Latent Semantic Analysis)\n"
      ],
      "id": "61597875"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# This is similar to LSA, but with the LdaModel instead\n",
        "# lda_model = models.LdaModel(corpus=corpus, id2word=dictionary, num_topics=num_topics, gamma_threshold=1e-3)\n",
        "\n",
        "# # Now you can print topics or save the model\n",
        "# lda_model.print_topics()"
      ],
      "id": "bf065c7b",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## LDA (Latent Dirichlet Allocation)\n"
      ],
      "id": "8beab73c"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from gensim.utils import simple_preprocess\n",
        "\n",
        "# Tokenize the text\n",
        "tokenized_speeches = [simple_preprocess(speech) for speech in df['speech']]\n",
        "\n",
        "# Create a bag-of-words model for each speech\n",
        "bow_corpus = [dictionary.doc2bow(text) for text in tokenized_speeches]\n",
        "lda_model_bow = models.LdaModel(corpus=bow_corpus, id2word=dictionary, num_topics=num_topics)\n",
        "\n",
        "# LDA model using TF-IDF\n",
        "lda_model_tfidf = models.LdaModel(corpus=corpus, id2word=dictionary, num_topics=num_topics)\n",
        "\n",
        "# Now you can print topics or save the models\n",
        "lda_model_bow.print_topics()\n",
        "lda_model_tfidf.print_topics()"
      ],
      "id": "2ccb1a8f",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## CTM (Correlated Topic Model)\n"
      ],
      "id": "69c37d3e"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import tomotopy as tp\n",
        "\n",
        "# Initialize CTM\n",
        "ctm = tp.CTModel(k=10)  # Replace '10' with the number of topics you want\n",
        "\n",
        "# Add documents to the model\n",
        "for text in df['speech']:\n",
        "    ctm.add_doc(text.split())  # Make sure the texts are tokenized if necessary\n",
        "\n",
        "# Train the model\n",
        "ctm.train(0)\n",
        "for i in range(100):\n",
        "    ctm.train(10)\n",
        "    print('Iteration: {}\\tLog-likelihood: {}'.format(i, ctm.ll_per_word))\n",
        "\n",
        "# Get the topics\n",
        "for i in range(ctm.k):\n",
        "    print(\"Topic #{}:\".format(i), ctm.get_topic_words(i))\n"
      ],
      "id": "4084e9a6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ATM (Author-Topic Model)\n"
      ],
      "id": "7c9b310c"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from gensim.models import AuthorTopicModel\n",
        "\n",
        "# You need to create a mapping of authors (presidents) to documents\n",
        "author2doc = {author: [] for author in df['president'].unique()}\n",
        "for i, row in df.iterrows():\n",
        "    author2doc[row['president']].append(i)\n",
        "\n",
        "# Author-Topic LDA model\n",
        "author_topic_model = AuthorTopicModel(corpus=bow_corpus, author2doc=author2doc, id2word=dictionary, num_topics=num_topics)\n",
        "\n",
        "# Now you can print topics or save the model\n",
        "author_topic_model.print_topics()"
      ],
      "id": "03649888",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "</div>"
      ],
      "id": "78410b66"
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3 (ipykernel)"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}