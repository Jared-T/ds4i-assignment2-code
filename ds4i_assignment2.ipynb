{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {},
      "source": [
        "---\n",
        "title: \"Assignment 2\"\n",
        "format: html\n",
        "author: \"Jared Tavares and Heeeyletje van Zyl\"\n",
        "---"
      ],
      "id": "78225c80"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Introduction \n",
        "--------------------------------------------------------------------------------\n",
        "\n",
        "\n",
        "# Read in the data\n"
      ],
      "id": "9586ba3b"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Loading in the necessary libraries\n",
        "import zipfile\n",
        "import os\n",
        "import pandas as pd\n",
        "import re\n",
        "import matplotlib.pyplot as plt\n",
        "from wordcloud import WordCloud, STOPWORDS\n",
        "from itertools import cycle\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "from collections import Counter\n",
        "from nltk.tokenize import word_tokenize"
      ],
      "id": "9a567562",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Unzip the file and get the list of filenames\n",
        "with zipfile.ZipFile(\"data/speeches.zip\", 'r') as zip_ref:\n",
        "    zip_ref.extractall(\"data\")\n",
        "\n",
        "filenames = os.listdir(\"data\")\n",
        "filenames = [filename for filename in filenames if filename.endswith('.txt')]\n",
        "\n",
        "# Read the content of each speech file and extract the date from the first line\n",
        "speeches = []\n",
        "dates = []\n",
        "for filename in filenames:\n",
        "    with open(os.path.join(\"data\", filename), 'r', encoding='utf-8') as file:\n",
        "        # Extract date from the first line\n",
        "        date = file.readline().strip()\n",
        "        dates.append(date)\n",
        "        \n",
        "        # Read the rest of the file\n",
        "        speeches.append(file.read())\n",
        "\n",
        "# Create DataFrame\n",
        "sona = pd.DataFrame({'filename': filenames, 'speech': speeches, 'date': dates})\n",
        "\n",
        "# Extract year and president for each speech\n",
        "sona['year'] = sona['filename'].str[:4]\n",
        "sona['president'] = sona['filename'].str.split('_').str[-1].str.split('.').str[0]\n",
        "\n",
        "# Clean the sona dataset by removing unnecessary text\n",
        "replace_reg = r'(http.*?(\\s|.$))|(www.*?(\\s|.$))|&amp;|&lt;|&gt;|\\n'\n",
        "sona['speech'] = sona['speech'].str.replace(replace_reg, ' ')\n",
        "\n",
        "# Split speeches into sentences\n",
        "# sona_sentences = sona['speech'].str.split(r'(?<!\\w\\.\\w.)(?<![A-Z][a-z]\\.)(?<=\\.|\\?)\\s', expand=True).stack().reset_index(level=-1, drop=True)\n",
        "# sona_sentences.name = 'sentence'\n",
        "\n",
        "# # Remove newline characters from the sentences\n",
        "# sona_sentences = sona_sentences.str.replace('\\n', '').str.strip()\n",
        "\n",
        "# # Merge with the president, date, and year columns to associate each sentence with the respective details\n",
        "# df_sentences = sona[['president', 'date', 'year']].join(sona_sentences)\n",
        "\n",
        "# Make a csv of the sentences\n",
        "sona.to_csv('data/sona_speeches.csv', index=False)"
      ],
      "id": "d3191433",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import nltk\n",
        "# Make sure to download the necessary NLTK corpus if you haven't already\n",
        "# nltk.download('wordnet')\n",
        "# nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "# Read in the sona speeches dataset\n",
        "sona_speeches_df = pd.read_csv('data/sona_speeches.csv')\n",
        "\n",
        "# Initialize the WordNetLemmatizer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# Function to convert NLTK's part-of-speech tags to WordNet's part-of-speech tags\n",
        "def get_wordnet_pos(word):\n",
        "    \"\"\"Map NLTK part of speech tags to WordNet part of speech tags.\"\"\"\n",
        "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
        "    tag_dict = {\"J\": nltk.corpus.wordnet.ADJ,\n",
        "                \"N\": nltk.corpus.wordnet.NOUN,\n",
        "                \"V\": nltk.corpus.wordnet.VERB,\n",
        "                \"R\": nltk.corpus.wordnet.ADV}\n",
        "\n",
        "    return tag_dict.get(tag, nltk.corpus.wordnet.NOUN)\n",
        "\n",
        "# Clean the text, convert to lowercase, and lemmatize each word\n",
        "def clean_text(text):\n",
        "    # Remove special characters: keep only letters, numbers, and basic punctuation\n",
        "    text = re.sub(r'[.]', ' ', text)  # Replaces periods with spaces\n",
        "    text = re.sub(r'[^a-zA-Z0-9 ]', '', text)\n",
        "    text = text.lower()  # Convert to lowercase\n",
        "    \n",
        "    # Tokenize the text\n",
        "    words = word_tokenize(text)\n",
        "    \n",
        "    # Lemmatize each word with its POS tag\n",
        "    lemmatized_words = [lemmatizer.lemmatize(w, get_wordnet_pos(w)) for w in words]\n",
        "    \n",
        "    # Join the lemmatized words back into one string\n",
        "    text = ' '.join(lemmatized_words)\n",
        "    return text\n",
        "\n",
        "# Apply the cleaning function to the speech column\n",
        "sona_speeches_df['speech'] = sona_speeches_df['speech'].apply(clean_text)\n",
        "\n",
        "# Make a csv of the sentences\n",
        "sona_speeches_df.to_csv('data/sona_speeches_adapted.csv', index=False)"
      ],
      "id": "b8d57fe9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Exploratory Data Analysis\n"
      ],
      "id": "c5853ffa"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
        "from collections import Counter\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Function to count words in speeches excluding stopwords\n",
        "def get_word_frequencies(speeches, stopwords):\n",
        "    word_counts = Counter()\n",
        "    for speech in speeches:\n",
        "        words = speech.lower().split()\n",
        "        # Remove stopwords from the count\n",
        "        words = [word.strip('.,!?\"\\'-()') for word in words if word.strip('.,!?\"\\'-()') not in stopwords]\n",
        "        word_counts.update(words)\n",
        "    return word_counts\n",
        "\n",
        "# Get the word frequencies excluding stopwords\n",
        "word_frequencies = get_word_frequencies(sona_speeches_df['speech'], ENGLISH_STOP_WORDS)\n",
        "\n",
        "# Get the top 10 most frequent words across all speeches\n",
        "top_10_words = word_frequencies.most_common(10)\n",
        "\n",
        "# Plotting the bar graph for the top 10 most frequent words across speeches\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.bar([word for word, count in top_10_words], [count for word, count in top_10_words])\n",
        "plt.title('Top 10 Most Frequent Words Across SONA Speeches')\n",
        "plt.xlabel('Words')\n",
        "plt.ylabel('Frequency')\n",
        "plt.xticks(rotation=45)\n",
        "plt.show()\n"
      ],
      "id": "f4e62bce",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Function to get top N frequent words for each president\n",
        "def get_top_words_by_president(speeches_df, n, stopwords):\n",
        "    presidents = speeches_df['president'].unique()\n",
        "    top_words_by_president = {}\n",
        "    for president in presidents:\n",
        "        president_speeches = speeches_df[speeches_df['president'] == president]['speech']\n",
        "        word_frequencies = get_word_frequencies(president_speeches, stopwords)\n",
        "        top_words_by_president[president] = word_frequencies.most_common(n)\n",
        "    return top_words_by_president\n",
        "\n",
        "# Get the top 10 most frequent words for each president\n",
        "top_10_words_by_president = get_top_words_by_president(sona_speeches_df, 10, ENGLISH_STOP_WORDS)\n",
        "\n",
        "# Plotting the bar graph for the top 10 most frequent words faceted by president\n",
        "fig, axes = plt.subplots(nrows=len(top_10_words_by_president), ncols=1, figsize=(10, 6 * len(top_10_words_by_president)))\n",
        "fig.suptitle('Top 10 Most Frequent Words by President (Excluding Stopwords)', y=1.02)\n",
        "\n",
        "for i, (president, top_words) in enumerate(top_10_words_by_president.items()):\n",
        "    axes[i].bar([word for word, count in top_words], [count for word, count in top_words])\n",
        "    axes[i].set_title(president)\n",
        "    axes[i].set_xlabel('Words')\n",
        "    axes[i].set_ylabel('Frequency')\n",
        "    axes[i].tick_params(axis='x', rotation=45)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "id": "c376d914",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# We will first identify the overall top words to track over time.\n",
        "overall_top_words = [word for word, count in top_10_words]\n",
        "\n",
        "# Function to get yearly frequency of specific words\n",
        "def get_yearly_word_trends(speeches_df, words):\n",
        "    yearly_word_trends = {word: [] for word in words}\n",
        "    years = sorted(speeches_df['year'].unique())\n",
        "    \n",
        "    for year in years:\n",
        "        year_speeches = speeches_df[speeches_df['year'] == year]['speech']\n",
        "        word_counts = get_word_frequencies(year_speeches, ENGLISH_STOP_WORDS)\n",
        "        for word in words:\n",
        "            yearly_word_trends[word].append(word_counts[word])\n",
        "    \n",
        "    return years, yearly_word_trends\n",
        "\n",
        "# Get the yearly trends for the overall top words\n",
        "years, yearly_word_trends = get_yearly_word_trends(sona_speeches_df, overall_top_words)\n",
        "\n",
        "# Plotting the line graph for how the most frequent words trend over time across speeches\n",
        "plt.figure(figsize=(14, 7))\n",
        "\n",
        "for word in overall_top_words:\n",
        "    plt.plot(years, yearly_word_trends[word], label=word, marker='o')\n",
        "\n",
        "plt.title('Trends of Most Frequent Words Over Time in SONA Speeches (Excluding Stopwords)')\n",
        "plt.xlabel('Year')\n",
        "plt.ylabel('Frequency')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.xticks(years, rotation=45)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "id": "717a083e",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Function to get yearly frequency of specific words for each president\n",
        "def get_presidential_yearly_word_trends(speeches_df, words):\n",
        "    presidents = speeches_df['president'].unique()\n",
        "    presidential_word_trends = {president: {word: [] for word in words} for president in presidents}\n",
        "    presidential_years = {president: sorted(speeches_df[speeches_df['president'] == president]['year'].unique()) for president in presidents}\n",
        "    \n",
        "    for president in presidents:\n",
        "        for year in presidential_years[president]:\n",
        "            year_speeches = speeches_df[(speeches_df['year'] == year) & (speeches_df['president'] == president)]['speech']\n",
        "            word_counts = get_word_frequencies(year_speeches, ENGLISH_STOP_WORDS)\n",
        "            for word in words:\n",
        "                presidential_word_trends[president][word].append(word_counts[word])\n",
        "    \n",
        "    return presidential_years, presidential_word_trends\n",
        "\n",
        "# Get the presidential yearly trends for the overall top words\n",
        "presidential_years, presidential_word_trends = get_presidential_yearly_word_trends(sona_speeches_df, overall_top_words)\n",
        "\n",
        "# Plotting the line graphs for the most frequent words for each president over time\n",
        "for president in presidential_years:\n",
        "    plt.figure(figsize=(14, 7))\n",
        "\n",
        "    for word in overall_top_words:\n",
        "        plt.plot(presidential_years[president], presidential_word_trends[president][word], label=word, marker='o')\n",
        "\n",
        "    plt.title(f'Trends of Most Frequent Words Over Time for President {president} (Excluding Stopwords)')\n",
        "    plt.xlabel('Year')\n",
        "    plt.ylabel('Frequency')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.xticks(presidential_years[president], rotation=45)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n"
      ],
      "id": "c9a66187",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from itertools import chain\n",
        "\n",
        "# Define the list of negation words\n",
        "negation_words = ['not', 'no', 'never', 'without', 'nor']\n",
        "\n",
        "# Function to get top N frequent bigrams for the given speeches\n",
        "def get_top_negation_bigrams(speeches, n, negation_words):\n",
        "    vectorizer = CountVectorizer(ngram_range=(2, 2), stop_words='english')\n",
        "    X = vectorizer.fit_transform(speeches)\n",
        "    feature_names = vectorizer.get_feature_names_out()\n",
        "    bigram_frequencies = zip(feature_names, X.toarray().sum(axis=0))\n",
        "    # TODO filter actual negation words\n",
        "    # Filter bigrams to only keep those with negation words\n",
        "    negation_bigrams = [(bigram, freq) for bigram, freq in bigram_frequencies if any(neg_word in bigram for neg_word in negation_words)]\n",
        "    negation_bigrams = sorted(negation_bigrams, key=lambda x: x[1], reverse=True)[:n]\n",
        "    \n",
        "    return negation_bigrams\n",
        "\n",
        "# Get the top 10 most frequent negation bigrams across all speeches\n",
        "top_10_negation_bigrams = get_top_negation_bigrams(sona_speeches_df['speech'], 10, negation_words)\n",
        "\n",
        "# Plotting the bar graph for the top 10 most frequent negation bigrams across speeches\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.bar([bigram for bigram, count in top_10_negation_bigrams], [count for bigram, count in top_10_negation_bigrams])\n",
        "plt.title('Top 10 Most Frequent Negation Bigrams Across SONA Speeches')\n",
        "plt.xlabel('Bigrams')\n",
        "plt.ylabel('Frequency')\n",
        "plt.xticks(rotation=45)\n",
        "plt.show()\n"
      ],
      "id": "5877f9d4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Function to get top N frequent negation bigrams for each president\n",
        "def get_top_negation_bigrams_by_president(speeches_df, n, negation_words):\n",
        "    presidents = speeches_df['president'].unique()\n",
        "    top_negation_bigrams_by_president = {}\n",
        "    for president in presidents:\n",
        "        president_speeches = speeches_df[speeches_df['president'] == president]['speech']\n",
        "        negation_bigrams = get_top_negation_bigrams(president_speeches, n, negation_words)\n",
        "        top_negation_bigrams_by_president[president] = negation_bigrams\n",
        "    return top_negation_bigrams_by_president\n",
        "\n",
        "# Get the top 10 most frequent negation bigrams for each president\n",
        "top_10_negation_bigrams_by_president = get_top_negation_bigrams_by_president(sona_speeches_df, 10, negation_words)\n",
        "\n",
        "# Plotting the bar graph for the top 10 most frequent negation bigrams faceted by president\n",
        "fig, axes = plt.subplots(nrows=len(top_10_negation_bigrams_by_president), ncols=1, figsize=(12, 6 * len(top_10_negation_bigrams_by_president)))\n",
        "fig.suptitle('Top 10 Most Frequent Negation Bigrams by President', y=1.02)\n",
        "\n",
        "for i, (president, negation_bigrams) in enumerate(top_10_negation_bigrams_by_president.items()):\n",
        "    axes[i].bar([bigram for bigram, count in negation_bigrams], [count for bigram, count in negation_bigrams])\n",
        "    axes[i].set_title(president)\n",
        "    axes[i].set_xlabel('Bigrams')\n",
        "    axes[i].set_ylabel('Frequency')\n",
        "    axes[i].tick_params(axis='x', rotation=45)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "id": "4e307c0b",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# We will first identify the overall top negation bigrams to track over time.\n",
        "overall_top_negation_bigrams = [bigram for bigram, count in top_10_negation_bigrams]\n",
        "\n",
        "# Function to get yearly frequency of specific bigrams\n",
        "def get_yearly_bigram_trends(speeches_df, bigrams):\n",
        "    yearly_bigram_trends = {bigram: [] for bigram in bigrams}\n",
        "    years = sorted(speeches_df['year'].unique())\n",
        "    \n",
        "    for year in years:\n",
        "        year_speeches = speeches_df[speeches_df['year'] == year]['speech']\n",
        "        negation_bigrams = get_top_negation_bigrams(year_speeches, None, negation_words)  # None for n to get all\n",
        "        negation_bigrams_dict = dict(negation_bigrams)\n",
        "        \n",
        "        for bigram in bigrams:\n",
        "            yearly_bigram_trends[bigram].append(negation_bigrams_dict.get(bigram, 0))\n",
        "    \n",
        "    return years, yearly_bigram_trends\n",
        "\n",
        "# Get the yearly trends for the overall top negation bigrams\n",
        "years, yearly_negation_bigram_trends = get_yearly_bigram_trends(sona_speeches_df, overall_top_negation_bigrams)\n",
        "\n",
        "# Plotting the line graph for how the most frequent negation bigrams trend over time across speeches\n",
        "plt.figure(figsize=(14, 7))\n",
        "\n",
        "for bigram in overall_top_negation_bigrams:\n",
        "    plt.plot(years, yearly_negation_bigram_trends[bigram], label=bigram, marker='o')\n",
        "\n",
        "plt.title('Trends of Most Frequent Negation Bigrams Over Time in SONA Speeches')\n",
        "plt.xlabel('Year')\n",
        "plt.ylabel('Frequency')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.xticks(years, rotation=45)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "id": "f5913526",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Sentiment analysis\n"
      ],
      "id": "10fff3b9"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from textblob import TextBlob\n",
        "from afinn import Afinn\n",
        "from nltk.corpus import opinion_lexicon\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# Make sure to download the necessary NLTK corpora if you haven't already\n",
        "import nltk\n",
        "# nltk.download('opinion_lexicon')\n",
        "# nltk.download('punkt')\n",
        "\n",
        "# Initialize Afinn and prepare Bing lexicon\n",
        "afinn = Afinn()\n",
        "positive_words = set(opinion_lexicon.positive())\n",
        "negative_words = set(opinion_lexicon.negative())\n",
        "\n",
        "# Function to calculate sentiment scores using Afinn\n",
        "def afinn_sentiment(text):\n",
        "    return afinn.score(text)\n",
        "\n",
        "# Function to calculate sentiment scores using Bing lexicon\n",
        "def bing_sentiment(word):\n",
        "    if word in positive_words:\n",
        "        return 1  # Positive sentiment\n",
        "    elif word in negative_words:\n",
        "        return -1  # Negative sentiment\n",
        "    else:\n",
        "        return 0  # Neutral sentiment\n",
        "\n",
        "# Function to analyze the sentiment of each word in the speech\n",
        "def analyze_sentiment(speech):\n",
        "    # Tokenize words\n",
        "    words = word_tokenize(speech)\n",
        "    \n",
        "    # TextBlob\n",
        "    tb_sentiments = [TextBlob(word).sentiment.polarity for word in words]\n",
        "    \n",
        "    # Afinn\n",
        "    afinn_sentiments = [afinn_sentiment(word) for word in words]\n",
        "    \n",
        "    # Bing\n",
        "    bing_sentiments = [bing_sentiment(word) for word in words]\n",
        "    \n",
        "    # Aggregate results\n",
        "    sentiment_data = pd.DataFrame({\n",
        "        'word': words,\n",
        "        'textblob': tb_sentiments,\n",
        "        'afinn': afinn_sentiments,\n",
        "        'bing': bing_sentiments\n",
        "    })\n",
        "    \n",
        "    return sentiment_data\n",
        "\n",
        "# Apply the sentiment analysis\n",
        "all_sentiments = pd.concat([analyze_sentiment(speech) for speech in sona_speeches_df['speech']])\n",
        "\n",
        "# Group by word and calculate mean sentiment\n",
        "aggregated_sentiments = all_sentiments.groupby('word').agg('mean').reset_index()\n",
        "\n",
        "# Function to plot bar graphs for the words that contribute most to sentiment\n",
        "def plot_sentiment_words(sentiment_df, lexicon_name):\n",
        "    top_positive = sentiment_df[sentiment_df[lexicon_name] > 0].sort_values(by=lexicon_name, ascending=False).head(10)\n",
        "    top_negative = sentiment_df[sentiment_df[lexicon_name] < 0].sort_values(by=lexicon_name).head(10)\n",
        "    \n",
        "    fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
        "    axes[0].barh(top_positive['word'], top_positive[lexicon_name])\n",
        "    axes[0].set_title('Top Positive Words - ' + lexicon_name)\n",
        "    axes[0].invert_yaxis()  # labels read top-to-bottom\n",
        "    axes[0].set_xlabel('Sentiment Score')\n",
        "\n",
        "    axes[1].barh(top_negative['word'], top_negative[lexicon_name])\n",
        "    axes[1].set_title('Top Negative Words - ' + lexicon_name)\n",
        "    axes[1].invert_yaxis()  # labels read top-to-bottom\n",
        "    axes[1].set_xlabel('Sentiment Score')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Plot for each lexicon\n",
        "plot_sentiment_words(aggregated_sentiments, 'textblob')\n",
        "plot_sentiment_words(aggregated_sentiments, 'afinn')\n",
        "plot_sentiment_words(aggregated_sentiments, 'bing')\n"
      ],
      "id": "36daa995",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import re\n",
        "import nltk\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import opinion_lexicon\n",
        "from textblob import TextBlob\n",
        "from afinn import Afinn\n",
        "from nltk.util import bigrams\n",
        "from itertools import chain\n",
        "\n",
        "# Initialize the WordNetLemmatizer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# Function to convert NLTK's part-of-speech tags to WordNet's part-of-speech tags\n",
        "def get_wordnet_pos(word):\n",
        "    \"\"\"Map NLTK part of speech tags to WordNet part of speech tags.\"\"\"\n",
        "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
        "    tag_dict = {\"J\": nltk.corpus.wordnet.ADJ,\n",
        "                \"N\": nltk.corpus.wordnet.NOUN,\n",
        "                \"V\": nltk.corpus.wordnet.VERB,\n",
        "                \"R\": nltk.corpus.wordnet.ADV}\n",
        "    return tag_dict.get(tag, nltk.corpus.wordnet.NOUN)\n",
        "\n",
        "# Clean the text, convert to lowercase, and lemmatize each word\n",
        "def clean_text_bi(text):\n",
        "    # Remove special characters: keep only letters, numbers, and basic punctuation\n",
        "    text = re.sub(r'[.]', ' ', text)  # Replaces periods with spaces\n",
        "    text = re.sub(r'[^a-zA-Z0-9 ]', '', text)\n",
        "    text = text.lower()  # Convert to lowercase\n",
        "    \n",
        "    # Tokenize the text\n",
        "    words = word_tokenize(text)\n",
        "    \n",
        "    # Lemmatize each word with its POS tag\n",
        "    lemmatized_words = [lemmatizer.lemmatize(w, get_wordnet_pos(w)) for w in words]\n",
        "    \n",
        "    return lemmatized_words\n",
        "\n",
        "# Apply the cleaning and lemmatization to the dataset\n",
        "sona_speeches_df['bigram_words'] = sona_speeches_df['speech'].apply(clean_text_bi)\n",
        "\n",
        "# Initialize Afinn and prepare Bing lexicon\n",
        "afinn = Afinn()\n",
        "positive_words = set(opinion_lexicon.positive())\n",
        "negative_words = set(opinion_lexicon.negative())\n",
        "\n",
        "# Function to calculate sentiment scores using Afinn\n",
        "def afinn_sentiment(text):\n",
        "    return afinn.score(text)\n",
        "\n",
        "# Function to calculate sentiment scores using Bing lexicon\n",
        "def bing_sentiment(word):\n",
        "    if word in positive_words:\n",
        "        return 1  # Positive sentiment\n",
        "    elif word in negative_words:\n",
        "        return -1  # Negative sentiment\n",
        "    else:\n",
        "        return 0  # Neutral sentiment\n",
        "\n",
        "# Function to get sentiment score for a bigram\n",
        "def bigram_sentiment(bigram, lexicon_score_func):\n",
        "    return lexicon_score_func(' '.join(bigram))\n",
        "\n",
        "# Function to analyze the sentiment of bigrams\n",
        "def analyze_bigram_sentiment(cleaned_words_list):\n",
        "    bigram_list = list(bigrams(cleaned_words_list))\n",
        "    \n",
        "    # Calculate sentiment for each bigram\n",
        "    tb_sentiments = [TextBlob(' '.join(bigram)).sentiment.polarity for bigram in bigram_list]\n",
        "    afinn_sentiments = [afinn_sentiment(' '.join(bigram)) for bigram in bigram_list]\n",
        "    bing_sentiments = [bigram_sentiment(bigram, bing_sentiment) for bigram in bigram_list]\n",
        "    \n",
        "    sentiment_data = pd.DataFrame({\n",
        "        'bigram': [' '.join(bigram) for bigram in bigram_list],\n",
        "        'textblob': tb_sentiments,\n",
        "        'afinn': afinn_sentiments,\n",
        "        'bing': bing_sentiments\n",
        "    })\n",
        "    \n",
        "    return sentiment_data\n",
        "\n",
        "# Apply the sentiment analysis for bigrams\n",
        "all_bigram_sentiments = pd.concat([analyze_bigram_sentiment(words) for words in sona_speeches_df['bigram_words']])\n",
        "\n",
        "# Group by bigram and calculate mean sentiment\n",
        "aggregated_bigram_sentiments = all_bigram_sentiments.groupby('bigram').agg('mean').reset_index()\n",
        "\n",
        "# Function to plot bar graphs for the bigrams that contribute most to sentiment\n",
        "def plot_bigram_sentiment_words(sentiment_df, lexicon_name):\n",
        "    top_positive_bigrams = sentiment_df[sentiment_df[lexicon_name] > 0].sort_values(by=lexicon_name, ascending=False).head(10)\n",
        "    top_negative_bigrams = sentiment_df[sentiment_df[lexicon_name] < 0].sort_values(by=lexicon_name).head(10)\n",
        "    \n",
        "    fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
        "    axes[0].barh(top_positive_bigrams['bigram'], top_positive_bigrams[lexicon_name])\n",
        "    axes[0].set_title(f'Top Positive Bigrams - {lexicon_name}')\n",
        "    axes[0].invert_yaxis()  # labels read top-to-bottom\n",
        "    axes[0].set_xlabel('Sentiment Score')\n",
        "\n",
        "    axes[1].barh(top_negative_bigrams['bigram'], top_negative_bigrams[lexicon_name])\n",
        "    axes[1].set_title(f'Top Negative Bigrams - {lexicon_name}')\n",
        "    axes[1].invert_yaxis()  # labels read top-to-bottom\n",
        "    axes[1].set_xlabel('Sentiment Score')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Plot for each lexicon\n",
        "plot_bigram_sentiment_words(aggregated_bigram_sentiments, 'textblob')\n",
        "plot_bigram_sentiment_words(aggregated_bigram_sentiments, 'afinn')\n",
        "\n",
        "# TODO: Fix Bing lexicon for bigrams\n",
        "plot_bigram_sentiment_words(aggregated_bigram_sentiments, 'bing')\n"
      ],
      "id": "238849e7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Function to calculate overall sentiment for a speech using TextBlob\n",
        "def textblob_sentiment(text):\n",
        "    return TextBlob(text).sentiment.polarity\n",
        "\n",
        "# Function to calculate overall sentiment for a speech using Afinn (normalized)\n",
        "def afinn_sentiment(text):\n",
        "    afinn = Afinn()\n",
        "    words = text.split()  # Split text into words\n",
        "    if len(words) > 0:   # Avoid division by zero\n",
        "        return afinn.score(text) / len(words)  # Normalized score\n",
        "    else:\n",
        "        return 0\n",
        "\n",
        "\n",
        "# Calculate sentiment scores\n",
        "sona_speeches_df['textblob_sentiment'] = sona_speeches_df['speech'].apply(textblob_sentiment)\n",
        "sona_speeches_df['afinn_sentiment'] = sona_speeches_df['speech'].apply(afinn_sentiment)\n",
        "\n",
        "# Ensure the 'year' column is of type int if it's not already\n",
        "sona_speeches_df['year'] = sona_speeches_df['year'].astype(int)\n",
        "\n",
        "# Sort by year\n",
        "sona_speeches_df.sort_values('year', inplace=True)\n",
        "\n",
        "# Plotting the sentiment trend over time\n",
        "plt.figure(figsize=(14, 7))\n",
        "\n",
        "# TextBlob sentiment trend\n",
        "plt.plot(sona_speeches_df['year'], sona_speeches_df['textblob_sentiment'], label='TextBlob Sentiment')\n",
        "\n",
        "# Afinn sentiment trend\n",
        "plt.plot(sona_speeches_df['year'], sona_speeches_df['afinn_sentiment'], label='Afinn Sentiment', alpha=0.7)\n",
        "\n",
        "plt.title('Sentiment Trend of SONA Speeches Over Time')\n",
        "plt.xlabel('Year')\n",
        "plt.ylabel('Sentiment Score')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ],
      "id": "c3887675",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Topic modelling\n"
      ],
      "id": "bc490909"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Load dataset - already lemmatized and cleaned, but still has stopwords\n",
        "df = pd.read_csv('data/sona_speeches_adapted.csv')\n",
        "\n",
        "# Function to clean and remove stopwords\n",
        "def remove_stopwords(text):\n",
        "    return ' '.join([word.strip('.,!?\"\\'-()') for word in text.split() if word.strip('.,!?\"\\'-()').lower() not in ENGLISH_STOP_WORDS])\n",
        "\n",
        "# Remove numeric values from the text\n",
        "df['speech'] = df['speech'].str.replace(r'\\d+', '')\n",
        "\n",
        "# Apply the function to each speech\n",
        "df['speech'] = df['speech'].apply(remove_stopwords)"
      ],
      "id": "bf171965",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## LSA\n"
      ],
      "id": "044f393e"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import pandas as pd\n",
        "from gensim import corpora, models, matutils\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Using TF-IDF\n",
        "tfidf_vectorizer = TfidfVectorizer()\n",
        "tfidf_matrix = tfidf_vectorizer.fit_transform(df['speech']) # Ensure 'speech_text' is the correct column name\n",
        "\n",
        "# Convert to Gensim format\n",
        "corpus = matutils.Sparse2Corpus(tfidf_matrix, documents_columns=False)\n",
        "dictionary = corpora.Dictionary.from_corpus(corpus, id2word=dict((id, word) for word, id in tfidf_vectorizer.vocabulary_.items()))\n",
        "\n",
        "# LSA model\n",
        "num_topics = 5  # or however many topics you want\n",
        "lsi_model = models.LsiModel(corpus=corpus, id2word=dictionary, num_topics=num_topics)\n",
        "\n",
        "# Now you can print topics or save the model\n",
        "lsi_topics = lsi_model.print_topics()\n",
        "for topic in lsi_topics:\n",
        "    print(topic)\n"
      ],
      "id": "60a65f6f",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## pLSA (Probabilistic Latent Semantic Analysis)\n"
      ],
      "id": "0bcaf591"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# This is similar to LSA, but with the LdaModel instead\n",
        "lda_model = models.LdaModel(corpus=corpus, id2word=dictionary, num_topics=num_topics, gamma_threshold=1e-3)\n",
        "\n",
        "# Now you can print topics or save the model\n",
        "lda_model.print_topics()"
      ],
      "id": "1ff787fb",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## LDA (Latent Dirichlet Allocation)\n"
      ],
      "id": "10b84010"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from gensim.utils import simple_preprocess\n",
        "\n",
        "# Tokenize the text\n",
        "tokenized_speeches = [simple_preprocess(speech) for speech in df['speech']]\n",
        "\n",
        "# Create a bag-of-words model for each speech\n",
        "bow_corpus = [dictionary.doc2bow(text) for text in tokenized_speeches]\n",
        "lda_model_bow = models.LdaModel(corpus=bow_corpus, id2word=dictionary, num_topics=num_topics)\n",
        "\n",
        "# LDA model using TF-IDF\n",
        "lda_model_tfidf = models.LdaModel(corpus=corpus, id2word=dictionary, num_topics=num_topics)\n",
        "\n",
        "# Now you can print topics or save the models\n",
        "lda_model_bow.print_topics()\n",
        "lda_model_tfidf.print_topics()"
      ],
      "id": "d6a5c015",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## corrLDA (Correlated Topic Model)\n"
      ],
      "id": "8cbe58ac"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import tomotopy as tp\n",
        "\n",
        "# Initialize CTM\n",
        "ctm = tp.CTModel(k=10)  # Replace '10' with the number of topics you want\n",
        "\n",
        "# Add documents to the model\n",
        "for text in df['speech']:\n",
        "    ctm.add_doc(text.split())  # Make sure the texts are tokenized if necessary\n",
        "\n",
        "# Train the model\n",
        "ctm.train(0)\n",
        "for i in range(100):\n",
        "    ctm.train(10)\n",
        "    print('Iteration: {}\\tLog-likelihood: {}'.format(i, ctm.ll_per_word))\n",
        "\n",
        "# Get the topics\n",
        "for i in range(ctm.k):\n",
        "    print(\"Topic #{}:\".format(i), ctm.get_topic_words(i))\n"
      ],
      "id": "a0b44d96",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Author-Topic LDA\n"
      ],
      "id": "e8e7c0b2"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from gensim.models import AuthorTopicModel\n",
        "\n",
        "# You need to create a mapping of authors (presidents) to documents\n",
        "author2doc = {author: [] for author in df['president'].unique()}\n",
        "for i, row in df.iterrows():\n",
        "    author2doc[row['president']].append(i)\n",
        "\n",
        "# Author-Topic LDA model\n",
        "author_topic_model = AuthorTopicModel(corpus=bow_corpus, author2doc=author2doc, id2word=dictionary, num_topics=num_topics)\n",
        "\n",
        "# Now you can print topics or save the model\n",
        "author_topic_model.print_topics()"
      ],
      "id": "d735ad64",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3 (ipykernel)"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}