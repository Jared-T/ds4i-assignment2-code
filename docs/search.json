[
  {
    "objectID": "ds4i_assignment2.html",
    "href": "ds4i_assignment2.html",
    "title": "Sentiments and Topics in South African SONA Speeches",
    "section": "",
<<<<<<< HEAD
    "text": "Abstract\n\n\n\nIntroduction\n\n\nThe field of Natural Language Processing (NLP) is faceted by techniques tailored for theme tracking and opinion mining which merge part of text analysis. Though, of particular prominence, is the extraction of latent thematic patterns and the establishment of the extent of emotionality expressed in political-based texts.\nGiven such political context, it is of specific interest to analyse the annual State of the Nation Address (SONA) speeches delivered by six different South African presidents (F.W. de Klerk, N.R. Mandela, T.M. Mbeki, K.P. Motlanthe, J.G. Zuma, and M.C. Ramaphosa) ranging over twenty-nine years (from 1994 to 2023). This analysis, descriptive and data-driven in nature, endeavours to examine the content of the SONA speeches in terms of themes via topic modelling (TM) and emotions via sentiment analysis (SentA). Applying a double-bifurcated approach, SentA will be executed within a macro and micro context both at the text (all-presidents versus by-president SONA speeches, respectively) and token (sentences versus words, respectively) level, as shown in Figure 1. This underlying framework is also assumed for TM, with the exceptions of only employing it within a macro-context at text level and a micro-context at the token level, as seen in Figure 2.\n\n\n\nFigure 1: Illustration of how sentA will be implemented within a different-scales-within-different-levels framework for the presidential-SONA-speeech text analysis.\n\n\n\n\n\nFigure 2: Depiction of how TM will be done using a similar approach to sentA, though tokens will only be defined in terms of words (and not also as sentences) at a text level of all SONA speeches (so disregarding the micro-president context).\n\n\nThrough such a multi-layered lens, the identification of any trends, both in terms of topics and sentiments, over time at both a large (presidents as a collective) as well as at a small (each president as an individual) scale is attainable. This explicates not only an aggregated perspective of the general political discourse prevailing within South Africa (SA), but also a more niche outlook of the specific rhetoric employed by each of the country’s serving presidents during different date periods.\nTo achieve all of the above-mentioned, it is first relevant to revise foundational terms and review related literature in context of politics and NLP. All pertinent pre-processing of the political text data is then considered, followed by a discussion delving into the details of each SentA and TM approach applied. Specifically, two different lexicons are leveraged to describe sentiments, whilst five different topic models are tackled to uncover themes within South-African-presidents’ SONA speeches. Ensuing the implementation of these methodologies, the results thereof are detailed in terms insights and interpretations. Thereafter, an overall evaluation of the techniques in terms of efficacy and inadequacy is overviewed. Finally, focal findings are highlighted and potential improvements as part of future research are recommended.\n\nLiterature Review\n\n\n SONA \nSONA, a pivotal event in the political programme of Parliament, serves as a presidential summary for the South African public. Specifically, the country’s current domestic affairs and international relations are reflected upon, past governmental work is perused, and future plans in terms of policies and civil projects are proposed. Through this address, accountability on the part of government is re-instilled and transparency with the public is re-affirmed on an annual basis, either once (non-election year) or twice (pre-and-post election) (Minister Faith Muthambi 2017). The text analysis of such SONA speeches, via the implementation of TM and SentA, has been previously done for Philippine presidents (Miranda and Bringula 2021). Though, it is now of interest to extend such an application to another country, SA.\n Topic modelling (TM) \nTM, an unsupervised learning approach, implicates the identification of underlying abstract themes in some body of text, in the absence of pre-specified labels (Cho 2019). In general, there are two topic-model assumptions: each document comprises of a mixture of topics and each topic consists of a collection of words (Zhang 2018). Different types of topic models exist, each with varying complexity in terms of the way in which topics are generated. The simplest one, Latent Semantic Analysis (LSA), has previously been implemented to discover patterns of lexical cohesion in political speech, specifically that of the former Prime Minister of the United Kingdom, Margaret Thatcher (Klebanov, Diermeier, and Beigman 2008). Improving on LSA methodology, Probabilistic LSA (pLSA) has been implemented in healthcare (Zhu 2014) and educational (Ming et al. 2014) contexts, albeit no application thereof in political science was found. A further sophisticated model, Latent Dirichlet Allocation (LDA), has been used to determine trending topics in news on governmental YouTube channels (Subhan et al. 2023).\n Sentiment analysis (SentA) \nSentA involves deciphering the intent of words to infer certain emotional dimensions labelled either in polarized (negative/positive) or higher-dimensional terms (niche feelings like joy/sadness). Various unigram lexicons have been derived to such extents. For example, the R-based \\(\\texttt{nrc}\\) lexicon dichotomously classifies words with yes/no labels in categories such as positive, negative, anticipation, anger, and so forth. In contrast, the Python-based \\(\\texttt{TextBlob}\\) lexicon processes textual data in the form of a tuple where a polarity score (ranges between -1 and +1 which relates to negative and positive sentiment, respectively) and a subjectivity score (ranges between 0 and 1 which refers to being very objective or very subjective, respectively) is produced. Using such pre-defined lexicons has been previously utilized to analyze political communication, specifically in terms of campaign polarization, via SentA (Haselmayer and Jenny 2017).\n\nData\n\n\n Tokenization \nThe process of tokenization entails breaking up given text into units, referred to as tokens (or terms), which are meaningful for analysis (Zhang 2018). In this case, these tokens take on different structures, based on either a macro-context (i.e., sentences) or micro-context (i.e., words). At both scales, the way in which these tokens are valued will be varied. The value will either be defined by a bag-of-words (BoW) or term-frequency, inverse-document-frequency (tf-idf) approach. The former way implicates accounting for the number of occurrences of some token in some document. On the other hand, the latter way not only regards the frequency of some token, but also the significance thereof. Thus, tf-idf involves the assignment of some weight to each token in a document which in turn reflects its importance relative to the entire collection of documents (corpus). It then follows that the tf-idf value of a token t in a document d within a corpus D is calculated as the product of two constituents. The first being tf(t,d) defined as the quotient of the frequency of token t in document d and the total number of tokens in document d, whereas the second is idf(t, D) denoted by the quotient of the natural logarithm of the total number of documents in corpus D and the number of documents containing the token t (Silge and Robinson 2017). N\n Number of topics \nIn order to determine the optimal number of topics, a coherence score is calculated. This metric measures the ability of a topic model to distinguish well between topics that are semantically interpretable by humans and are not simply statistical-inference artifacts. Hence, the number of topics as well as any other topic-model hyperparameters (like \\(\\alpha\\) and \\(\\beta\\) for LDA) are tuned to values that yield the maximum coherence score, allowing for the most understandable themes.\n\n\n\nMethods\n\n\n\nTopic modelling\n\n Latent Semantic Analysis (LSA) \n\n\n\nFigure 3: Schematic representation of LSA outlining the factorization of the DTM matrix.\n\n\nLSA (Deerwester et al. 1990) is a non-probabilistic, non-generative model where a form of matrix factorization is utilized to uncover few latent topics, capturing meaningful relationships among documents/tokens. As depicted in Figure 3, in the first step, a document-term matrix DTM is generated from the raw text data by tokenizing d documents into w words (or sentences), forming the columns and rows respectively. Each row-column entry is either valued via the BoW or tf-idf approach. This DTM-matrix, which is often sparse and high-dimensional, is then decomposed via a dimensionality-reduction-technique, namely truncated Singular Value Decomposition (SVD). Consequently, in the second step the DTM-matrix becomes the product of three matrices: the topic-word matrix \\(A_{t*}\\) (for the tokens), the topic-prevalence matrix \\(B_{t*}\\) (for the latent semantic factors), and the transposed document-topic matrix \\(C^{T}_{t*}\\) (for the document). Here, t*, the optimal number of topics, is a hyperparameter which is refined at a value (via the coherence-measure approach) that retains the most significant dimensions in the transformed space. In the final step, the text data is then encoded using this top-topic number.\nGiven LSA only implicates a DTM-matrix, the implementation thereof is generally efficient. Though, with the involvement of truncated SVD, some computational intensity and a lack of quick updates with new, incoming text-data can arise. Additional LSA drawbacks include: the lack of interpretability, the underlying linear-model framework (which results in poor performance on text-data with non-linear dependencies), and the underlying Gaussian assumption for tokens in documents (which may not be an appropriate distribution).\n Probabilistic Latent Semantic Analysis (pLSA) \n\n\n\nFigure 4: Schematic representation of pLSA, where the different-shade-of-blue colours highlight similarities shared with LSA-related matrices shown in Figure 3.\n\n\nInstead of implementing truncated SVD, pLSA (Hofmann 1999) rather utilizes a generative, probabilistic model. Within this framework, a document d is first selected with probability P(d). Then given this, a latent topic t is present in this selected document d and so chosen with probability of P(t|d). Finally, given this chosen topic t, a word w (or sentence) is generated from it with probability P(w|t), as shown in Figure 4. It is noted that the values of P(d) is determined directly from the corpus D which is defined in terms of a DTM matrix. In contrast, the probabilities P(t|d) and P(w|t) are parameters modelled as multinomial distributions and iteratively updated via the Expectation-Maximization (EM) algorithm. Direct parallelism between LSA and pLSA can be drawn via the methods’ parameterization, as conveyed via matching colours of the topic-word matrix and P(w|t), the document-topic matrix and P(d|t) as well as the topic-prevalence matrix and P(t) displayed in Figure 3 and Figure 4, respectively.\nDespite pLSA implicitly addressing LSA-related disadvantages, this method still involves two main drawbacks. There is no probability model for the document-topic probabilities P(t|d), resulting in the inability to assign topic mixtures to new, unseen documents not trained on. Model parameters also then increase linearly with the number of documents added, making this method more susceptible to overfitting.\n Latent Dirichlet Allocation \n\n\n\nFigure 5: Schematic representation of LDA where the dark-blue-shaded block represents observed words.\n\n\nLDA is another generative, probabilistic model which can be deemed as a hierarchical Bayesian version of pLSA. Via explicitly defining a generative model for the document-topic probabilities, both the above-mentioned pitfalls of pLSA are improved upon. The number of parameters to estimate drastically decrease and the ability to apply and generalize to new, unseen documents is attainable. As presented in Figure 5, the initial steps first involve randomly sampling a document-topic probability distribution \\(\\theta\\) from a Dirichlet (Dir) distribution \\(\\eta\\), followed by randomly sampling a topic-word probability distribution \\(\\phi\\) from another Dirichlet distribution \\(\\tau\\). From the \\(\\theta\\) distribution, a topic t is selected by drawing from a multinomial (Mult) distribution (third step) and from the \\(\\phi\\) distribution given said topic t, a word w (or sentences) is sampled from another multinomial distribution (fourth step). The associated LDA-parameters are then estimated via a variational expectation maximization algorithm or collapsed Gibbs sampling.\n Correlated Topic Model (CTM) \n\n\n\nFigure 6: Schematic representation of CTM where the dark-blue-shaded block represents observed words, whilst the light-grey colour outlines the distinctions from the LDA topic model presented in Figure 5.\n\n\nFollowing closely to LDA, the CTM (Lafferty and Blei 2005) additionally allows for the ability to model the presence of any correlated topics. Such topic correlations are introduced via the inclusion of the multivariate normal (MultNorm) distribution with t length-vector of means \\(\\mu\\) and t \\(\\times\\) t covariance matrix \\(\\Sigma\\) where the resulting values are then mapped into probabilities by passing through a logistic (log) transformation. Comparing Figure 5 and Figure 6, the nuance between LDA and CTM is highlighted using a light-grey colour, where the discrepancy in the models come about from replacing the Dirichlet distribution (which involves the implicit assumption of independence across topics) with the logit-normal distribution (which now explicitly enables for topic dependency via a covariance structure) for generating document-topic probabilities. The other generative processes previously outlined for LDA is retained and repeated for CTM. Given this additional model complexity, the more convoluted mean-field variational inference algorithm is employed for CTM-parameter estimation which necessitates many iterations for optimization purposes. CTM is consequently computationally more expensive than LDA. Though, this snag is far outweighed by the procurement of richer topics with overt relationships acknowledged between these.\n Author Topic Model (ATM) \n\n\n\nFigure 7: Schematic representation of ATM where the dark-blue-shaded blocks represents observed words and authors, whilst the light-grey colour highlights the differences compared to the LDA topic model presented in Figure 5.\n\n\nATM (Rosen-Zvi et al. 2012) extends LDA via the inclusion of authorship information with topics. Again, inspecting Figure 5 and Figure 7, the slight discrepancies between these two models are accentuated with the light-grey colour. Here, for each word w in the document d an author a is sampled uniformly (Uni) at random. Each author is associated with a distribution over topics (\\(\\Psi\\)) sampled from a Dirichlet prior \\(\\alpha\\). The resultant mixture weights corresponding to the chosen author are used to select a topic t, then a word w (or sentence) is generated according to the topic-word distribution \\(\\phi\\) (drawn from another Dirichlet prior \\(\\beta\\)) corresponding to that said chosen topic t. Therefore, through the estimation of the \\(\\psi\\) and \\(\\phi\\) parameters, not only is information obtained about which topics authors generally relate to, but also a representation of these document contents in terms of these topics, respectively.\n\nSentiment analysis\n\n AFINN \nThe R-based \\(\\texttt{AFINN}\\) lexicon scores words across a range spanning from the value of -5 to +5. Intuitively, words scored closer to the lower-boundary value relate to more negative sentiment, and in contrast higher positive sentiment is revealed if rather closer to the upper-boundary value (Silge and Robinson 2017).\n Bing \nUnlike \\(\\texttt{AFINN}\\) , the R-based \\(\\texttt{bing}\\) lexicon does not provide sentiments via some scoring system. Instead, it simply assigns a binary label of a word being interpreted as either positive or negative (Silge and Robinson 2017).\n\nExploratory Data Analysis\n\n\n\n\n\nFigure 8: Most frequent words used across all SONA speeches, irrespective of president.\n\n\nFrom Figure 8, it is evident that the word “government” is mainly referenced to across all SONA speeches. This word dominance draws upon how the importance of this authority body, which is integral to the governance of SA, is emphasized. The frequent usage of the words “people” and “public” indicates a sense of inclusivity where the idea of togetherness is implicitly suggested. Other words, such as “development” and “new”, are indicative of ideas of growth and renewal. Lastly, a sense of security and safety is provided with the recurring use of the word “ensure”.\n\n\n\n\n\n\n\n(a) de Klerk\n\n\n\n\n\n\n\n(b) Mandela\n\n\n\n\n\n\n\n\n\n(c) Mbeki\n\n\n\n\n\n\n\n(d) Motlanthe\n\n\n\n\n\n\n\n\n\n(e) Zuma\n\n\n\n\n\n\n\n(f) Ramaphosa\n\n\n\n\nFigure 9: Most frequent words used in SONA speeches, faceted by president.\n\n\nAfter faceting the most frequent words by president, as displayed in Figure 9, there are some slight nuances noted. For instance, former president de Klerk used words which were emblematic of the political paradigm shift that occurred during the time of his term. Words such as “transitional”, “constitutional”, and “constitution” reflects the country’s progression from an exclusive, segregated to a more inclusive, democratic state. This political and legal reform directed towards achieving societal equality is further underscored by the words, “parties”, “party”, and “election”. The pivotal role of proper partnerships being formed, which would have further aided in maintaining this change, is foregrounded with the word “alliance”.\nSimilarly, this idea of unity has also been foregrounded in the other five presidents’ speeches with the commonly shared word “people”. Though, unlike de Klerk, the other former presidents (Mandela, Mbeki, Motlanthe, Zuma) and current president (Ramaphosa), seem to similarly place more focus on the explicit communication of policies and vision (“development” and “work”) and the establishment of a sense of responsibility and accountability on their part as president (“government” and “ensure”). Some minor distinctions between these aforementioned presidents can be made. Mandela, Mbeki, and Motlanthe, for example, seemed to draw more attention to “society” or “social” progress, whilst Zuma and Ramaphosa appeared to place more prominence on “economic” progress.\n\nResults\n\n\n\nSentiment Analysis\n\n\n\n\n\n\n\n\n(a) \\(\\texttt{AFINN}\\): All speeches\n\n\n\n\n\n\n\n(b) \\(\\texttt{bing}\\): All speeches\n\n\n\n\nFigure 10: Overall sentiment score for SONA speeches across time (segmented by presidential terms), compared for two different lexicons.\n\n\nComparing Figure 10 (a) and Figure 10 (b), it is evident that there is no obvious, overt difference in the computed net sentiment scores, which are overall positive, across time and presidents for the two different lexicons. Any slight variation between \\(\\texttt{AFINN}\\) and \\(\\texttt{bing}\\) is most likely attributed to the lexicons’ varying scales (+5/-5 versus +1/-1, respectively). Hence, any sentiment derived from the former lexicon might be slightly more exaggerated in nature compared to the latter lexicon. This is noted when checking the \\(y\\)-axes range of the sentiment scores, which reach a maximum of 600 for the \\(\\texttt{AFINN}\\) lexicon and only 300 for the \\(\\texttt{bing}\\) lexicon.\nAcross both lexicons, from de Klerk to Mbeki’s presidential terms, positive sentiment seems to steadily rise. Though, after a peak of high, positive sentiment scores from Mbeki’s SONA speeches, there is a slight decline in this overall positivity. This is especially present throughout Zuma’s presidential term.\n\n\n\n\n\n\n\n(a) \\(\\texttt{AFINN}\\): de Klerk speeches\n\n\n\n\n\n\n\n(b) \\(\\texttt{bing}\\): de Klerk speeches\n\n\n\n\n\n\n\n\n\n(c) \\(\\texttt{AFINN}\\): Mandela speeches\n\n\n\n\n\n\n\n(d) \\(\\texttt{bing}\\): Mandela speeches\n\n\n\n\n\n\n\n\n\n(e) \\(\\texttt{AFINN}\\): Mbeki speeches\n\n\n\n\n\n\n\n(f) \\(\\texttt{bing}\\): Mbeki speeches\n\n\n\n\n\n\n\n\n\n(g) \\(\\texttt{AFINN}\\): Motlanthe speeches\n\n\n\n\n\n\n\n(h) \\(\\texttt{bing}\\): Motlanthe speeches\n\n\n\n\n\n\n\n\n\n(i) \\(\\texttt{AFINN}\\): Zuma speeches\n\n\n\n\n\n\n\n(j) \\(\\texttt{bing}\\): Zuma speeches\n\n\n\n\n\n\n\n\n\n(k) \\(\\texttt{AFINN}\\): Ramaphosa speeches\n\n\n\n\n\n\n\n(l) \\(\\texttt{bing}\\): Ramaphosa speeches\n\n\n\n\nFigure 11: Trajectory of sentiment score through SONA-speech sentences stratified by president, comparing across two different lexicons.\n\n\nFrom inspecting Figure 11 , it is apparent that the relative trajectory of underlying emotion is generally similar for each president. The dips and troughs in sentiment prevalent for both \\(\\texttt{bing}\\) and \\(\\texttt{AFINN}\\) lexicons occur at approximately the same sentences in the respective presidents’ speeches. Though, there is some stark contrast found between the two lexicons when comparing for Zuma’s speeches. For this president, the negative falls and positive rises are more exaggerated for the \\(\\texttt{AFINN}\\) compared to the \\(\\texttt{bing}\\) lexicon. Additionally, the sentiment pattern of Mbeki’s speeches again seems more skewed to the positive side, with more frequent extreme rises to high sentiment score values across sentences. It is also again seen that more negative sentiment is expressed in Zuma’s speeches, give the more dominant dips. For Ramaphosa, there appears to be more of a balance between positive-and-negative sentiment. There are no extreme, outlying rises/falls, rather a more consistent sawtooth-like pattern is prominent.\n\n\n\n\n\n\n\n(a) \\(\\texttt{AFINN}\\): All speeches\n\n\n\n\n\n\n\n(b) \\(\\texttt{bing}\\): All speeches\n\n\n\n\nFigure 12: Words which contribute to the positive and negative sentiment across all SONA speeches.\n\n\nSimilarly with the general sentiment-score trajectories, there are more overlaps between the two lexicons when comparing the specific words which contribute to the positive and negative sentiments, as displayed in Figure 12. The same seven (out of the top ten) words commonly contribute to negative sentiment (“corruption”, “crime”, “violence”), in addition to positive sentiment (“improve”, “support”, “progress”) across both lexicons. Albeit, the extent of these aforementioned words’ contributions to the respective sentiments do slightly vary in amounts. Some unique, independent words also add to the negative sentiment for the \\(\\texttt{AFINN}\\) lexicon (“problems”, “unemployment”) and \\(\\texttt{bing}\\) lexicon (“issues”). Likewise, there are distinctive words for this former lexicon (“growth”, “ensure”, “great”) and latter lexicon (“well”) attributed to positive sentiment.\n\n\n\n\n\n\n\n(a) \\(\\texttt{AFINN}\\): de Klerk speeches\n\n\n\n\n\n\n\n(b) \\(\\texttt{bing}\\): de Klerk speeches\n\n\n\n\n\n\n\n\n\n(c) \\(\\texttt{AFINN}\\): Mandela speeches\n\n\n\n\n\n\n\n(d) \\(\\texttt{bing}\\): Mandela speeches\n\n\n\n\n\n\n\n\n\n(e) \\(\\texttt{AFINN}\\): Mbeki speeches\n\n\n\n\n\n\n\n(f) \\(\\texttt{bing}\\): Mbeki speeches\n\n\n\n\n\n\n\n\n\n(g) \\(\\texttt{AFINN}\\): Motlanthe speeches\n\n\n\n\n\n\n\n(h) \\(\\texttt{bing}\\): Motlanthe speeches\n\n\n\n\n\n\n\n\n\n(i) \\(\\texttt{AFINN}\\): Zuma speeches\n\n\n\n\n\n\n\n(j) \\(\\texttt{bing}\\): Zuma speeches\n\n\n\n\n\n\n\n\n\n(k) \\(\\texttt{AFINN}\\): Ramaphosa speeches\n\n\n\n\n\n\n\n(l) \\(\\texttt{bing}\\): Ramaphosa speeches\n\n\n\n\nFigure 13: Words which contribute to the positive and negative sentiment for each specific presidents’ SONA speech/speeches.\n\n\nAfter faceting Figure 12 by president, as presented in the sub-plots of Figure 13, essentially no variability is indicated in terms of uniqueness and the contribution magnitude thereof. For both lexicons, the same set of words add the same amount to each sentiment. Furthermore, commonalities between words contributing to the sentiments is evident between the five presidents after de Klerk. Negative-sentiment words like “unconstitutional”, “deprive”, “discrimination”, and “boycott” and positive-sentiment words such as “proud”, “succeeded”, and “peaceful” only features in de Klerk’s speech. All of these aforementioned words seem to directly relate to the change in political context during de Klerk’s term. Whilst, the words prevailing in the other five presidents’ speeches appear to foreground the continuation of this changed political climate with positive-sentiment words like “improve”, “better”, “freedom” and “peace”. Additionally, the shared negative-sentiment words like “crime”, “corruption” and “poverty” foreground the commonality of perpetuating problems that became pronounced throughout all presidential tenures after de Klerk.\n\nTopic modelling Results\n\n\n\nLatent Semantic Analysis (LSA)\n\nIn the micro-context (i.e., word tokenization) of LSA implementation, the maximum coherence score of approximately -1.5 seen in Figure 14 indicates that three topics are optimal when utilizing the tf-idf approach. In contrast, there is no discernible difference in the coherence scores across a range of topic numbers when instead using the BoW approach. Hence, for comparative purposes, three topics are also chosen as best in this instance.\n\n\n\nFigure 14: Coherence plot for LSA where SONA speeches were tokenized by words.\n\n\nConsidering Figure 15, a different overarching focus seems to come to the fore contingent on whether the BoW or tf-idf approach is examined. With reference to the BoW-related corpus, the broad scope of the three topics appears associated to national frameworks and the future thereof. More particularly, the first topic labelled as governance alludes to processes (“work”, “development”) established by institutions (“government”) which direct communities (“public”, “people”). Words like “infrastructure”, “investment” and “energy” seem to indicate structural resources, which encompasses the second topic. The third topic, sustainability, is more related to the prospective of long-term (“years”) endurance (“continue”) across these governmental processes and resources.\nOn the other hand, with respect to the tf-idf-related corpus, the three topics now instead appear concentrated on current issues. The first topic centers on a sole problem, particularly that of common (“compatriots”) “pandemic” preparedness (“plan”). Whereas, the second topic broadens to other additional issues contextual to the country. Hence, in this case, the challenges range from energy (“eskom”) problems (“loadshedding”) to the “covid” “pandemic” to the government’s corruption (state “capture”). Most words contained in the third topic all already feature in topic one or two already. Given this lack of unique distinction, topic three is said to simply be a synthesis of the two, aforementioned topics.\n\n\n\n\n\n\n\n(a) BoW: Topic 1 ~ governance\n\n\n\n\n\n\n\n(b) tf-idf: Topic 1 ~ pandemic preparedness\n\n\n\n\n\n\n\n\n\n(c) BoW: Topic 2 ~ structural resources\n\n\n\n\n\n\n\n(d) tf-idf: Topic 2 ~ contemporary challenges\n\n\n\n\n\n\n\n\n\n(e) BoW: Topic 3 ~ sustainability\n\n\n\n\n\n\n\n(f) tf-idf: Topic 3 ~ an amalgamation\n\n\n\n\nFigure 15: Topic-by-word plots for BoW (left) and tf-idf (right) approach for LSA implementation.\n\n\nUnlike within the micro-context, LSA now applied in the maro-context (i.e., sentence tokenization) indicates more variability in the choice of the optimal topic number for both BoW and tf-idf approaches. Due to the excessive jumps between high and low coherence scores across the topic amounts seen in Figure 16, it is opted to choose the lowest best number. This choice aims to limit potential theme-overlapping (i.e., many common words shared across topics) and allow for more conciseness.\n\n\n\nFigure 16: Coherence plot for LSA where SONA speeches were tokenized by sentences.\n\n\nGiven Figure 17, more of an overlap in topic scope across both corpus types is seen for the sentence-tokenized speeches. For instance, the second topic for the BoW-related corpus and the first topic for the tf-idf-related corpus are both labelled as governance, given the commonly shared words (“government”, “people”, “work” and “development”). The first topic for this former-mentioned corpus accounts for the internal structures (“leaders”, “assembly”, “president”, “members”) related to governance. Whilst, the second topic of the tf-idf-related corpus is very distinguishable with the dominant weighting of the word “thank”, connotated with a sense of gratitude. This topic also refers to some reformation indicators like “opportunity” and “growth”.\n\n\n\n\n\n\n\n(a) BoW: Topic 1 ~ governmental structures\n\n\n\n\n\n\n\n(b) tf-idf: Topic 1 ~ governance\n\n\n\n\n\n\n\n\n\n(c) BoW: Topic 2 ~ governance\n\n\n\n\n\n\n\n(d) tf-idf: Topic 2 ~ reformation and gratitude\n\n\n\n\nFigure 17: SONA speeches tokenized by sentences for BoW (left) and tf-idf (right) approach for LSA implementation.\n\n\n\npLSA\n\nAs previously found with the implementation of LSA, there is essentially no variability in the coherence scores when utilizing the BoW approach for the word-tokenized application of pLSA. Though, like with LSA, there are fluctuating changes in the coherence scores across topic numbers when considering the tf-idf approach. Again, it is opted to take the minimum-best (based on tf-idf) number of three topics for both approaches which\n\n\n\nFigure 18: Coherence plot for pLSA where SONA speeches were tokenized by words.\n\n\n\n\n\n\n\n\n\n(a) BoW: Topic 1\n\n\n\n\n\n\n\n(b) tf-idf: Topic 1\n\n\n\n\n\n\n\n\n\n(c) BoW: Topic 2\n\n\n\n\n\n\n\n(d) tf-idf: Topic 2\n\n\n\n\n\n\n\n\n\n(e) BoW: Topic 3\n\n\n\n\n\n\n\n(f) tf-idf: Topic 3\n\n\n\n\n\n\n\n\n\n(g) BoW: Topic 4\n\n\n\n\n\n\n\n(h) tf-idf: Topic 4\n\n\n\n\n\n\n\n\n\n(i) BoW: Topic 5\n\n\n\n\n\n\n\n(j) tf-idf: Topic 5\n\n\n\n\nFigure 19: Topic plots for pLSA tokenized by words and executed within a BoW (left) and tf-idf (right) framework.\n\n\n\n\n\nFigure 20: Coherence plot for pLSA where SONA speeches were tokenized by sentences.\n\n\n\n\n\n\n\n\n\n(a) BoW: Topic 1\n\n\n\n\n\n\n\n(b) tf-idf: Topic 1\n\n\n\n\n\n\n\n\n\n(c) BoW: Topic 2\n\n\n\n\n\n\n\n(d) tf-idf: Topic 2\n\n\n\n\n\n\n\n\n\n(e) BoW: Topic 3\n\n\n\n\n\n\n\n(f) tf-idf: Topic 3\n\n\n\n\n\n\n\n\n\n(g) BoW: Topic 4\n\n\n\n\n\n\n\n(h) tf-idf: Topic 4\n\n\n\n\nFigure 21: Topic plots for pLSA using sentences as tokens and applied within a BoW (left) and tf-idf (right) framework.\n\n\n\nLDA\n\n\n\n\n\n\n\n\n(a) Words\n\n\n\n\n\n\n\n(b) Sentences\n\n\n\n\nFigure 22: Contour plots visualizing variation in coherence scores tuned across range of different \\(\\alpha\\) and \\(\\beta\\) hyperparameter values.\n\n\n\n\n\n\n\nTable 1: Coherence scores obtained from a hyperparameter-combination grid search for implementation of LDA on SONA speeches tokenized by words. \n\n\nCorpus\nTopics\nAlpha\nBeta\nCoherence\n\n\n\n\nBoW\n9\n0.60\n0.50\n-0.42\n\n\nBoW\n9\n0.60\n0.10\n-0.42\n\n\nBoW\n9\n0.60\n0.70\n-0.42\n\n\nBoW\n9\n0.60\n0.90\n-0.42\n\n\nBoW\n9\n0.60\n0.30\n-0.42\n\n\ntf-idf\n7\n0.20\n0.90\n-4.25\n\n\ntf-idf\n7\n0.20\n0.10\n-4.25\n\n\ntf-idf\n7\n0.20\n0.30\n-4.25\n\n\ntf-idf\n7\n0.20\n0.50\n-4.25\n\n\ntf-idf\n7\n0.20\n0.70\n-4.25\n\n\n\n\n\n\n\n\n\n\nTable 2: Coherence scores obtained from a hyperparameter-combination grid search for implementation of LDA on SONA speeches tokenized by sentences. \n\n\nCorpus\nTopics\nAlpha\nBeta\nCoherence\n\n\n\n\nBoW\n2\n0.90\n0.90\n-18.98\n\n\nBoW\n2\n0.80\n0.50\n-18.98\n\n\nBoW\n2\n0.70\n0.10\n-18.98\n\n\nBoW\n2\n0.70\n0.30\n-18.98\n\n\nBoW\n2\n0.70\n0.50\n-18.98\n\n\ntf-idf\n2\n0.40\n0.10\n-20.56\n\n\ntf-idf\n2\n0.40\n0.30\n-20.56\n\n\ntf-idf\n2\n0.40\n0.50\n-20.56\n\n\ntf-idf\n2\n0.40\n0.70\n-20.56\n\n\ntf-idf\n2\n0.40\n0.90\n-20.56\n\n\n\n\n\n pyLDAvis for Words \n\n\n\n\n\n\n\n\n\n\n\n pyLDAvis for Sentences \n\n\n\n\n\n\n\n\n\n\n\n\nCTM\n\n\n\n\n\n\n\n\n(a) Words ~ economy\n\n\n\n\n\n\n\n(b) Sentences ~ economy\n\n\n\n\nFigure 23: Coherence plots for CTM across different tokens (words or sentences) implemented within BoW approach.\n\n\n\n\n\n\n\n\n\n(a) Sentences: Topic 1 ~ economy\n\n\n\n\n\n\n\n(b) Words: Topic 1 ~ economy\n\n\n\n\n\n\n\n\n\n(c) Sentences: Topic 2 ~ economic and structural advancement\n\n\n\n\n\n\n\n(d) Words: Topic 2 ~ societal collectives\n\n\n\n\n\n\n\n\n\n(e) Sentences: Topic 3 ~ social collectives\n\n\n\n\nFigure 24: Topic plots where either sentences (left) or words (right) were used as tokens for CTM application.\n\n\n\nATM (Author-Topic Model)\n\n\n\n\n\n\n\n\n(a) Words\n\n\n\n\n\n\n\n(b) Sentences\n\n\n\n\nFigure 25: Coherence plots for ATM across different tokens (words or sentences) implemented within BoW approach.\n\n\n\n\n\n\n\n\n\n(a) Sentences: Topic 1 ~ social cohesion\n\n\n\n\n\n\n\n(b) Words: Topic 1 ~ diplomatic strategy\n\n\n\n\n\n\n\n\n\n(c) Sentences: Topic 2 ~ prospective plans\n\n\n\n\n\n\n\n(d) Words: Topic 2 ~ general strategic vision\n\n\n\n\n\n\n\n\n\n(e) Sentences: Topic 3 ~ esteem within governmental structures\n\n\n\n\nFigure 26: Topic plots where either sentences (left) or words (right) were used as tokens for ATM application.\n\n\n\n\n\n\n\nTable 3: ATM topics for each president. \n\n\nPresident\nDocument IDs\nWord-Based Topics\nSentence-Based Topics\n\n\n\n\nMandela\n4, 16, 0, 23, 31, 21, 15\nDiplomatic strategy (0.52); General strategic vision (0.48)\nSocial cohesion (1.00)\n\n\nRamaphosa\n7, 26, 3, 24, 8, 1, 35\nDiplomatic strategy (0.30); General strategic vision (0.70)\nSocial cohesion (0.66); Esteem within governmental structures (0.34)\n\n\nMbeki\n9, 14, 22, 34, 32, 28, 29, 2, 12, 25\nDiplomatic strategy (0.33); General strategic vision (0.67)\nSocial cohesion (1.00)\n\n\nZuma\n13, 5, 6, 20, 19, 30, 27, 33, 11, 10\nDiplomatic strategy (0.47); General strategic vision (0.53)\nSocial cohesion (1.00)\n\n\ndeKlerk\n17\nDiplomatic strategy (0.56); General strategic vision (0.44)\nSocial cohesion (0.02); Esteem within governmental structures (0.98)\n\n\nMotlanthe\n18\nDiplomatic strategy (0.55); General strategic vision (0.45)\nSocial cohesion (0.17); Prospective plans (0.83)\n\n\n\n\n\n\n\n\n\n\nReferences\n\nCho, Hae-Wol. 2019. “Topic Modeling.” Osong Public Health and Research Perspectives 10 (June): 115–16. https://doi.org/10.24171/j.phrp.2019.10.3.01.\n\n\nDeerwester, Scott, Susan T. Dumais, George W. Furnas, Thomas K. Landauer, and Richard Harshman. 1990. “Indexing by Latent Semantic Analysis.” Journal of the American Society for Information Science 41 (6): 391–407. https://doi.org/https://doi.org/10.1002/(SICI)1097-4571(199009)41:6&lt;391::AID-ASI1&gt;3.0.CO;2-9.\n\n\nHaselmayer, Martin, and Marcelo Jenny. 2017. “Sentiment Analysis of Political Communication: Combining a Dictionary Approach with Crowdcoding.” Quality and Quantity 51 (November): 2623–46. https://doi.org/10.1007/s11135-016-0412-4.\n\n\nHofmann, Thomas. 1999. “Probabilistic Latent Semantic Indexing.” In Proceedings of the 22nd Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, 50–57. SIGIR ’99. New York, NY, USA: Association for Computing Machinery. https://doi.org/10.1145/312624.312649.\n\n\nKlebanov, Beata Beigman, Daniel Diermeier, and Eyal Beigman. 2008. “Lexical Cohesion Analysis of Political Speech.” Political Analysis 16 (4): 447–63. http://www.jstor.org/stable/25791949.\n\n\nLafferty, John, and David Blei. 2005. “Correlated Topic Models.” In Advances in Neural Information Processing Systems, edited by Y. Weiss, B. Schölkopf, and J. Platt. Vol. 18. MIT Press. https://proceedings.neurips.cc/paper_files/paper/2005/file/9e82757e9a1c12cb710ad680db11f6f1-Paper.pdf.\n\n\nMing, Ding, Dong Bin, Yan Yonghong, and Ding Yousheng. 2014. “The Application of PLSA Features in the Automatic Assessment System for English Oral Test.” Computer Modelling and New Technologies 18: 414–18. http://www.cmnt.lv/en/on-line-journal/2014/2014-volume-18-12/part-c-operation-research-and-decision-making/the-application-of-plsa-features-in-the-automatic-assessment-system-for-english-oral-test.\n\n\nMinister Faith Muthambi. 2017. “SONA enables us to take part in our democracy.” 2017. https://www.gcis.gov.za/sona-enables-us-take-part-our-democracy.\n\n\nMiranda, John Paul P., and Rex P. Bringula. 2021. “Exploring Philippine Presidents’ speeches: A sentiment analysis and topic modeling approach.” Edited by John Kwame Boateng. Cogent Social Sciences 7 (1): 1932030. https://doi.org/10.1080/23311886.2021.1932030.\n\n\nRosen-Zvi, Michal, Thomas Griffiths, Mark Steyvers, and Padhraic Smyth. 2012. “The Author-Topic Model for Authors and Documents.” https://arxiv.org/abs/1207.4169.\n\n\nSilge, Julia, and David Robinson. 2017. Text Mining with R: A Tidy Approach. 1st ed. O’Reilly Media, Inc.\n\n\nSubhan, Subhan, M. Faris Al Hakim, Prasetyo Listiaji, and Wahyu Syafrizal. 2023. “Modeling news topics on government youtube channels with latent Dirichlet allocation method.” AIP Conference Proceedings 2614 (1): 040009. https://doi.org/10.1063/5.0125954.\n\n\nZhang, Zhiyong. 2018. “Text Mining for Social and Behavioral Research using R.” 2018. https://books.psychstat.org/textmining/index.html.\n\n\nZhu, Shaoping. 2014. “Pain Expression Recognition Based on pLSA Model.” The Scientific World Journal, 2356–6140. https://doi.org/10.1155/2014/736106."
=======
    "text": "Abstract\n\n\n\nIntroduction\n\n\nThe field of Natural Language Processing (NLP) is faceted by techniques tailored for theme tracking and opinion mining which merge part of text analysis. Though, of particular prominence, is the extraction of latent thematic patterns and the establishment of the extent of emotionality expressed in political-based texts.\nGiven such political context, it is of specific interest to analyse the annual State of the Nation Address (SONA) speeches delivered by six different South African presidents (F.W. de Klerk, N.R. Mandela, T.M. Mbeki, K.P. Motlanthe, J.G. Zuma, and M.C. Ramaphosa) ranging over twenty-nine years (from 1994 to 2023). This analysis, descriptive and data-driven in nature, endeavours to examine the content of the SONA speeches in terms of themes via topic modelling (TM) and emotions via sentiment analysis (SentA). Applying a double-bifurcated approach, SentA will be executed within a macro and micro context both at the text (all-presidents versus by-president SONA speeches, respectively) and token (sentences versus words, respectively) level, as shown in Figure 1. This underlying framework is also assumed for TM, with the exceptions of only employing it within a macro-context at text level and a micro-context at the token level, as seen in Figure 2.\n\n\n\nFigure 1: Illustration of how sentA will be implemented within a different-scales-within-different-levels framework for the presidential-SONA-speeech text analysis.\n\n\n\n\n\nFigure 2: Depiction of how TM will be done using a similar approach to sentA, though tokens will only be defined in terms of words (and not also as sentences) at a text level of all SONA speeches (so disregarding the micro-president context).\n\n\nThrough such a multi-layered lens, the identification of any trends, both in terms of topics and sentiments, over time at both a large (presidents as a collective) as well as at a small (each president as an individual) scale is attainable. This explicates not only an aggregated perspective of the general political discourse prevailing within South Africa (SA), but also a more niche outlook of the specific rhetoric employed by each of the country’s serving presidents during different date periods.\nTo achieve all of the above-mentioned, it is first relevant to revise foundational terms and review related literature in context of politics and NLP. All pertinent pre-processing of the political text data is then considered, followed by a discussion delving into the details of each SentA and TM approach applied. Specifically, two different lexicons are leveraged to describe sentiments, whilst five different topic models are tackled to uncover themes within South-African-presidents’ SONA speeches. Ensuing the implementation of these methodologies, the results thereof are detailed in terms insights and interpretations. Thereafter, an overall evaluation of the techniques in terms of efficacy and inadequacy is overviewed. Finally, focal findings are highlighted and potential improvements as part of future research are recommended.\n\nLiterature Review\n\n\n SONA \nSONA, a pivotal event in the political programme of Parliament, serves as a presidential summary for the South African public. Specifically, the country’s current domestic affairs and international relations are reflected upon, past governmental work is perused, and future plans in terms of policies and civil projects are proposed. Through this address, accountability on the part of government is re-instilled and transparency with the public is re-affirmed on an annual basis, either once (non-election year) or twice (pre-and-post election) (Minister Faith Muthambi 2017). The text analysis of such SONA speeches, via the implementation of TM and SentA, has been previously done for Philippine presidents (Miranda and Bringula 2021). Though, it is now of interest to extend such an application to another country, SA.\n Topic modelling (TM) \nTM, an unsupervised learning approach, implicates the identification of underlying abstract themes in some body of text, in the absence of pre-specified labels (Cho 2019). In general, there are two topic-model assumptions: each document comprises of a mixture of topics and each topic consists of a collection of words (Zhang 2018). Different types of topic models exist, each with varying complexity in terms of the way in which topics are generated. The simplest one, Latent Semantic Analysis (LSA), has previously been implemented to discover patterns of lexical cohesion in political speech, specifically that of the former Prime Minister of the United Kingdom, Margaret Thatcher (Klebanov, Diermeier, and Beigman 2008). Improving on LSA methodology, Probabilistic LSA (pLSA) has been implemented in healthcare (Zhu 2014) and educational (Ming et al. 2014) contexts, albeit no application thereof in political science was found. A further sophisticated model, Latent Dirichlet Allocation (LDA), has been used to determine trending topics in news on governmental YouTube channels (Subhan et al. 2023).\n Sentiment analysis (SentA) \nSentA involves deciphering the intent of words to infer certain emotional dimensions labelled either in polarized (negative/positive) or higher-dimensional terms (niche feelings like joy/sadness). Various unigram lexicons have been derived to such extents. For example, the R-based \\(\\texttt{nrc}\\) lexicon dichotomously classifies words with yes/no labels in categories such as positive, negative, anticipation, anger, and so forth. In contrast, the Python-based \\(\\texttt{TextBlob}\\) lexicon processes textual data in the form of a tuple where a polarity score (ranges between -1 and +1 which relates to negative and positive sentiment, respectively) and a subjectivity score (ranges between 0 and 1 which refers to being very objective or very subjective, respectively) is produced. Using such pre-defined lexicons has been previously utilized to analyze political communication, specifically in terms of campaign polarization, via SentA (Haselmayer and Jenny 2017).\n\nData\n\n\n Tokenization \nThe process of tokenization entails breaking up given text into units, referred to as tokens (or terms), which are meaningful for analysis (Zhang 2018). In this case, these tokens take on different structures, based on either a macro-context (i.e., sentences) or micro-context (i.e., words). At both scales, the way in which these tokens are valued will be varied. The value will either be defined by a bag-of-words (BoW) or term-frequency, inverse-document-frequency (tf-idf) approach. The former way implicates accounting for the number of occurrences of some token in some document. On the other hand, the latter way not only regards the frequency of some token, but also the significance thereof. Thus, tf-idf involves the assignment of some weight to each token in a document which in turn reflects its importance relative to the entire collection of documents (corpus). It then follows that the tf-idf value of a token t in a document d within a corpus D is calculated as the product of two constituents. The first being tf(t,d) defined as the quotient of the frequency of token t in document d and the total number of tokens in document d, whereas the second is idf(t, D) denoted by the quotient of the natural logarithm of the total number of documents in corpus D and the number of documents containing the token t (Silge and Robinson 2017). N\n Number of topics \nIn order to determine the optimal number of topics, a coherence score is calculated. This metric measures the ability of a topic model to distinguish well between topics that are semantically interpretable by humans and are not simply statistical-inference artifacts. Hence, the number of topics as well as any other topic-model hyperparameters (like \\(\\alpha\\) and \\(\\beta\\) for LDA) are tuned to values that yield the maximum coherence score, allowing for the most understandable themes.\n\n\n\nMethods\n\n\n\nTopic modelling\n\n Latent Semantic Analysis (LSA) \n\n\n\nFigure 3: Schematic representation of LSA outlining the factorization of the DTM matrix.\n\n\nLSA (Deerwester et al. 1990) is a non-probabilistic, non-generative model where a form of matrix factorization is utilized to uncover few latent topics, capturing meaningful relationships among documents/tokens. As depicted in Figure 3, in the first step, a document-term matrix DTM is generated from the raw text data by tokenizing d documents into w words (or sentences), forming the columns and rows respectively. Each row-column entry is either valued via the BoW or tf-idf approach. This DTM-matrix, which is often sparse and high-dimensional, is then decomposed via a dimensionality-reduction-technique, namely truncated Singular Value Decomposition (SVD). Consequently, in the second step the DTM-matrix becomes the product of three matrices: the topic-word matrix \\(A_{t*}\\) (for the tokens), the topic-prevalence matrix \\(B_{t*}\\) (for the latent semantic factors), and the transposed document-topic matrix \\(C^{T}_{t*}\\) (for the document). Here, t*, the optimal number of topics, is a hyperparameter which is refined at a value (via the coherence-measure approach) that retains the most significant dimensions in the transformed space. In the final step, the text data is then encoded using this top-topic number.\nGiven LSA only implicates a DTM-matrix, the implementation thereof is generally efficient. Though, with the involvement of truncated SVD, some computational intensity and a lack of quick updates with new, incoming text-data can arise. Additional LSA drawbacks include: the lack of interpretability, the underlying linear-model framework (which results in poor performance on text-data with non-linear dependencies), and the underlying Gaussian assumption for tokens in documents (which may not be an appropriate distribution).\n Probabilistic Latent Semantic Analysis (pLSA) \n\n\n\nFigure 4: Schematic representation of pLSA, where the different-shade-of-blue colours highlight similarities shared with LSA-related matrices shown in Figure 3.\n\n\nInstead of implementing truncated SVD, pLSA (Hofmann 1999) rather utilizes a generative, probabilistic model. Within this framework, a document d is first selected with probability P(d). Then given this, a latent topic t is present in this selected document d and so chosen with probability of P(t|d). Finally, given this chosen topic t, a word w (or sentence) is generated from it with probability P(w|t), as shown in Figure 4. It is noted that the values of P(d) is determined directly from the corpus D which is defined in terms of a DTM matrix. In contrast, the probabilities P(t|d) and P(w|t) are parameters modelled as multinomial distributions and iteratively updated via the Expectation-Maximization (EM) algorithm. Direct parallelism between LSA and pLSA can be drawn via the methods’ parameterization, as conveyed via matching colours of the topic-word matrix and P(w|t), the document-topic matrix and P(d|t) as well as the topic-prevalence matrix and P(t) displayed in Figure 3 and Figure 4, respectively.\nDespite pLSA implicitly addressing LSA-related disadvantages, this method still involves two main drawbacks. There is no probability model for the document-topic probabilities P(t|d), resulting in the inability to assign topic mixtures to new, unseen documents not trained on. Model parameters also then increase linearly with the number of documents added, making this method more susceptible to overfitting.\n Latent Dirichlet Allocation \n\n\n\nFigure 5: Schematic representation of LDA where the dark-blue-shaded block represents observed words.\n\n\nLDA is another generative, probabilistic model which can be deemed as a hierarchical Bayesian version of pLSA. Via explicitly defining a generative model for the document-topic probabilities, both the above-mentioned pitfalls of pLSA are improved upon. The number of parameters to estimate drastically decrease and the ability to apply and generalize to new, unseen documents is attainable. As presented in Figure 5, the initial steps first involve randomly sampling a document-topic probability distribution \\(\\theta\\) from a Dirichlet (Dir) distribution \\(\\eta\\), followed by randomly sampling a topic-word probability distribution \\(\\phi\\) from another Dirichlet distribution \\(\\tau\\). From the \\(\\theta\\) distribution, a topic t is selected by drawing from a multinomial (Mult) distribution (third step) and from the \\(\\phi\\) distribution given said topic t, a word w (or sentences) is sampled from another multinomial distribution (fourth step). The associated LDA-parameters are then estimated via a variational expectation maximization algorithm or collapsed Gibbs sampling.\n Correlated Topic Model (CTM) \n\n\n\nFigure 6: Schematic representation of CTM where the dark-blue-shaded block represents observed words, whilst the light-grey colour outlines the distinctions from the LDA topic model presented in Figure 5.\n\n\nFollowing closely to LDA, the CTM (Lafferty and Blei 2005) additionally allows for the ability to model the presence of any correlated topics. Such topic correlations are introduced via the inclusion of the multivariate normal (MultNorm) distribution with t length-vector of means \\(\\mu\\) and t \\(\\times\\) t covariance matrix \\(\\Sigma\\) where the resulting values are then mapped into probabilities by passing through a logistic (log) transformation. Comparing Figure 5 and Figure 6, the nuance between LDA and CTM is highlighted using a light-grey colour, where the discrepancy in the models come about from replacing the Dirichlet distribution (which involves the implicit assumption of independence across topics) with the logit-normal distribution (which now explicitly enables for topic dependency via a covariance structure) for generating document-topic probabilities. The other generative processes previously outlined for LDA is retained and repeated for CTM. Given this additional model complexity, the more convoluted mean-field variational inference algorithm is employed for CTM-parameter estimation which necessitates many iterations for optimization purposes. CTM is consequently computationally more expensive than LDA. Though, this snag is far outweighed by the procurement of richer topics with overt relationships acknowledged between these.\n Author Topic Model (ATM) \n\n\n\nFigure 7: Schematic representation of ATM where the dark-blue-shaded blocks represents observed words and authors, whilst the light-grey colour highlights the differences compared to the LDA topic model presented in Figure 5.\n\n\nATM (Rosen-Zvi et al. 2012) extends LDA via the inclusion of authorship information with topics. Again, inspecting Figure 5 and Figure 7, the slight discrepancies between these two models are accentuated with the light-grey colour. Here, for each word w in the document d an author a is sampled uniformly (Uni) at random. Each author is associated with a distribution over topics (\\(\\Psi\\)) sampled from a Dirichlet prior \\(\\alpha\\). The resultant mixture weights corresponding to the chosen author are used to select a topic t, then a word w (or sentence) is generated according to the topic-word distribution \\(\\phi\\) (drawn from another Dirichlet prior \\(\\beta\\)) corresponding to that said chosen topic t. Therefore, through the estimation of the \\(\\psi\\) and \\(\\phi\\) parameters, not only is information obtained about which topics authors generally relate to, but also a representation of these document contents in terms of these topics, respectively.\n\nSentiment analysis\n\n AFINN \nThe R-based \\(\\texttt{AFINN}\\) lexicon scores words across a range spanning from the value of -5 to +5. Intuitively, words scored closer to the lower-boundary value relate to more negative sentiment, and in contrast higher positive sentiment is revealed if rather closer to the upper-boundary value (Silge and Robinson 2017).\n Bing \nUnlike \\(\\texttt{AFINN}\\) , the R-based \\(\\texttt{bing}\\) lexicon does not provide sentiments via some scoring system. Instead, it simply assigns a binary label of a word being interpreted as either positive or negative (Silge and Robinson 2017).\n\nExploratory Data Analysis\n\n\n\n\n\nFigure 8: Most frequent words used across all SONA speeches, irrespective of president.\n\n\nFrom Figure 8, it is evident that the word “government” is mainly referenced to across all SONA speeches. This word dominance draws upon how the importance of this authority body, which is integral to the governance of SA, is emphasized. The frequent usage of the words “people” and “public” indicates a sense of inclusivity where the idea of togetherness is implicitly suggested. Other words, such as “development” and “new”, are indicative of ideas of growth and renewal. Lastly, a sense of security and safety is provided with the recurring use of the word “ensure”.\n\n\n\n\n\n\n\n(a) de Klerk\n\n\n\n\n\n\n\n(b) Mandela\n\n\n\n\n\n\n\n\n\n(c) Mbeki\n\n\n\n\n\n\n\n(d) Motlanthe\n\n\n\n\n\n\n\n\n\n(e) Zuma\n\n\n\n\n\n\n\n(f) Ramaphosa\n\n\n\n\nFigure 9: Most frequent words used in SONA speeches, faceted by president.\n\n\nAfter faceting the most frequent words by president, as displayed in Figure 9, there are some slight nuances noted. For instance, former president de Klerk used words which were emblematic of the political paradigm shift that occurred during the time of his term. Words such as “transitional”, “constitutional”, and “constitution” reflects the country’s progression from an exclusive, segregated to a more inclusive, democratic state. This political and legal reform directed towards achieving societal equality is further underscored by the words, “parties”, “party”, and “election”. The pivotal role of proper partnerships being formed, which would have further aided in maintaining this change, is foregrounded with the word “alliance”.\nSimilarly, this idea of unity has also been foregrounded in the other five presidents’ speeches with the commonly shared word “people”. Though, unlike de Klerk, the other former presidents (Mandela, Mbeki, Motlanthe, Zuma) and current president (Ramaphosa), seem to similarly place more focus on the explicit communication of policies and vision (“development” and “work”) and the establishment of a sense of responsibility and accountability on their part as president (“government” and “ensure”). Some minor distinctions between these aforementioned presidents can be made. Mandela, Mbeki, and Motlanthe, for example, seemed to draw more attention to “society” or “social” progress, whilst Zuma and Ramaphosa appeared to place more prominence on “economic” progress.\n\nResults\n\n\n\nSentiment Analysis\n\n\n\n\n\n\n\n\n(a) \\(\\texttt{AFINN}\\): All speeches\n\n\n\n\n\n\n\n(b) \\(\\texttt{bing}\\): All speeches\n\n\n\n\nFigure 10: Overall sentiment score for SONA speeches across time (segmented by presidential terms), compared for two different lexicons.\n\n\nComparing Figure 10 (a) and Figure 10 (b), it is evident that there is no obvious, overt difference in the computed net sentiment scores, which are overall positive, across time and presidents for the two different lexicons. Any slight variation between \\(\\texttt{AFINN}\\) and \\(\\texttt{bing}\\) is most likely attributed to the lexicons’ varying scales (+5/-5 versus +1/-1, respectively). Hence, any sentiment derived from the former lexicon might be slightly more exaggerated in nature compared to the latter lexicon. This is noted when checking the \\(y\\)-axes range of the sentiment scores, which reach a maximum of 600 for the \\(\\texttt{AFINN}\\) lexicon and only 300 for the \\(\\texttt{bing}\\) lexicon.\nAcross both lexicons, from de Klerk to Mbeki’s presidential terms, positive sentiment seems to steadily rise. Though, after a peak of high, positive sentiment scores from Mbeki’s SONA speeches, there is a slight decline in this overall positivity. This is especially present throughout Zuma’s presidential term.\n\n\n\n\n\n\n\n(a) \\(\\texttt{AFINN}\\): de Klerk speeches\n\n\n\n\n\n\n\n(b) \\(\\texttt{bing}\\): de Klerk speeches\n\n\n\n\n\n\n\n\n\n(c) \\(\\texttt{AFINN}\\): Mandela speeches\n\n\n\n\n\n\n\n(d) \\(\\texttt{bing}\\): Mandela speeches\n\n\n\n\n\n\n\n\n\n(e) \\(\\texttt{AFINN}\\): Mbeki speeches\n\n\n\n\n\n\n\n(f) \\(\\texttt{bing}\\): Mbeki speeches\n\n\n\n\n\n\n\n\n\n(g) \\(\\texttt{AFINN}\\): Motlanthe speeches\n\n\n\n\n\n\n\n(h) \\(\\texttt{bing}\\): Motlanthe speeches\n\n\n\n\n\n\n\n\n\n(i) \\(\\texttt{AFINN}\\): Zuma speeches\n\n\n\n\n\n\n\n(j) \\(\\texttt{bing}\\): Zuma speeches\n\n\n\n\n\n\n\n\n\n(k) \\(\\texttt{AFINN}\\): Ramaphosa speeches\n\n\n\n\n\n\n\n(l) \\(\\texttt{bing}\\): Ramaphosa speeches\n\n\n\n\nFigure 11: Trajectory of sentiment score through SONA-speech sentences stratified by president, comparing across two different lexicons.\n\n\nFrom inspecting Figure 11 , it is apparent that the relative trajectory of underlying emotion is generally similar for each president. The dips and troughs in sentiment prevalent for both \\(\\texttt{bing}\\) and \\(\\texttt{AFINN}\\) lexicons occur at approximately the same sentences in the respective presidents’ speeches. Though, there is some stark contrast found between the two lexicons when comparing for Zuma’s speeches. For this president, the negative falls and positive rises are more exaggerated for the \\(\\texttt{AFINN}\\) compared to the \\(\\texttt{bing}\\) lexicon. Additionally, the sentiment pattern of Mbeki’s speeches again seems more skewed to the positive side, with more frequent extreme rises to high sentiment score values across sentences. It is also again seen that more negative sentiment is expressed in Zuma’s speeches, give the more dominant dips. For Ramaphosa, there appears to be more of a balance between positive-and-negative sentiment. There are no extreme, outlying rises/falls, rather a more consistent sawtooth-like pattern is prominent.\n\n\n\n\n\n\n\n(a) \\(\\texttt{AFINN}\\): All speeches\n\n\n\n\n\n\n\n(b) \\(\\texttt{bing}\\): All speeches\n\n\n\n\nFigure 12: Words which contribute to the positive and negative sentiment across all SONA speeches.\n\n\nSimilarly with the general sentiment-score trajectories, there are more overlaps between the two lexicons when comparing the specific words which contribute to the positive and negative sentiments, as displayed in Figure 12. The same seven (out of the top ten) words commonly contribute to negative sentiment (“corruption”, “crime”, “violence”), in addition to positive sentiment (“improve”, “support”, “progress”) across both lexicons. Albeit, the extent of these aforementioned words’ contributions to the respective sentiments do slightly vary in amounts. Some unique, independent words also add to the negative sentiment for the \\(\\texttt{AFINN}\\) lexicon (“problems”, “unemployment”) and \\(\\texttt{bing}\\) lexicon (“issues”). Likewise, there are distinctive words for this former lexicon (“growth”, “ensure”, “great”) and latter lexicon (“well”) attributed to positive sentiment.\n\n\n\n\n\n\n\n(a) \\(\\texttt{AFINN}\\): de Klerk speeches\n\n\n\n\n\n\n\n(b) \\(\\texttt{bing}\\): de Klerk speeches\n\n\n\n\n\n\n\n\n\n(c) \\(\\texttt{AFINN}\\): Mandela speeches\n\n\n\n\n\n\n\n(d) \\(\\texttt{bing}\\): Mandela speeches\n\n\n\n\n\n\n\n\n\n(e) \\(\\texttt{AFINN}\\): Mbeki speeches\n\n\n\n\n\n\n\n(f) \\(\\texttt{bing}\\): Mbeki speeches\n\n\n\n\n\n\n\n\n\n(g) \\(\\texttt{AFINN}\\): Motlanthe speeches\n\n\n\n\n\n\n\n(h) \\(\\texttt{bing}\\): Motlanthe speeches\n\n\n\n\n\n\n\n\n\n(i) \\(\\texttt{AFINN}\\): Zuma speeches\n\n\n\n\n\n\n\n(j) \\(\\texttt{bing}\\): Zuma speeches\n\n\n\n\n\n\n\n\n\n(k) \\(\\texttt{AFINN}\\): Ramaphosa speeches\n\n\n\n\n\n\n\n(l) \\(\\texttt{bing}\\): Ramaphosa speeches\n\n\n\n\nFigure 13: Words which contribute to the positive and negative sentiment for each specific presidents’ SONA speech/speeches.\n\n\nAfter faceting Figure 12 by president, as presented in the sub-plots of Figure 13, essentially no variability is indicated in terms of uniqueness and the contribution magnitude thereof. For both lexicons, the same set of words add the same amount to each sentiment. Furthermore, commonalities between words contributing to the sentiments is evident between the five presidents after de Klerk. Negative-sentiment words like “unconstitutional”, “deprive”, “discrimination”, and “boycott” and positive-sentiment words such as “proud”, “succeeded”, and “peaceful” only features in de Klerk’s speech. All of these aforementioned words seem to directly relate to the change in political context during de Klerk’s term. Whilst, the words prevailing in the other five presidents’ speeches appear to foreground the continuation of this changed political climate with positive-sentiment words like “improve”, “better”, “freedom” and “peace”. Additionally, the shared negative-sentiment words like “crime”, “corruption” and “poverty” foreground the commonality of perpetuating problems that became pronounced throughout all presidential tenures after de Klerk.\n\nTopic modelling Results\n\n\n\nLatent Semantic Analysis (LSA)\n\nIn the micro-context (i.e., word tokenization) of LSA implementation, the maximum coherence score of approximately -1.5 seen in Figure 14 indicates that three topics are optimal when utilizing the tf-idf approach. In contrast, there is no discernible difference in the coherence scores across a range of topic numbers when instead using the BoW approach. Hence, for comparative purposes, three topics are also chosen as best in this instance.\n\n\n\nFigure 14: Coherence plot for LSA where SONA speeches were tokenized by words.\n\n\nConsidering Figure 15, a different overarching focus seems to come to the fore contingent on whether the BoW or tf-idf approach is examined. With reference to the BoW-related corpus, the broad scope of the three topics appears associated to national frameworks and the future thereof. More particularly, the first topic labelled as governance alludes to processes (“work”, “development”) established by institutions (“government”) which direct communities (“public”, “people”). Words like “infrastructure”, “investment” and “energy” seem to indicate structural resources, which encompasses the second topic. The third topic, sustainability, is more related to the prospective of long-term (“years”) endurance (“continue”) across these governmental processes and resources.\nOn the other hand, with respect to the tf-idf-related corpus, the three topics now instead appear concentrated on current issues. The first topic centers on a sole problem, particularly that of common (“compatriots”) “pandemic” preparedness (“plan”). Whereas, the second topic broadens to other additional issues contextual to the country. Hence, in this case, the challenges range from energy (“eskom”) problems (“loadshedding”) to the “covid” “pandemic” to the government’s corruption (state “capture”). Most words contained in the third topic all already feature in topic one or two already. Given this lack of unique distinction, topic three is said to simply be a synthesis of the two, aforementioned topics.\n\n\n\n\n\n\n\n(a) BoW: Topic 1 ~ governance\n\n\n\n\n\n\n\n(b) tf-idf: Topic 1 ~ pandemic preparedness\n\n\n\n\n\n\n\n\n\n(c) BoW: Topic 2 ~ structural resources\n\n\n\n\n\n\n\n(d) tf-idf: Topic 2 ~ contemporary challenges\n\n\n\n\n\n\n\n\n\n(e) BoW: Topic 3 ~ sustainability\n\n\n\n\n\n\n\n(f) tf-idf: Topic 3 ~ an amalgamation\n\n\n\n\nFigure 15: Topic-by-word plots for BoW (left) and tf-idf (right) approach for LSA implementation.\n\n\nUnlike within the micro-context, LSA now applied in the maro-context (i.e., sentence tokenization) indicates more variability in the choice of the optimal topic number for both BoW and tf-idf approaches. Due to the excessive jumps between high and low coherence scores across the topic amounts seen in Figure 16, it is opted to choose the lowest best number. This choice aims to limit potential theme-overlapping (i.e., many common words shared across topics) and allow for more conciseness.\n\n\n\nFigure 16: Coherence plot for LSA where SONA speeches were tokenized by sentences.\n\n\nGiven Figure 17, more of an overlap in topic scope across both corpus types is seen for the sentence-tokenized speeches. For instance, the second topic for the BoW-related corpus and the first topic for the tf-idf-related corpus are both labelled as governance, given the commonly shared words (“government”, “people”, “work” and “development”). The first topic for this former-mentioned corpus accounts for the internal structures (“leaders”, “assembly”, “president”, “members”) related to governance. Whilst, the second topic of the tf-idf-related corpus is very distinguishable with the dominant weighting of the word “thank”, connotated with a sense of gratitude. This topic also refers to some reformation indicators like “opportunity” and “growth”.\n\n\n\n\n\n\n\n(a) BoW: Topic 1 ~ governmental structures\n\n\n\n\n\n\n\n(b) tf-idf: Topic 1 ~ governance\n\n\n\n\n\n\n\n\n\n(c) BoW: Topic 2 ~ governance\n\n\n\n\n\n\n\n(d) tf-idf: Topic 2 ~ reformation and gratitude\n\n\n\n\nFigure 17: SONA speeches tokenized by sentences for BoW (left) and tf-idf (right) approach for LSA implementation.\n\n\n\npLSA\n\nAs previously found with the implementation of LSA, there is essentially no variability in the coherence scores when utilizing the BoW approach for the word-tokenized application of pLSA. Though, like with LSA, there are fluctuating changes in the coherence scores across topic numbers when considering the tf-idf approach. Again, it is opted to take the minimum-best (based on tf-idf) number of three topics for both approaches which\n\n\n\nFigure 18: Coherence plot for pLSA where SONA speeches were tokenized by words.\n\n\n\n\n\n\n\n\n\n(a) BoW: Topic 1\n\n\n\n\n\n\n\n(b) tf-idf: Topic 1\n\n\n\n\n\n\n\n\n\n(c) BoW: Topic 2\n\n\n\n\n\n\n\n(d) tf-idf: Topic 2\n\n\n\n\n\n\n\n\n\n(e) BoW: Topic 3\n\n\n\n\n\n\n\n(f) tf-idf: Topic 3\n\n\n\n\n\n\n\n\n\n(g) BoW: Topic 4\n\n\n\n\n\n\n\n(h) tf-idf: Topic 4\n\n\n\n\n\n\n\n\n\n(i) BoW: Topic 5\n\n\n\n\n\n\n\n(j) tf-idf: Topic 5\n\n\n\n\nFigure 19: Topic plots for pLSA tokenized by words and executed within a BoW (left) and tf-idf (right) framework.\n\n\n\n\n\nFigure 20: Coherence plot for pLSA where SONA speeches were tokenized by sentences.\n\n\n\n\n\n\n\n\n\n(a) BoW: Topic 1\n\n\n\n\n\n\n\n(b) tf-idf: Topic 1\n\n\n\n\n\n\n\n\n\n(c) BoW: Topic 2\n\n\n\n\n\n\n\n(d) tf-idf: Topic 2\n\n\n\n\n\n\n\n\n\n(e) BoW: Topic 3\n\n\n\n\n\n\n\n(f) tf-idf: Topic 3\n\n\n\n\n\n\n\n\n\n(g) BoW: Topic 4\n\n\n\n\n\n\n\n(h) tf-idf: Topic 4\n\n\n\n\nFigure 21: Topic plots for pLSA using sentences as tokens and applied within a BoW (left) and tf-idf (right) framework.\n\n\n\nLDA\n\n\n\n\n\n\n\n\n(a) Words\n\n\n\n\n\n\n\n(b) Sentences\n\n\n\n\nFigure 22: Contour plots visualizing variation in coherence scores tuned across range of different \\(\\alpha\\) and \\(\\beta\\) hyperparameter values.\n\n\n\n\n\n\n\nTable 1: Coherence scores obtained from a hyperparameter-combination grid search for implementation of LDA on SONA speeches tokenized by words. \n\n\nCorpus\nTopics\nAlpha\nBeta\nCoherence\n\n\n\n\nBoW\n9\n0.60\n0.50\n-0.42\n\n\nBoW\n9\n0.60\n0.10\n-0.42\n\n\nBoW\n9\n0.60\n0.70\n-0.42\n\n\nBoW\n9\n0.60\n0.90\n-0.42\n\n\nBoW\n9\n0.60\n0.30\n-0.42\n\n\ntf-idf\n7\n0.20\n0.90\n-4.25\n\n\ntf-idf\n7\n0.20\n0.10\n-4.25\n\n\ntf-idf\n7\n0.20\n0.30\n-4.25\n\n\ntf-idf\n7\n0.20\n0.50\n-4.25\n\n\ntf-idf\n7\n0.20\n0.70\n-4.25\n\n\n\n\n\n\n\n\n\n\nTable 2: Coherence scores obtained from a hyperparameter-combination grid search for implementation of LDA on SONA speeches tokenized by sentences. \n\n\nCorpus\nTopics\nAlpha\nBeta\nCoherence\n\n\n\n\nBoW\n2\n0.90\n0.90\n-18.98\n\n\nBoW\n2\n0.80\n0.50\n-18.98\n\n\nBoW\n2\n0.70\n0.10\n-18.98\n\n\nBoW\n2\n0.70\n0.30\n-18.98\n\n\nBoW\n2\n0.70\n0.50\n-18.98\n\n\ntf-idf\n2\n0.40\n0.10\n-20.56\n\n\ntf-idf\n2\n0.40\n0.30\n-20.56\n\n\ntf-idf\n2\n0.40\n0.50\n-20.56\n\n\ntf-idf\n2\n0.40\n0.70\n-20.56\n\n\ntf-idf\n2\n0.40\n0.90\n-20.56\n\n\n\n\n\n pyLDAvis for Words \n\n\n\n\n\n\n\n\n\n\n\n pyLDAvis for Sentences \n\n\n\n\n\n\n\n\n\n\n\n\nCTM\n\n\n\n\n\n\n\n\n(a) Words ~ economy\n\n\n\n\n\n\n\n(b) Sentences ~ economy\n\n\n\n\nFigure 23: Coherence plots for CTM across different tokens (words or sentences) implemented within BoW approach.\n\n\n\n\n\n\n\n\n\n(a) Sentences: Topic 1 ~ economy\n\n\n\n\n\n\n\n(b) Words: Topic 1 ~ economy\n\n\n\n\n\n\n\n\n\n(c) Sentences: Topic 2 ~ economic and structural advancement\n\n\n\n\n\n\n\n(d) Words: Topic 2 ~ societal collectives\n\n\n\n\n\n\n\n\n\n(e) Sentences: Topic 3 ~ social collectives\n\n\n\n\nFigure 24: Topic plots where either sentences (left) or words (right) were used as tokens for CTM application.\n\n\n\nATM (Author-Topic Model)\n\n\n\n\n\n\n\n\n(a) Words\n\n\n\n\n\n\n\n(b) Sentences\n\n\n\n\nFigure 25: Coherence plots for ATM across different tokens (words or sentences) implemented within BoW approach.\n\n\n\n\n\n\n\n\n\n(a) Sentences: Topic 1 ~ social cohesion\n\n\n\n\n\n\n\n(b) Words: Topic 1 ~ diplomatic strategy\n\n\n\n\n\n\n\n\n\n(c) Sentences: Topic 2 ~ prospective plans\n\n\n\n\n\n\n\n(d) Words: Topic 2 ~ general strategic vision\n\n\n\n\n\n\n\n\n\n(e) Sentences: Topic 3 ~ esteem within governmental structures\n\n\n\n\nFigure 26: Topic plots where either sentences (left) or words (right) were used as tokens for ATM application.\n\n\n\n\n\n\n\nReferences\n\nCho, Hae-Wol. 2019. “Topic Modeling.” Osong Public Health and Research Perspectives 10 (June): 115–16. https://doi.org/10.24171/j.phrp.2019.10.3.01.\n\n\nDeerwester, Scott, Susan T. Dumais, George W. Furnas, Thomas K. Landauer, and Richard Harshman. 1990. “Indexing by Latent Semantic Analysis.” Journal of the American Society for Information Science 41 (6): 391–407. https://doi.org/https://doi.org/10.1002/(SICI)1097-4571(199009)41:6&lt;391::AID-ASI1&gt;3.0.CO;2-9.\n\n\nHaselmayer, Martin, and Marcelo Jenny. 2017. “Sentiment Analysis of Political Communication: Combining a Dictionary Approach with Crowdcoding.” Quality and Quantity 51 (November): 2623–46. https://doi.org/10.1007/s11135-016-0412-4.\n\n\nHofmann, Thomas. 1999. “Probabilistic Latent Semantic Indexing.” In Proceedings of the 22nd Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, 50–57. SIGIR ’99. New York, NY, USA: Association for Computing Machinery. https://doi.org/10.1145/312624.312649.\n\n\nKlebanov, Beata Beigman, Daniel Diermeier, and Eyal Beigman. 2008. “Lexical Cohesion Analysis of Political Speech.” Political Analysis 16 (4): 447–63. http://www.jstor.org/stable/25791949.\n\n\nLafferty, John, and David Blei. 2005. “Correlated Topic Models.” In Advances in Neural Information Processing Systems, edited by Y. Weiss, B. Schölkopf, and J. Platt. Vol. 18. MIT Press. https://proceedings.neurips.cc/paper_files/paper/2005/file/9e82757e9a1c12cb710ad680db11f6f1-Paper.pdf.\n\n\nMing, Ding, Dong Bin, Yan Yonghong, and Ding Yousheng. 2014. “The Application of PLSA Features in the Automatic Assessment System for English Oral Test.” Computer Modelling and New Technologies 18: 414–18. http://www.cmnt.lv/en/on-line-journal/2014/2014-volume-18-12/part-c-operation-research-and-decision-making/the-application-of-plsa-features-in-the-automatic-assessment-system-for-english-oral-test.\n\n\nMinister Faith Muthambi. 2017. “SONA enables us to take part in our democracy.” 2017. https://www.gcis.gov.za/sona-enables-us-take-part-our-democracy.\n\n\nMiranda, John Paul P., and Rex P. Bringula. 2021. “Exploring Philippine Presidents’ speeches: A sentiment analysis and topic modeling approach.” Edited by John Kwame Boateng. Cogent Social Sciences 7 (1): 1932030. https://doi.org/10.1080/23311886.2021.1932030.\n\n\nRosen-Zvi, Michal, Thomas Griffiths, Mark Steyvers, and Padhraic Smyth. 2012. “The Author-Topic Model for Authors and Documents.” https://arxiv.org/abs/1207.4169.\n\n\nSilge, Julia, and David Robinson. 2017. Text Mining with R: A Tidy Approach. 1st ed. O’Reilly Media, Inc.\n\n\nSubhan, Subhan, M. Faris Al Hakim, Prasetyo Listiaji, and Wahyu Syafrizal. 2023. “Modeling news topics on government youtube channels with latent Dirichlet allocation method.” AIP Conference Proceedings 2614 (1): 040009. https://doi.org/10.1063/5.0125954.\n\n\nZhang, Zhiyong. 2018. “Text Mining for Social and Behavioral Research using R.” 2018. https://books.psychstat.org/textmining/index.html.\n\n\nZhu, Shaoping. 2014. “Pain Expression Recognition Based on pLSA Model.” The Scientific World Journal, 2356–6140. https://doi.org/10.1155/2014/736106."
>>>>>>> 9326fdfa4e5f503da6f1fb968943122f200f7190
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "STA5073Z Data Science for Industry Assignment 2",
    "section": "",
    "text": "This is the website for the second assignment (pertaining to sentiment analysis and topic modelling) of the Masters-level course “Data Science for Industry” (DS4I) at the University of Cape Town. All relevant files related to this assignment are archived here."
  },
  {
    "objectID": "index.html#links-to-the-repos",
    "href": "index.html#links-to-the-repos",
    "title": "STA5073Z Data Science for Industry Assignment 2",
    "section": "Links to the repos:",
    "text": "Links to the repos:\n\nWebsite files\nLink to the website files"
  },
  {
    "objectID": "plaigarism_declaration.html",
    "href": "plaigarism_declaration.html",
    "title": "Plaigarism Declaration",
    "section": "",
    "text": "We, Heiletjé van Zyl and Jared Tavares, declare that this second DS4I assignment titled, “Sentiments and Topics in South African SONA Speeches” and the work presented in it are our own. We confirm that:\n\nThis work was done wholly while in candidature as an accredited course to obtain a degree at this University.\nThe contents of this assignment has not been previously submitted for a degree or any other qualification at this University or any other institution.\nWhere we have consulted the published work of others, this is always clearly attributed.\nWhere we have quoted from the work of others, the source is always given. With the exception of such quotations, this assignment is entirely our own work.\nWe have acknowledged all main sources of help.\n\nSignature: HMMvZ, JMT\nDate: 14 November 2023"
  },
  {
    "objectID": "code_appendix.html",
    "href": "code_appendix.html",
    "title": "Code",
    "section": "",
    "text": "# Loading in the necessary libraries\nimport zipfile\nimport os\nimport pandas as pd\nimport re\nimport matplotlib.pyplot as plt\nfrom wordcloud import WordCloud, STOPWORDS\nfrom itertools import cycle\nimport seaborn as sns\nimport numpy as np\nfrom collections import Counter\nimport nltk\nfrom nltk.tokenize import word_tokenize\nfrom nltk.corpus import wordnet, stopwords, words\nfrom nltk.stem import WordNetLemmatizer\nfrom ast import literal_eval\nfrom collections import defaultdict\nimport pyLDAvis.gensim_models\nimport gensim\nfrom gensim.models import LsiModel, TfidfModel\nfrom gensim.models.coherencemodel import CoherenceModel\nfrom gensim import corpora\nfrom gensim.utils import simple_preprocess\nfrom plsa import Corpus, Pipeline, Visualize\nfrom plsa.pipeline import DEFAULT_PIPELINE\nfrom plsa.algorithms import PLSA\nfrom gensim.models import AuthorTopicModel\nfrom gensim.models import LdaModel\nimport tqdm\n\n# Global params\nplt.rcParams['xtick.labelsize'] = 18\nplt.rcParams['ytick.labelsize'] = 18\n\n# Set the global label sizes for the plots\nplt.rcParams['axes.labelsize'] = 20\n\n# Set the global legend size\nplt.rcParams['legend.fontsize'] = 18\n\n# Unzip the file and get the list of filenames\nwith zipfile.ZipFile(\"data/speeches.zip\", 'r') as zip_ref:\n    zip_ref.extractall(\"data\")\n\nfilenames = os.listdir(\"data\")\nfilenames = [filename for filename in filenames if filename.endswith('.txt')]\n\n# Read the content of each speech file and extract the date from the first line\nspeeches = []\ndates = []\nfor filename in filenames:\n    with open(os.path.join(\"data\", filename), 'r', encoding='utf-8') as file:\n        # Extract date from the first line\n        date = file.readline().strip()\n        dates.append(date)\n        \n        # Read the rest of the file\n        speeches.append(file.read())\n\n# Create DataFrame\nsona = pd.DataFrame({'filename': filenames, 'speech': speeches, 'date': dates})\n\n# Extract year and president for each speech\nsona['year'] = sona['filename'].str[:4]\nsona['president'] = sona['filename'].str.split('_').str[-1].str.split('.').str[0]\n\n# Clean the sona dataset by removing unnecessary text\nreplace_reg = r'(http.*?(\\s|.$))|(www.*?(\\s|.$))|&amp;|&lt;|&gt;|\\n'\nsona['speech'] = sona['speech'].str.replace(replace_reg, ' ')\n\n# Split speeches into sentences\nsona_sentences = sona.copy()\n\nimport itertools\n\n# Replace new lines with space and split into sentences based on regular expression\nsona_sentences['speech'] = sona_sentences['speech'].str.replace('\\n', ' ').str.split(r'(?&lt;!\\w\\.\\w.)(?&lt;![A-Z][a-z]\\.)(?&lt;=\\.|\\?)\\s')\n\n# Flatten the list of sentence fragments to avoid nested lists\nsona_sentences['speech'] = sona_sentences['speech'].apply(lambda sentences: list(itertools.chain.from_iterable(sentence.split('.') for sentence in sentences)))\n\n# Remove empty strings from the list of sentences\nsona_sentences['speech'] = sona_sentences['speech'].apply(lambda sentences: [sentence.strip() for sentence in sentences if sentence.strip()])\n\n# Make a csv of the speeches\nsona.to_csv('data/sona_speeches.csv', index=False)\n\n# Make a csv of the sentences\nsona_sentences.to_csv('data/sona_sentences_untransformed.csv', index=False)\n\n\n# Make sure to download the necessary NLTK corpus if you haven't already\n#nltk.download('wordnet')\n#nltk.download('averaged_perceptron_tagger')\n#nltk.download('words')\n\n# Read in the sona speeches dataset\nsona_speeches_df = pd.read_csv('data/sona_speeches.csv')\nsona_sentences_clean = pd.read_csv('data/sona_sentences_untransformed.csv')\nsona_sentences_clean['speech'] = sona_sentences_clean['speech'].apply(literal_eval)\n\n# Initialize the WordNetLemmatizer\nlemmatizer = WordNetLemmatizer()\nstop_words = set(stopwords.words('english'))\nenglish_words = set(words.words())\nadditional_words = {\n    'honourable', 'member', 'chairperson',\n    'south', 'africa', 'african', 'africans', 'year',\n    'madame', 'madam', 'soes', 'ms', 'madams', 'madames', 'mw',\n    'compatriotsthe',\n    'also'\n}\n\n# Function to convert NLTK's part-of-speech tags to WordNet's part-of-speech tags\ndef get_wordnet_pos(word):\n    \"\"\"Map NLTK part of speech tags to WordNet part of speech tags.\"\"\"\n    tag = nltk.pos_tag([word])[0][1][0].upper()\n    tag_dict = {\"J\": nltk.corpus.wordnet.ADJ,\n                \"N\": nltk.corpus.wordnet.NOUN,\n                \"V\": nltk.corpus.wordnet.VERB,\n                \"R\": nltk.corpus.wordnet.ADV}\n\n    return tag_dict.get(tag, nltk.corpus.wordnet.NOUN)\n\n# Clean the text, convert to lowercase, and lemmatize each word\ndef clean_text(text):\n    # Remove special characters: keep only letters, numbers, and basic punctuation\n    text = re.sub(r'[.;]', ' ', text)  # Replaces periods with spaces\n    text = re.sub(r'[^a-zA-Z ]', '', text)\n    text = text.lower()  # Convert to lowercase\n    \n    # Tokenize the text\n    words = word_tokenize(text)\n    \n    # Remove stop words\n    words = [word for word in words if word not in stop_words]\n    \n    # Remove additional words\n    words = [word for word in words if word not in additional_words]\n\n    # Lemmatize each word with the correct POS tag\n    lemmatized_words = []\n    for word, tag in nltk.pos_tag(words):\n        wntag = get_wordnet_pos(tag)\n        lemmatized_word = lemmatizer.lemmatize(word, wntag)\n        # Only append the lemmatized word if it is in the set of English words\n        if lemmatized_word in english_words:\n            lemmatized_words.append(lemmatized_word)\n    \n    # Join the lemmatized words back into one string\n    text = ' '.join(words)\n    return text\n\ndef clean_text_no_word_removals(text):\n    # Remove special characters: keep only letters, numbers, and basic punctuation\n    text = re.sub(r'[.;]', ' ', text)  # Replaces periods with spaces\n    text = re.sub(r'[^a-zA-Z ]', '', text)\n    text = text.lower()  # Convert to lowercase\n    return text\n\n# Apply the cleaning function to the speech column\ntempdf = sona_speeches_df.copy()\nsona_speeches_df['speech'] = tempdf['speech'].apply(clean_text)\nsona_speeches_df['speech_untrans'] = tempdf['speech'].apply(clean_text_no_word_removals)\n\ndef clean_speeches(speeches):\n    # The input is expected to be a list of strings\n    return [clean_text(sentence) for sentence in speeches]\n\n# Apply the cleaning to the sentences too\nsona_sentences_clean['sentence'] = sona_sentences_clean['speech'].apply(lambda speeches: [clean_text(sentence) for sentence in speeches])\n\n# Apply the cleaning to sentences that need to keep their words\nsona_sentences_clean['sent_untrans'] = sona_sentences_clean['speech'].apply(lambda speeches: [clean_text_no_word_removals(sentence) for sentence in speeches])\n\n# Make a csv of the speeches\nsona_speeches_df.to_csv('data/sona_speeches_adapted.csv', index=False)\n\n# Remove the speech column from the sentences DataFrame\nsona_sentences_clean.drop(columns=['speech'], inplace=True)\n\n# Make a csv of the sentences\nsona_sentences_clean.to_csv('data/sona_sentences_clean.csv', index=False)\n\nsona_sentences_clean = pd.read_csv('data/sona_sentences_clean.csv')\nsona_sentences_clean['sentence'] = sona_sentences_clean['sentence'].apply(literal_eval)\nsona_sentences_clean['sent_untrans'] = sona_sentences_clean['sent_untrans'].apply(literal_eval)\n\n# Make the sentences into a single column\nsona_sentences_alltogether = sona_sentences_clean.explode('sentence')\nsona_sentences_all_untrans = sona_sentences_clean.explode('sent_untrans')\n\n# Drop the other columns\nsona_sentences_alltogether.drop(columns=['sent_untrans'], inplace=True)\nsona_sentences_all_untrans.drop(columns=['sentence'], inplace=True)\n\n# Make a csv of the sentences\nsona_sentences_all_untrans.to_csv('data/sona_sentiment_sentences.csv', index=False)\n\n# Speeches\nsona_speeches_clean = pd.read_csv('data/sona_speeches_adapted.csv')\n\n\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n\nmax_features = 2000\n\nbow_vectorizer = CountVectorizer(max_features=max_features)\ntfidf_vectorizer = TfidfVectorizer(max_features=max_features)\n\n# Transformed on the words\nbow_matrix_words = bow_vectorizer.fit_transform(sona_speeches_clean['speech'])\ntfidf_matrix_words = tfidf_vectorizer.fit_transform(sona_speeches_clean['speech'])\n\n\nfrom matplotlib.colors import LinearSegmentedColormap\n\ncmap = plt.cm.cividis\n\nnorm = plt.Normalize(0, 100)\n\n# Define a colour map based on cividis\n# Define a new colormap using a smaller slice of the cividis colormap, this time stopping well before the yellows\ncividis_modified = cmap(np.linspace(0, 0.4, cmap.N))  # Using only 40% of the colormap range\n\n# Create a new colormap from the data\ncividis_no_yellow_light = LinearSegmentedColormap.from_list('cividis_no_yellow_light', cividis_modified)\n\n# Let's pick three colors from the modified colormap\ncolormap = [cividis_no_yellow_light(norm(0)), \n          cividis_no_yellow_light(norm(50)), \n          cividis_no_yellow_light(norm(100))]\n\n\nfrom sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\nfrom collections import Counter\nimport matplotlib.pyplot as plt\n\n# Function to count words in speeches excluding stopwords\ndef get_word_frequencies(speeches, stopwords):\n    word_counts = Counter()\n    for speech in speeches:\n        words = speech.lower().split()\n        # Remove stopwords from the count\n        words = [word.strip('.,!?\"\\'-()') for word in words if word.strip('.,!?\"\\'-()') not in stopwords]\n        word_counts.update(words)\n    return word_counts\n\n# Get the word frequencies excluding stopwords\nword_frequencies = get_word_frequencies(sona_speeches_clean['speech'], ENGLISH_STOP_WORDS)\n\n# Get the top 10 most frequent words across all speeches\ntop_10_words = word_frequencies.most_common(10)\n\n\n# Plotting\nplt.figure(figsize=(10, 6))\nplt.bar([word for word, count in top_10_words], [count for word, count in top_10_words], color=colormap[2])\nplt.xlabel('Words')\nplt.ylabel('Frequency')\nplt.xticks()\nplt.yticks()\nplt.xticks(rotation=45)\n\n# Save the plot as a PNG file\nplt.savefig(f'saved_plots/overall_top_words.png', bbox_inches='tight')\nplt.close()  # Close the figure to avoid displaying it in the notebook\n\n\n# Function to get top N frequent words for each president\ndef get_top_words_by_president(speeches_df, n, stopwords):\n    presidents = speeches_df['president'].unique()\n    top_words_by_president = {}\n    for president in presidents:\n        president_speeches = speeches_df[speeches_df['president'] == president]['speech']\n        word_frequencies = get_word_frequencies(president_speeches, stopwords)\n        top_words_by_president[president] = word_frequencies.most_common(n)\n    return top_words_by_president\n\n# Get the top 10 most frequent words for each president\ntop_10_words_by_president = get_top_words_by_president(sona_speeches_clean, 10, ENGLISH_STOP_WORDS)\n\n\n# Plot the word frequencies for each president\nfor president, top_words in top_10_words_by_president.items():\n    \n    # Individual plot for each president\n    plt.figure(figsize=(10, 6))\n    plt.bar([word for word, count in top_words], [count for word, count in top_words], color=colormap[0])\n    plt.xlabel('Words')\n    plt.ylabel('Frequency')\n    plt.xticks(rotation=45)\n\n    # Save the plot as a PNG file\n    plt.savefig(f'saved_plots/{president}_top_words.png', bbox_inches='tight')\n    plt.close()  # Close the figure to avoid displaying it in the notebook\n\n\nimport pandas as pd\nimport nltk\nfrom nltk.corpus import opinion_lexicon\nfrom nltk.tokenize import treebank\nfrom afinn import Afinn\n\nfrom dateutil import parser\n\n# Function to parse date strings based on the described rule\ndef parse_date(date_str):\n    # Split the string by comma and take the last part\n    date_part = date_str.split(',')[-1].strip()\n    # Parse the date part into a datetime object\n    return parser.parse(date_part)\n\n# Define a function to get Bing lexicon sentiment scores\ndef get_bing_sentiment(text):\n    tokenizer = treebank.TreebankWordTokenizer()\n    tokens = tokenizer.tokenize(text.lower())\n    pos_score = sum(1 for word in tokens if word in positive_words)\n    neg_score = sum(1 for word in tokens if word in negative_words)\n    compound_score = pos_score - neg_score\n    return compound_score\n\n\n# Load the AFINN lexicon\nafinn = Afinn()\n\n# Define a function to get AFINN sentiment scores\ndef get_afinn_sentiment(text):\n    return afinn.score(text)\n\n# Load positive and negative words\npositive_words = set(opinion_lexicon.positive())\nnegative_words = set(opinion_lexicon.negative())\n\n# Apply Bing sentiment analysis\nsona_speeches_clean['bing_sentiment'] = sona_speeches_clean['speech_untrans'].apply(get_bing_sentiment)\nsona_sentences_all_untrans['bing_sentiment'] = sona_sentences_all_untrans['sent_untrans'].apply(get_bing_sentiment)\n\n# Apply AFINN sentiment analysis\nsona_speeches_clean['afinn_sentiment'] = sona_speeches_clean['speech_untrans'].apply(lambda text: get_afinn_sentiment(text))\nsona_sentences_all_untrans['afinn_sentiment'] = sona_sentences_all_untrans['sent_untrans'].apply(lambda text: get_afinn_sentiment(text))\n\n# Convert the date strings to datetime objects\nsona_speeches_clean['date'] = sona_speeches_clean['date'].apply(parse_date)\nsona_sentences_all_untrans['date'] = sona_sentences_all_untrans['date'].apply(parse_date)\n\n# Sort the DataFrames by date in ascending order\nsona_speeches_clean.sort_values('date', ascending=True, inplace=True)\n#sona_sentences_all_untrans.sort_values('date', ascending=True, inplace=True)\n\n# Create a new variable which is the date as a string\nsona_speeches_clean['date_str'] = sona_speeches_clean['date'].dt.strftime('%Y-%m-%d')\nsona_sentences_all_untrans['date_str'] = sona_sentences_all_untrans['date'].dt.strftime('%Y-%m-%d')\n\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\n# For plotting sentiment scores of speeches by each president\ndef plot_speeches_by_president(df, lexicon):\n    plt.figure(figsize=(10, 6))\n\n    presidents = df['president'].unique()\n\n    lexicon_lab = lexicon\n\n    if lexicon == 'afinn':\n        lexicon_lab = 'AFINN'\n\n    colors = ['lightsteelblue', colormap[1], 'midnightblue', 'lightgray', 'darkgray',  'dimgray']\n\n    for idx, president in enumerate(presidents):\n        president_df = df[df['president'] == president]\n        plt.bar(president_df['date_str'], president_df[f'{lexicon}_sentiment'], label=president, color=colors[idx])\n    plt.xlabel('Date')\n    plt.ylabel(f'Sentiment Score')\n    plt.xticks(rotation=45)\n    plt.yticks()\n    plt.legend(loc =\"upper left\")\n    plt.savefig(f'sentiment_plots/speech_{lexicon}_all.png', bbox_inches='tight')\n    plt.close()  # Close the figure to avoid displaying it in the notebook\n\n\n# For plotting sentiment scores of sentences by each president\ndef plot_sentences_by_president(df, lexicon):\n    #plt.figure(figsize=(10, 6))\n\n    presidents = df['president'].unique()\n\n    colors = [colormap[1],  'dimgray', 'midnightblue', 'darkgray', 'lightsteelblue', 'lightgray']\n\n    lexicon_lab = lexicon\n\n    if lexicon == 'afinn':\n        lexicon_lab = 'AFINN'\n    \n    # Create a copy of the DataFrame\n    df = df.copy()\n\n    # Add a column for the sentence number\n    df['sentence_num'] = df.groupby('date_str').cumcount() + 1\n    \n    i = 0\n\n    for idx, president in enumerate(presidents):\n        plt.figure(figsize=(10, 6))\n        president_df = df[df['president'] == president]\n        plt.bar(president_df['sentence_num'], president_df[f'{lexicon}_sentiment'], label=president, color=colors[i])\n        plt.xlabel('Sentence')\n        plt.ylabel(f'Sentiment Score')\n        plt.xticks()\n        plt.savefig(f'sentiment_plots/sent_{lexicon}_{president}.png', bbox_inches='tight')\n        plt.close()  # Close the figure to avoid displaying it in the notebook\n\n        i += 1\n\n    \n# Assuming 'date' is a column in datetime format and 'president' is the name of each president\nplot_speeches_by_president(sona_speeches_clean, 'bing')\nplot_speeches_by_president(sona_speeches_clean, 'afinn')\n\nplot_sentences_by_president(sona_sentences_all_untrans, 'bing')\nplot_sentences_by_president(sona_sentences_all_untrans, 'afinn')\n\n# Function to calculate word sentiments across all speeches of a president\ndef calculate_word_sentiments(president_speeches, lexicon):\n    # Combine all speeches into one large text\n    all_speeches = ' '.join(president_speeches)\n    # Tokenize the text into words and filter out stopwords and non-alphabetic tokens\n    words = [word for word in word_tokenize(all_speeches.lower()) if word.isalpha() and word not in stopwords.words('english')]\n    # Get sentiment score for each word\n    word_sentiments = defaultdict(int)\n    for word in words:\n        # Get the sentiment score for the word\n        if lexicon == 'bing':\n            sentiment = get_bing_sentiment(word)\n        elif lexicon == 'afinn':\n            sentiment = get_afinn_sentiment(word)\n\n        word_sentiments[word] += sentiment\n    return word_sentiments\n\n# Function to plot the top positive and negative words\ndef plot_top_words(word_sentiments, president, lexicon):\n    # Sort words by sentiment score\n    sorted_words = sorted(word_sentiments.items(), key=lambda kv: kv[1])\n    # Select the top 10 positive and negative words\n    top_positive_words = sorted_words[-10:]\n    top_negative_words = sorted_words[:10]\n\n    # Words and their sentiment scores for plotting\n    words_positive, scores_positive = zip(*top_positive_words)\n    words_negative, scores_negative = zip(*top_negative_words)\n\n    fig, ax = plt.subplots(figsize=(10, 6))\n\n    # Plot the top negative words\n    ax.barh(range(10), scores_negative, color=colormap[2], label='Negative')\n    # Plot the top positive words\n    ax.barh(range(10, 20), scores_positive, color=colormap[0], label='Positive')\n\n    # Add the word labels\n    ax.set_yticks(range(20))\n    ax.set_yticklabels(words_negative + words_positive)\n    \n    # Set the labels and title\n    ax.set_xlabel(f'Contribution to Sentiment Score')\n    ax.legend(loc = \"lower right\")\n\n    # Adjust the view so negative words are at the bottom and positive at the top\n    ax.set_ylim(-1, 20)\n    \n    plt.tight_layout()\n    # Save the plot as a PNG file\n    plt.savefig(f'sentiment_plots/word_contr_{lexicon}_{president}.png', bbox_inches='tight')\n    plt.close()  # Close the figure to avoid displaying it in the notebook\n\n# Aggregate the speeches by president and calculate the top words\npresidents_speeches = sona_speeches_clean.groupby('president')['speech_untrans'].apply(list)\nfor president, speeches in presidents_speeches.items():\n    word_sentiments_bing = calculate_word_sentiments(speeches, 'bing')\n    word_sentiments_afinn = calculate_word_sentiments(speeches, 'afinn')\n    plot_top_words(word_sentiments_bing, president, 'bing')\n    plot_top_words(word_sentiments_bing, president, 'AFINN')\n\n\n# Function to calculate word sentiments across all speeches\ndef calculate_word_sentiments(speeches, lexicon):\n    # Combine all speeches into one large text\n    all_speeches = ' '.join(speeches)\n    # Tokenize the text into words and filter out stopwords and non-alphabetic tokens\n    words = [word for word in word_tokenize(all_speeches.lower()) if word.isalpha() and word not in stopwords.words('english')]\n    # Get sentiment score for each word\n    word_sentiments = defaultdict(int)\n    for word in words:\n        # Get the sentiment score for the word\n        if lexicon == 'bing':\n            sentiment = get_bing_sentiment(word)\n        elif lexicon == 'afinn':\n            sentiment = get_afinn_sentiment(word)\n\n        word_sentiments[word] += sentiment\n    return word_sentiments\n\n# Function to plot the top positive and negative words\ndef plot_top_words(word_sentiments, lexicon):\n    # Sort words by sentiment score\n    sorted_words = sorted(word_sentiments.items(), key=lambda kv: kv[1])\n    # Select the top 10 positive and negative words\n    top_positive_words = sorted_words[-10:]\n    top_negative_words = sorted_words[:10]\n\n    # Words and their sentiment scores for plotting\n    words_positive, scores_positive = zip(*top_positive_words)\n    words_negative, scores_negative = zip(*top_negative_words)\n\n    fig, ax = plt.subplots(figsize=(10, 6))\n\n    # Plot the top negative words\n    ax.barh(range(10), scores_negative, color=colormap[2], label='Negative')\n    # Plot the top positive words\n    ax.barh(range(10, 20), scores_positive, color=colormap[0], label='Positive')\n\n    # Add the word labels\n    ax.set_yticks(range(20))\n    ax.set_yticklabels(words_negative + words_positive)\n    # ax.yticks(fontsize=16)\n\n    # Set the labels and title\n    ax.set_xlabel(f'Contribution to Sentiment Score')\n    ax.legend()\n    # ax.xticks(fontsize=16)\n\n    # Adjust the view so negative words are at the bottom and positive at the top\n    ax.set_ylim(-1, 20)\n    \n    plt.tight_layout()\n    plt.savefig(f'sentiment_plots/word_contr_{lexicon}_all.png', bbox_inches='tight')\n    plt.close()  # Close the figure to avoid displaying it in the notebook\n\n# Calculate the word sentiments across all speeches for each lexicon\nall_speeches = sona_speeches_clean['speech_untrans'].tolist()\nword_sentiments_bing = calculate_word_sentiments(all_speeches, 'bing')\nword_sentiments_afinn = calculate_word_sentiments(all_speeches, 'afinn')\n\n# Plot the top words for each lexicon\nplot_top_words(word_sentiments_bing, 'bing')\nplot_top_words(word_sentiments_afinn, 'AFINN')\n\n\ntexts = sona_speeches_clean['speech']\nsentences = sona_sentences_alltogether['sentence']\n\n# Further process tokens using gensim's simple_preprocess\ntokenized_texts = [simple_preprocess(doc, deacc=True) for doc in texts]  # deacc=True removes punctuations\ntokenized_sentences = [simple_preprocess(doc, deacc=True) for doc in sentences]  # deacc=True removes punctuations\n\n# Create a Gensim dictionary from the tokenized documents\ndictionary = corpora.Dictionary(tokenized_texts)\ndict_sentences = corpora.Dictionary(tokenized_sentences)\n\n#dictionary.filter_extremes(no_below=2, no_above=0.7)\n#dict_sentences.filter_extremes(no_below=2, no_above=0.7)\n\n# Create a BOW corpus\nbow_corpus = [dictionary.doc2bow(text) for text in tokenized_texts]\nbow_corpus_sentences = [dict_sentences.doc2bow(text) for text in tokenized_sentences]\n\n# Create a TF-IDF corpus\ntfidf = TfidfModel(bow_corpus)\ntfidf_corpus = tfidf[bow_corpus]\n\ntfidf_sentences = TfidfModel(bow_corpus_sentences)\ntfidf_corpus_sentences = tfidf_sentences[bow_corpus_sentences]\n\n\n# Define the function to compute coherence values\ndef compute_coherence_values(dictionary, corpus, texts, start, limit, step, coherence='u_mass'):\n    coherence_values = []\n    model_list = []\n    for num_topics in range(start, limit, step):\n        model = LsiModel(corpus=corpus, num_topics=num_topics, id2word=dictionary)\n        model_list.append(model)\n        coherencemodel = CoherenceModel(model=model, texts=tokenized_texts, dictionary=dictionary, coherence=coherence)\n        coherence_values.append(coherencemodel.get_coherence())\n    return model_list, coherence_values\n\n# Set parameters\nstart, limit, step = 2, 20, 1\n\n# Compute coherence values for BOW\nbow_model_list, bow_coherence_values = compute_coherence_values(dictionary=dictionary, corpus=bow_corpus, texts=texts, start=start, limit=limit, step=step)\n\nbow_model_list_sentences, bow_coherence_values_sentences = compute_coherence_values(dictionary=dict_sentences, corpus=bow_corpus_sentences, texts=sentences, start=start, limit=limit, step=step)\n\n# Compute coherence values for TF-IDF\ntfidf_model_list, tfidf_coherence_values = compute_coherence_values(dictionary=dictionary, corpus=tfidf_corpus, texts=tokenized_texts, start=start, limit=limit, step=step)\n\ntfidf_model_list_sentences, tfidf_coherence_values_sentences = compute_coherence_values(dictionary=dict_sentences, corpus=tfidf_corpus_sentences, texts=sentences, start=start, limit=limit, step=step)\n\n# Save the to csv\ncoherence_df = pd.DataFrame({'bow_coherence_values': bow_coherence_values, 'tfidf_coherence_values': tfidf_coherence_values, 'bow_coherence_values_sentences': bow_coherence_values_sentences, 'tfidf_coherence_values_sentences': tfidf_coherence_values_sentences})\n\ncoherence_df.to_csv('lsa_plots/coherence_values.csv', index=False)\n\n\nfrom matplotlib.ticker import MaxNLocator\n\n# Read in the coherence values\ncoherence_df = pd.read_csv('lsa_plots/coherence_values.csv')\n\n# Extract the coherence values\nbow_coherence_values = coherence_df['bow_coherence_values']\ntfidf_coherence_values = coherence_df['tfidf_coherence_values']\nbow_coherence_values_sentences = coherence_df['bow_coherence_values_sentences']\ntfidf_coherence_values_sentences = coherence_df['tfidf_coherence_values_sentences']\n\n# Plotting the coherence values\nx = range(start, limit, step)\nplt.figure(figsize=(10, 6))\nplt.plot(x, bow_coherence_values, label='BoW', color='midnightblue')\nplt.plot(x, tfidf_coherence_values, label='tf-idf', color='darkgray')\nplt.xlabel(\"Number of Topics\")\nplt.ylabel(\"Coherence score\")\nplt.legend(loc='lower left')\nplt.gca().xaxis.set_major_locator(MaxNLocator(integer=True))\nplt.savefig(f'lsa_plots/words_coherence_plots.png', bbox_inches='tight')\nplt.close()\n\n\n# Plotting the coherence values\nx = range(start, limit, step)\nplt.figure(figsize=(10, 6))\nplt.plot(x, bow_coherence_values_sentences, label='BoW', color='midnightblue')\nplt.plot(x, tfidf_coherence_values_sentences, label='tf-idf', color='darkgray')\nplt.xlabel(\"Number of Topics\")\nplt.ylabel(\"Coherence score\")\nplt.legend(loc='lower right')\nplt.gca().xaxis.set_major_locator(MaxNLocator(integer=True))\nplt.savefig(f'lsa_plots/sentence_coherence_plots.png', bbox_inches='tight')\nplt.close()\n\n\n# Set the number of topics based on the plots\nlsa_bow_words = LsiModel(corpus=bow_corpus, num_topics=3, id2word=dictionary)\nlsa_bow_sentences = LsiModel(corpus=bow_corpus_sentences, num_topics=2, id2word=dict_sentences)\n\nlsa_tfidf_words = LsiModel(corpus=tfidf_corpus, num_topics=3, id2word=dictionary)\nlsa_tfidf_sentences = LsiModel(corpus=tfidf_corpus_sentences, num_topics=2, id2word=dict_sentences)\n\nimport matplotlib.pyplot as plt\n\n# Function to plot the top words for each topic in a single LSA model\ndef plot_top_words_for_each_topic(model, fiton, lexicon, num_words=10):\n\n    colors = ['lightsteelblue', 'midnightblue', 'lightgray', 'darkgray', 'dimgray']\n    \n    j = 0\n\n    for i in range(model.num_topics):\n        # Extract the top words for this topic\n        top_words = model.show_topic(i, num_words)\n        # Separate the words and their corresponding weights\n        words, weights = zip(*top_words)\n        weights = [abs(weight) for weight in weights]  # Use absolute values for weights\n\n        # Create a bar chart for the top words in this topic\n        plt.figure(figsize=(10, 6))\n        plt.barh(words, weights, color = colors[j])\n        # ax.set_yticklabels(words, fontsize=20)\n        j += 1\n\n        if j == 4:\n            j = 0\n\n        plt.xlabel('Weight')\n        plt.gca().invert_yaxis()  # Highest weights on top\n        plt.savefig(f'lsa_plots/{fiton}_{lexicon}_topic_{i + 1}.png', bbox_inches='tight')\n        plt.close()\n\n\n# Apply the plotting function to each of your LSA models\nplot_top_words_for_each_topic(lsa_bow_words, 'words', 'bow')\nplot_top_words_for_each_topic(lsa_bow_sentences, 'sentences', 'bow')\nplot_top_words_for_each_topic(lsa_tfidf_words, 'words', 'tfidf')\nplot_top_words_for_each_topic(lsa_tfidf_sentences, 'sentences', 'tfidf')\n\nsona_speeches_clean['speech'].to_csv('data/sona_speeches_only.csv', index=False)\nsona_sentences_alltogether['sentence'].to_csv('data/sona_sentences_only.csv', index=False)\n\nimport math\n\ndef calculate_umass_coherence(plsa_result, corpus, top_n=10, tf_idf=False):\n    # Extract the top N words for each topic\n    top_words_by_topic = [\n        [word for word, _ in plsa_result.word_given_topic[i][:top_n]]\n        for i in range(plsa_result.n_topics)\n    ]\n    \n    # Get document-word matrix (document frequency matrix)\n    doc_word_matrix = corpus.get_doc_word(tf_idf=tf_idf)\n    \n    # Calculate document frequencies for single words\n    word_doc_freq = np.sum(doc_word_matrix &gt; 0, axis=0)\n    \n    # Calculate coherence for each topic\n    topic_coherences = []\n    for top_words in top_words_by_topic:\n        pair_scores = []\n        for i, word in enumerate(top_words):\n            for j in range(i + 1, len(top_words)):\n                # Get indices in the vocabulary\n                word_i_index = corpus.index[word]\n                word_j_index = corpus.index[top_words[j]]\n                \n                # Count the documents where both words appear\n                both_docs = np.sum((doc_word_matrix[:, word_i_index] &gt; 0) & (doc_word_matrix[:, word_j_index] &gt; 0))\n                \n                # Calculate score for this word pair\n                score = math.log((both_docs + 1.0) / word_doc_freq[word_i_index])  # Add 1 to avoid log(0)\n                pair_scores.append(score)\n                \n        # Average over all pairs to get the topic coherence\n        topic_coherence = sum(pair_scores) / len(pair_scores)\n        topic_coherences.append(topic_coherence)\n        \n    # Average over all topics to get the overall coherence\n    overall_coherence = sum(topic_coherences) / len(topic_coherences)\n    return overall_coherence\n\n\npipeline = Pipeline(*DEFAULT_PIPELINE)\n\ncorpus = Corpus.from_csv(\"data/sona_speeches_only.csv\", pipeline)\ncorpus_sent = Corpus.from_csv(\"data/sona_sentences_only.csv\", pipeline)\n\ntopic_numbers = range(2, 12, 1)\n\n# Loop over the topic number and calculate the coherence values\nbow_coherence_values = []\nbow_coherence_values_sent = []\ntfidf_coherence_values = []\ntfidf_coherence_values_sent = []\n\nfor n_topics in topic_numbers:\n    # Initialize the models\n    tfidf_plsa = PLSA(corpus, n_topics, tf_idf=True)\n    bow_plsa = PLSA(corpus, n_topics, tf_idf=False)\n\n    tfidf_plsa_sent = PLSA(corpus_sent, n_topics, tf_idf=True)\n    bow_plsa_sent = PLSA(corpus_sent, n_topics, tf_idf=False)\n\n    # Fit the models\n    tfidf_result = tfidf_plsa.fit()\n    bow_result = bow_plsa.fit()\n    tfidf_result_sent = tfidf_plsa_sent.fit()\n    bow_result_sent = bow_plsa_sent.fit()\n\n    # Calculate the coherence values\n    bow_coherence_values.append(calculate_umass_coherence(bow_result, corpus, tf_idf=False))\n    tfidf_coherence_values.append(calculate_umass_coherence(tfidf_result, corpus, tf_idf=True))\n    bow_coherence_values_sent.append(calculate_umass_coherence(bow_result_sent, corpus_sent, tf_idf=False))\n    tfidf_coherence_values_sent.append(calculate_umass_coherence(tfidf_result_sent, corpus_sent, tf_idf=True))\n\n\n# Save the coherence value results\npd.DataFrame({\n    'topic_number': topic_numbers,\n    'bow_coherence': bow_coherence_values,\n    'tfidf_coherence': tfidf_coherence_values,\n    'bow_coherence_sent': bow_coherence_values_sent,\n    'tfidf_coherence_sent': tfidf_coherence_values_sent\n}).to_csv('data/saved_plsa_coherence_values.csv', index=False)\n\n\n# Load the coherence value results\ncoherence_values = pd.read_csv('data/saved_plsa_coherence_values.csv')\n\ntopic_numbers = coherence_values['topic_number']\nbow_coherence_values = coherence_values['bow_coherence']\ntfidf_coherence_values = coherence_values['tfidf_coherence']\nbow_coherence_values_sent = coherence_values['bow_coherence_sent']\ntfidf_coherence_values_sent = coherence_values['tfidf_coherence_sent']\n\n# Plot the speech coherence values - updated\nplt.figure(figsize=(10, 6))\nplt.plot(topic_numbers, bow_coherence_values, label='BoW', color='midnightblue')\nplt.plot(topic_numbers, tfidf_coherence_values, label='tf-idf', color='darkgray')\nplt.xlabel(\"Number of Topics\")\nplt.ylabel(\"Coherence score\")\nplt.legend(loc='lower right')\nplt.savefig(f'plsa_plots/words_coherence.png', bbox_inches='tight')\nplt.close()\n\n\n# Plot the sentence coherence values - updated\nplt.figure(figsize=(10, 6))\nplt.plot(topic_numbers, bow_coherence_values_sent, label='BoW', color='midnightblue')\nplt.plot(topic_numbers, tfidf_coherence_values_sent, label='tf-idf', color='darkgray')\nplt.xlabel(\"Number of Topics\")\nplt.ylabel(\"Coherence score\")\nplt.legend(loc='lower right')\nplt.savefig(f'plsa_plots/sentences_coherence.png', bbox_inches='tight')\nplt.close()\n\n# Fit the models with the calibrated number of topics\ntfidf_plsa = PLSA(corpus, 5, tf_idf=True)\nbow_plsa = PLSA(corpus, 5, tf_idf=False)\ntfidf_plsa_sent = PLSA(corpus_sent, 4, tf_idf=True)\nbow_plsa_sent = PLSA(corpus_sent, 4, tf_idf=False)\n\n# Fit the models\ntfidf_result = tfidf_plsa.fit()\nbow_result = bow_plsa.fit()\ntfidf_result_sent = tfidf_plsa_sent.fit()\nbow_result_sent = bow_plsa_sent.fit()\n\n# Function to plot the top words for a given topic\ndef plot_top_words_for_topic(word_given_topic, topic_num, corptype, color, top_n=10):\n    # Extract the top words for this topic\n    top_words_data = word_given_topic[topic_num][:top_n]\n    words, probabilities = zip(*top_words_data)\n\n    # Create a bar plot\n    plt.figure(figsize=(10, 6))\n    plt.barh(words, probabilities, color=color)\n    plt.xlabel('Probability')\n    plt.gca().invert_yaxis() \n    plt.savefig(f'plsa_plots/{corptype}_topic_{topic_num + 1}.png', bbox_inches='tight')\n    plt.close()\n\n\ndef plot_word_contribution(result, corptype):\n    # Number of topics in your model\n    n_topics = len(result.word_given_topic)\n\n    colors = ['lightsteelblue', 'midnightblue', 'lightgray', 'darkgray', 'dimgray']\n    \n    i = 0\n\n    # Plot the top words for each topic\n    for topic_num in range(n_topics):\n        plot_top_words_for_topic(result.word_given_topic, topic_num, corptype, colours[i])\n        i+=1\n\n        # if i == 4:\n        #     i = 0\n\n# Plot the word contribution for each topic for each model\nplot_word_contribution(bow_result, 'words-BoW')\nplot_word_contribution(tfidf_result, 'words-tf-idf')\nplot_word_contribution(bow_result_sent, 'sentences-BoW')\nplot_word_contribution(tfidf_result_sent, 'sentences-tf-idf')\n\n\ntexts = sona_speeches_clean['speech']\nsentences = sona_sentences_alltogether['sentence']\n\n# Further process tokens using gensim's simple_preprocess\ntokenized_texts = [simple_preprocess(doc, deacc=True) for doc in texts]  # deacc=True removes punctuations\ntokenized_sentences = [simple_preprocess(doc, deacc=True) for doc in sentences]  # deacc=True removes punctuations\n\n# Create a Gensim dictionary from the tokenized documents\ndictionary = corpora.Dictionary(tokenized_texts)\ndictionary.filter_extremes(no_below=3, no_above=0.7)\n\ndict_sentences = corpora.Dictionary(tokenized_sentences)\ndict_sentences.filter_extremes(no_below=3, no_above=0.7)\n\n# Create a BOW corpus\nbow_corpus = [dictionary.doc2bow(text) for text in tokenized_texts]\nbow_corpus_sentences = [dict_sentences.doc2bow(text) for text in tokenized_sentences]\n\n# Create a TF-IDF corpus\ntfidf = TfidfModel(bow_corpus)\ntfidf_corpus = tfidf[bow_corpus]\n\ntfidf_sentences = TfidfModel(bow_corpus_sentences)\ntfidf_corpus_sentences = tfidf_sentences[bow_corpus_sentences]\n\n\n# Define the function to compute coherence values\ndef compute_coherence_values(corpus, dictionary, k, a, b, texts):\n    lda_model = LdaModel(corpus=corpus,\n                         id2word=dictionary,\n                         num_topics=k, \n                         random_state=100,\n                         eval_every=None,\n                         alpha=a,\n                         eta=b)\n    \n    coherence_model_lda = CoherenceModel(model=lda_model, texts=texts, dictionary=dictionary, coherence='u_mass')\n    \n    return coherence_model_lda.get_coherence()\n\n# Define the parameter space for grid search\ngrid = {}\ngrid['Validation_Set'] = {}\n# Topics range\nmin_topics = 2\nmax_topics = 10\nstep_size = 1\ntopics_range = range(min_topics, max_topics, step_size)\n# Alpha parameter\nalpha = list(np.arange(0.1, 1, 0.1))\n# Beta parameter\nbeta = list(np.arange(0.1, 1, 0.2))\n\n# Validation sets\nnum_of_docs = len(bow_corpus)\ncorpus_sets = [tfidf_corpus, \n               bow_corpus]\ncorpus_title = ['TF-IDF Corpus', 'BoW Corpus']\nmodel_results = {'Validation_Set': [],\n                 'Topics': [],\n                 'Alpha': [],\n                 'Beta': [],\n                 'Coherence': []\n                }\nmodel_results_sentences = {'Validation_Set': [],\n                 'Topics': [],\n                 'Alpha': [],\n                 'Beta': [],\n                 'Coherence': []\n                }\n\n# Can take a long time to run\n# If you want to only test a few models, reduce the number of steps in topics_range\n# and/or limit the number of values in alpha and beta lists.\nif 1 == 1:\n    pbar = tqdm.tqdm(total=(max_topics-min_topics)*len(alpha)*len(beta)*len(corpus_sets))\n    \n    # iterate through validation corpuses\n    for i in range(len(corpus_sets)):\n        # iterate through number of topics\n        for k in topics_range:\n            # iterate through alpha values\n            for a in alpha:\n                # iterare through beta values\n                for b in beta:\n                    # get the coherence score for the given parameters\n                    cv = compute_coherence_values(corpus=corpus_sets[i], dictionary=dictionary, k=k, a=a, b=b, texts=tokenized_texts)\n                    # Save the model results\n                    model_results['Validation_Set'].append(corpus_title[i])\n                    model_results['Topics'].append(k)\n                    model_results['Alpha'].append(a)\n                    model_results['Beta'].append(b)\n                    model_results['Coherence'].append(cv)\n                    \n                    pbar.update(1)\n    pbar.close()\n\n\n# Get the results for the sentences\nif 1 == 1:\n    pbar = tqdm.tqdm(total=(max_topics-min_topics)*len(alpha)*len(beta)*len(corpus_sets))\n    \n    # iterate through validation corpuses\n    for i in range(len(corpus_sets)):\n        # iterate through number of topics\n        for k in topics_range:\n            # iterate through alpha values\n            for a in alpha:\n                # iterare through beta values\n                for b in beta:\n                    # get the coherence score for the given parameters\n                    cv = compute_coherence_values(corpus=corpus_sets[i], dictionary=dict_sentences, k=k, a=a, b=b, texts=tokenized_sentences)\n                    # Save the model results\n                    model_results_sentences['Validation_Set'].append(corpus_title[i])\n                    model_results_sentences['Topics'].append(k)\n                    model_results_sentences['Alpha'].append(a)\n                    model_results_sentences['Beta'].append(b)\n                    model_results_sentences['Coherence'].append(cv)\n                    \n                    pbar.update(1)\n    pbar.close()\n\n\n# # Save the results to a csv\n# model_results_df = pd.DataFrame(model_results)\n# model_results_sentences_df = pd.DataFrame(model_results_sentences)\n\n# model_results_df.to_csv('data/sona_speeches_lda.csv', index=False)\n# model_results_sentences_df.to_csv('data/sona_sentences_lda.csv', index=False)\n\n# Save the results to a csv\nmodel_results_df = pd.read_csv('data/sona_speeches_lda.csv')\nsorted_speeches_df = model_results_df.sort_values(by='Coherence', ascending=False)\n\n# Concatenate the head and tail of the DataFrame\ncombined_speeches_df = pd.concat([sorted_speeches_df.head(5), sorted_speeches_df.tail(5)])\n\ncombined_speeches_df['Validation_Set'] = combined_speeches_df['Validation_Set'].replace(['TF-IDF Corpus', 'BoW Corpus'], ['tf-idf', 'BoW'])\n\n# Change the validation set column name from \"Validation_Set\" to \"Corpus\"\ncombined_speeches_df = combined_speeches_df.rename(columns={'Validation_Set': 'Corpus'})\n\n# Save the results to a csv\nmodel_results_sentences_df = pd.read_csv('data/sona_sentences_lda.csv')\nsorted_sentences_df = model_results_sentences_df.sort_values(by='Coherence', ascending=False)\n\n# Concatenate the head and tail of the DataFrame\ncombined_sentences_df = pd.concat([sorted_sentences_df.head(5), sorted_sentences_df.tail(5)])\ncombined_sentences_df['Validation_Set'] = combined_sentences_df['Validation_Set'].replace(['TF-IDF Corpus', 'BoW Corpus'], ['tf-idf', 'BoW'])\n\n# Change the validation set column name from \"Validation_Set\" to \"Corpus\"\ncombined_sentences_df = combined_sentences_df.rename(columns={'Validation_Set': 'Corpus'})\n\nnum_cols = combined_sentences_df.select_dtypes(include=['number']).columns\n\n# Remove the first value from the num_cols\nnum_cols = num_cols[1:]\n\n# Use pivot_table to handle duplicate (Alpha, Beta) pairs by averaging their coherence values\npivot_table = model_results_df.pivot_table(index='Alpha', columns='Beta', values='Coherence', aggfunc=np.mean)\npivot_table_sentences = model_results_sentences_df.pivot_table(index='Alpha', columns='Beta', values='Coherence', aggfunc=np.mean)\n\n# Create the meshgrid for Alpha and Beta values\nAlpha, Beta = np.meshgrid(pivot_table.columns, pivot_table.index)\nAlpha_sentences, Beta_sentences = np.meshgrid(pivot_table_sentences.columns, pivot_table_sentences.index)\n\n# Create the contour plot using the values of the pivot_table\n# We need to use the values attribute to get the coherence scores as a 2D array\nplt.figure(figsize=(8, 6))\ncp = plt.contourf(Alpha, Beta, pivot_table.values, cmap='seismic', levels=100)\nplt.colorbar(cp)\nplt.xlabel('Beta')\nplt.ylabel('Alpha')\nplt.savefig(f'lda_plots/words_contour_plot.png', bbox_inches='tight')\nplt.close()\n\nplt.figure(figsize=(8, 6))\ncp = plt.contourf(Alpha_sentences, Beta_sentences, pivot_table_sentences.values, cmap='seismic', levels=100)\nplt.colorbar(cp)\nplt.xlabel('Beta')\nplt.ylabel('Alpha')\nplt.savefig(f'lda_plots/sentences_contour_plot.png', bbox_inches='tight')\nplt.close()\n\ndef style_df(df):\n    # Select the numeric columns except the first one\n    numeric_cols = df.select_dtypes(include=['number']).columns[1:]\n    format_dict = {col: \"{:.2f}\" for col in numeric_cols}\n    styles = [\n        dict(selector=\"th\", props=[(\"text-align\", \"center\")]),\n        dict(selector=\"td\", props=[(\"text-align\", \"center\")]),\n        dict(selector=\"\", props=[(\"margin\", \"auto\"), (\"border\", \"1px solid black\")])\n    ]\n    return df.style.set_table_styles(styles).format(format_dict).hide()\n\n\n# Save the results to a csv\nstyle_df(combined_speeches_df)\n\n\n# Save the results to a csv\nstyle_df(combined_sentences_df)\n\n\n# Train the best models for each corpus\nlda_model_speeches = LdaModel(corpus=bow_corpus,\n                         id2word=dictionary,\n                         num_topics=9, \n                         random_state=100,\n                         eval_every=None,\n                         alpha=0.6,\n                         eta=0.5)\n\nlda_model_sentences = LdaModel(corpus=bow_corpus,\n                         id2word=dict_sentences,\n                         num_topics=2, \n                         random_state=100,\n                         eval_every=None,\n                         alpha=0.9,\n                         eta=0.9)\n\n# Prepare the visualization data\nvis_data_speeches = pyLDAvis.gensim_models.prepare(lda_model_speeches, bow_corpus, dictionary)\nvis_data_sentences = pyLDAvis.gensim_models.prepare(lda_model_sentences, bow_corpus_sentences, dict_sentences)\n\n# Enable the automatic display of visualizations in the Jupyter notebook\npyLDAvis.enable_notebook()\n\n# Display the visualization\npyLDAvis.display(vis_data_speeches)\n\n\n# Display the visualization\npyLDAvis.display(vis_data_sentences)\n\n\nimport tomotopy as tp\nimport matplotlib.pyplot as plt\n\n# Function to calculate coherence scores\ndef calculate_coherence(model, metric='u_mass'):\n    coherence = tp.coherence.Coherence(model, coherence=metric)\n    return coherence.get_score()\n\n# Prepare the data for the CTM model\ntokenized_docs = [text.split() for text in sona_speeches_clean['speech']]  # Ensure the texts are tokenized\ntokenized_sentences = [text.split() for text in sona_sentences_alltogether['sentence']]  # Ensure the texts are tokenized\n\n\n# Define the range of topic numbers you want to test\ntopic_numbers = range(2, 12, 1)  # for example from 2 to 20 by step of 2\n\n# Store coherence scores for plotting\ncoherence_scores = []\ncoherence_scores_sentences = []\n\nfor k in topic_numbers:\n    # Initialize CTM with the current number of topics\n    ctm = tp.CTModel(k=k)\n    ctms = tp.CTModel(k=k)\n\n    # Add documents to the model\n    for tokens in tokenized_docs:\n        ctm.add_doc(tokens)\n\n    # Add sentences to the model\n    for tokens in tokenized_sentences:\n        ctms.add_doc(tokens)    \n\n    # Train the model\n    ctm.train(0)\n    ctms.train(0)\n\n    for _ in range(100):\n        ctm.train(10)\n        ctms.train(10)\n\n    # Calculate and store the coherence score\n    score = calculate_coherence(ctm)\n    score_sentences = calculate_coherence(ctms)\n\n    coherence_scores.append(score)\n    coherence_scores_sentences.append(score_sentences)\n\n    #(f\"Topics: {k}, Coherence Score: {score}\")\n\n# Save the coherence scores to a csv\npd.DataFrame({\n    'topic_number': topic_numbers,\n    'bow_coherence_values': coherence_scores,\n    'bow_coherence_values_sentences': coherence_scores_sentences\n}).to_csv('data/saved_ctm_coherence_values.csv', index=False)\n\n\n# Read in the saved values\ncoherence_values = pd.read_csv('data/saved_ctm_coherence_values.csv')\n\ntopic_numbers = coherence_values['topic_number']\ncoherence_scores = coherence_values['bow_coherence_values']\ncoherence_scores_sentences = coherence_values['bow_coherence_values_sentences']\n\n# Plot the speech coherence scores\nplt.figure(figsize=(10, 6))\nplt.plot(topic_numbers, coherence_scores,  color='midnightblue')\nplt.xlabel('Number of Topics')\nplt.ylabel('Coherence Score')\nplt.xticks(topic_numbers)\nplt.savefig(f'ctm_plots/words_coherence_plots.png', bbox_inches='tight')\nplt.close()\n\n# Plot the sentence coherence scores\nplt.figure(figsize=(10, 6))\nplt.plot(topic_numbers, coherence_scores_sentences, color='lightsteelblue')\nplt.xlabel('Number of Topics')\nplt.ylabel('Coherence Score')\nplt.xticks(topic_numbers)\nplt.savefig(f'ctm_plots/sentences_coherence_plots.png', bbox_inches='tight')\nplt.close()\n\n# Train the models with the optimal number of topics\nctm = tp.CTModel(k=2)\nctms = tp.CTModel(k=3)\n\n# Add documents to the model\nfor tokens in tokenized_docs:\n    ctm.add_doc(tokens)\n\n# Add sentences to the model\nfor tokens in tokenized_sentences:\n    ctms.add_doc(tokens)\n\n# Train the model\nctm.train(0)\nctms.train(0)\n\nfor _ in range(100):\n    ctm.train(10)\n    ctms.train(10)\n\n# Function to plot the top words for one topic\ndef plot_top_words_for_topic(model, modtype, topic_num, color, top_n=10):\n    # Extract the top words for this topic\n    top_words = model.get_topic_words(topic_num, top_n=top_n)\n    words, weights = zip(*top_words)\n\n    # Create a bar chart for the top words in this topic\n    plt.figure(figsize=(10, 6))\n    plt.barh(words, weights, color=color)\n    plt.xlabel('Weight')\n    plt.gca().invert_yaxis()  # Highest weights on top\n    plt.savefig(f'ctm_plots/{modtype}_{topic_num+1}.png', bbox_inches='tight')\n    plt.close()\n\n# Plot the top words for each topic for the CTModel of documents\ni = 0\ncolours = ['midnightblue', 'lightsteelblue']\n\nfor k in range(ctm.k):\n    plot_top_words_for_topic(ctm, 'words', k, colours[i])\n    i+=1\n\n    if i == 2:\n        i = 0\n\ni = 0\n# Plot the top words for each topic for the CTModel of sentences\nfor k in range(ctms.k):\n    plot_top_words_for_topic(ctms, 'sentences', k, colours[i])\n    i+=1\n\n    if i == 2:\n        i = 0\n\n\n# Note that ATM only works for BoW. Raw word counts (BoW) is standard because these models are based on the assumption that the data is generated from a multinomial distribution, which does not hold with TF-IDF weights.\n\ntexts = sona_speeches_clean['speech']\nsentences = sona_sentences_alltogether['sentence']\n\n# Further process tokens using gensim's simple_preprocess\ntokenized_texts = [simple_preprocess(doc, deacc=True) for doc in texts]  # deacc=True removes punctuations\ntokenized_sentences = [simple_preprocess(doc, deacc=True) for doc in sentences]  # deacc=True removes punctuations\n\n# Create a Gensim dictionary from the tokenized documents\ndictionary = corpora.Dictionary(tokenized_texts)\ndictionary.filter_extremes(no_below=3, no_above=0.7)\n\ndict_sentences = corpora.Dictionary(tokenized_sentences)\ndict_sentences.filter_extremes(no_below=3, no_above=0.7)\n\n# Create a BOW corpus\nbow_corpus = [dictionary.doc2bow(text) for text in tokenized_texts]\nbow_corpus_sentences = [dict_sentences.doc2bow(text) for text in tokenized_sentences]\n\n# Prepare the data for the AuthorTopicModel\n# Create a mapping of authors to documents\nauthor2doc = {author: [] for author in sona_speeches_clean['president'].unique()}\nfor i, row in sona_speeches_clean.iterrows():\n    author2doc[row['president']].append(i)\n\n# Create a mapping of authors to sentences\nauthor2sent = {author: [] for author in sona_sentences_alltogether['president'].unique()}\nfor i, row in sona_sentences_alltogether.iterrows():\n    author2sent[row['president']].append(i)\n\n\n# Define the range of topic numbers you want to test\ntopic_numbers = range(2, 12, 1)  # for example from 2 to 20 by step of 2\n\n# Store coherence scores for plotting\ncoherence_scores = []\ncoherence_scores_sentences = []\n\nfor num_topics in topic_numbers:\n    # Author-Topic LDA model with the current number of topics\n    author_topic_model = AuthorTopicModel(corpus=bow_corpus, author2doc=author2doc, id2word=dictionary, num_topics=num_topics)\n    author_topic_model_sentences = AuthorTopicModel(corpus=bow_corpus_sentences, author2doc=author2sent, id2word=dict_sentences, num_topics=num_topics)\n\n    # Train the model\n    author_topic_model.update(bow_corpus, author2doc=author2doc)\n    author_topic_model_sentences.update(bow_corpus_sentences, author2doc=author2sent)\n\n    # Compute coherence score\n    cm = CoherenceModel(model=author_topic_model, texts=tokenized_docs, dictionary=dictionary, coherence='u_mass')\n    cm_sentences = CoherenceModel(model=author_topic_model_sentences, texts=tokenized_sentences, dictionary=dict_sentences, coherence='u_mass')\n\n    coherence = cm.get_coherence()\n    coherence_sentences = cm_sentences.get_coherence()\n\n    coherence_scores.append(coherence)\n    coherence_scores_sentences.append(coherence_sentences)\n\n# Save the coherence scores to a csv\npd.DataFrame({\n    'topic_number': topic_numbers,\n    'bow_coherence_values': coherence_scores,\n    'bow_coherence_values_sentences': coherence_scores_sentences\n}).to_csv('data/saved_atm_coherence_values.csv', index=False)\n\n\n\n# Read in the saved values\ncoherence_values = pd.read_csv('data/saved_atm_coherence_values.csv')\n\ntopic_numbers = coherence_values['topic_number']\ncoherence_scores = coherence_values['bow_coherence_values']\ncoherence_scores_sentences = coherence_values['bow_coherence_values_sentences']\n\n# Plot the speech coherence scores\nplt.figure(figsize=(10, 6))\nplt.plot(topic_numbers, coherence_scores,  color='midnightblue')\nplt.title('Coherence Scores by Number of Topics')\nplt.xlabel('Number of Topics')\nplt.ylabel('Coherence Score')\nplt.xticks(topic_numbers)\nplt.savefig(f'atm_plots/words_coherence_plots.png', bbox_inches='tight')\nplt.close()\n\n# Plot the sentence coherence scores\nplt.figure(figsize=(10, 6))\nplt.plot(topic_numbers, coherence_scores_sentences, color='lightsteelblue')\nplt.title('Coherence Scores by Number of Topics')\nplt.xlabel('Number of Topics')\nplt.ylabel('Coherence Score')\nplt.xticks(topic_numbers)\nplt.savefig(f'atm_plots/sentences_coherence_plots.png', bbox_inches='tight')\nplt.close()\n\n\n# Author-Topic LDA model with the current number of topics\nauthor_topic_model = AuthorTopicModel(corpus=bow_corpus, author2doc=author2doc, id2word=dictionary, num_topics=2)\nauthor_topic_model_sentences = AuthorTopicModel(corpus=bow_corpus_sentences, author2doc=author2sent, id2word=dict_sentences, num_topics=3)\n\n# Train the model\nauthor_topic_model.update(bow_corpus, author2doc=author2doc)\nauthor_topic_model_sentences.update(bow_corpus_sentences, author2doc=author2sent)\n\ncolours = ['midnightblue', 'lightsteelblue', 'darkgray']\n\n# Function to plot the top words for one topic in an AuthorTopicModel\ndef plot_top_words_for_author_topic_model(model, modtype, topic_num, colour, top_n=10):\n    # Extract the top words for this topic\n    top_words = model.get_topic_terms(topic_num, topn=top_n)\n    words, weights = zip(*[(model.id2word[word_id], weight) for word_id, weight in top_words])\n\n    # Create a bar chart for the top words in this topic\n    plt.figure(figsize=(10, 6))\n    plt.barh(words, weights, color=colour)\n    plt.xlabel('Weight')\n    plt.gca().invert_yaxis()  # Highest weights on top\n    plt.savefig(f'atm_plots/{modtype}_{topic_num+1}.png', bbox_inches='tight')\n    plt.close()\n\n# Plot the top words for each topic for the AuthorTopicModel of documents\ni = 0\nfor k in range(author_topic_model.num_topics):\n    plot_top_words_for_author_topic_model(author_topic_model, \"words\",k, colours[i])\n\n    i += 1\n\n    if i == 3:\n        i = 0\n\ni = 0\n# Plot the top words for each topic for the AuthorTopicModel of sentences\nfor k in range(author_topic_model_sentences.num_topics):\n    plot_top_words_for_author_topic_model(author_topic_model_sentences, \"sentences\", k, colours[i])\n\n    i += 1\n\n    if i == 3:\n        i = 0\n\nfrom pprint import pprint\n\ndef show_author(name, model, topic_labels):\n    print('\\n%s' % name)\n    print('Docs:', pd.Series(author_topic_model.author2doc[name]).unique())\n    print('Topics:')\n    pprint([(topic_labels[topic[0]], topic[1]) for topic in model[name]])"
  },
  {
    "objectID": "chatgpt_part.html",
    "href": "chatgpt_part.html",
    "title": "ChatGPT Reflection",
    "section": "",
    "text": "In an endeavour to explore the capabilities of artificial intelligence in the realm of data analysis, specifically sentiment analysis and topic modelling, we employed ChatGPT, a large language model developed by OpenAI. This reflection critically assesses its performance across five distinct areas.\n\nCode Generation for Python:\n\nChatGPT demonstrated a commendable aptitude in assisting with Python code generation. Using ChatGPT4, which has a Python interpreter, its ability to understand and generate code snippets was impressive, significantly streamlining the initial stages of the data analysis process. However, it is important to note that the model’s suggestions occasionally required adjustments to align with specific project requirements, underscoring the necessity for a foundational understanding of Python. Prompts that were slightly vague also resulted in less relevant outputs, highlighting the importance of providing specific instructions to the model.\n\nAcademic Writing:\n\nThe model’s proficiency extended to aiding in the creation of academic write-ups. Upon receiving a bullet-point list of key findings (from figures or tables), ChatGPT efficiently transformed it into a coherent and elegantly structured paragraph, adhering to an academic style. This feature was particularly beneficial in synthesising complex information into a format suitable for academic discourse.\n\nDebugging and Quarto Markdown:\n\nChatGPT’s performance in debugging code and resolving Quarto markdown errors was a mixed experience. While it efficiently identified and suggested fixes for several common errors, its capability was somewhat limited in diagnosing more complex, context-specific issues (Quarto markdown is also relatively new, so the model may not have had enough data to train on for QMD files specifically). This limitation necessitated additional manual intervention, indicating an area for potential improvement.\n\nCreative Writing Processes:\n\nThe model’s utility in creative tasks, such as generating topic names based on a list of keywords, was noteworthy. It exhibited a creative flair, suggesting engaging and pertinent topic names that encapsulated the essence of the top words in a given topic (and often the same names as ones that a human would come up with). This feature was invaluable in adding a creative dimension to the otherwise technical task of topic modelling.\n\nResearch Questions and Citations:\n\nChatGPT proved to be a reliable resource in answering general research questions, although it struggled to match those answers to accurate citations, which meant that sources would often have to be found manually. Despite this drawback, its ability to utilise a wide range of sources and present information in a concise manner was particularly advantageous. However, the model’s reliance on pre-existing knowledge up to its last training cut-off in April 2023 meant that the most recent publications were not included, highlighting the importance of supplementing its suggestions with up-to-date research, if required.\n\nAdditional comments:\n\nChatGPT, in its role as an assistant in this sentiment analysis and topic modelling project, exhibited strengths in code generation, academic writing, and creative brainstorming. However, its limitations in handling complex debugging tasks and its dependency on pre-trained data for research and citations suggest that while it is an invaluable tool, it cannot yet fully replace human expertise and up-to-date research methodologies. Prompts to guide the model towards more specific outputs and periodic validation of its suggestions against current standards were essential in optimising its performance. In a nutshell, it is currently a more powerful version of a search engine and just like a search engine, you have to have an idea of what you are looking for."
  }
]