[
  {
    "objectID": "code_appendix.html",
    "href": "code_appendix.html",
    "title": "Code",
    "section": "",
    "text": "Code below"
  },
  {
    "objectID": "code_appendix.html#to-visualise-the-overlap-between-the-sentences-of-the-presidents",
    "href": "code_appendix.html#to-visualise-the-overlap-between-the-sentences-of-the-presidents",
    "title": "Code for Assignment 1",
    "section": "To visualise the overlap between the sentences of the presidents",
    "text": "To visualise the overlap between the sentences of the presidents\n\n# Convert the sentences to BoW representation\ndef bow(data):\n    # Extract the text column from the input data\n    text_data = data['sentence']\n\n    # Initialize a CountVectorizer for BOW representation with specified preprocessing steps\n    vectorizer = CountVectorizer(lowercase=True, token_pattern=r\"(?u)\\b\\w+\\b\", stop_words='english')\n\n    # Convert text data to BOW representation\n    bow_matrix = vectorizer.fit_transform(text_data)\n\n    # Convert BOW matrix to a DataFrame for better readability\n    bow_df = pd.DataFrame(bow_matrix.toarray(), columns=vectorizer.get_feature_names_out())\n\n    return bow_df\n\n# Convert the sentences to TF-IDF representation\ndef tf_idf(data):\n    # Extract sentences from the dataframe\n    sentences = data['sentence'].tolist()\n\n    # Initialize a TfidfVectorizer with specified preprocessing steps\n    tfidf_vectorizer = TfidfVectorizer(stop_words='english')\n\n    # Convert sentences to TF-IDF representation\n    tfidf_matrix = tfidf_vectorizer.fit_transform(sentences)\n\n    # Convert TF-IDF matrix to a DataFrame for better readability\n    tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=tfidf_vectorizer.get_feature_names_out())\n\n    return tfidf_df\n\n# Tokenize the input text data and pad the sequences to a fixed length.\ndef tokenize_text(text_data, labels, max_features=10000, maxlen=100):\n    # Initialize and fit the tokenizer on the text data\n    tokenizer = Tokenizer(num_words=max_features)\n    tokenizer.fit_on_texts(text_data)\n    sequences = tokenizer.texts_to_sequences(text_data)\n\n    # Filter out sequences with length 0\n    valid_indices = [i for i, s in enumerate(sequences) if len(s) &gt; 0]\n    valid_labels = [labels.iloc[i] for i in valid_indices]\n    valid_sequences = [sequences[i] for i in valid_indices]\n\n    # Pad sequences to the specified maximum length\n    padded_sequences = pad_sequences(valid_sequences, maxlen=maxlen)\n\n    return padded_sequences, valid_labels, tokenizer\n\n\n# Sample a subset of the data to alleviate memory issues\nsample_df = df_sentences.sample(frac=0.4, random_state=1)\n\n# Encoding sentences using Bag of Words (BOW)\nbow_encoded = bow(sample_df)\ny_bow = sample_df['president']\n\n# Dimensionality reduction using t-SNE\ntsne = TSNE(n_components=2, random_state=1)\nbow_tsne = tsne.fit_transform(bow_encoded)\n\n# Visualise the results\nplt.figure(figsize=(12, 8))\nfor president in sample_df['president'].unique():\n    indices = [i for i, label in enumerate(y_bow) if label == president]\n    plt.scatter(bow_tsne[indices, 0], bow_tsne[indices, 1], label=president, alpha=0.7)\n\nplt.title('Visualisation of Sentences using Bag of Words (BOW)')\nplt.legend()\nplt.show()\n\n\n# Encoding sentences using TF-IDF for the sample data\ntfidf_encoded_sample = tf_idf(sample_df)\n\n# Dimensionality reduction using t-SNE for the TF-IDF encoded data\ntfidf_tsne_sample = tsne.fit_transform(tfidf_encoded_sample)\n\n# Visualise the results for the TF-IDF encoded sample data\nplt.figure(figsize=(12, 8))\nfor president in sample_df['president'].unique():\n    indices = [i for i, label in enumerate(sample_df['president']) if label == president]\n    plt.scatter(tfidf_tsne_sample[indices, 0], tfidf_tsne_sample[indices, 1], label=president, alpha=0.7)\n\nplt.title('Visualisation of Sampled Sentences using TF-IDF')\nplt.legend()\nplt.show()\n\n\n# Encoding sentences using Tokenization with Padding for the sample data\nx_pad_sample, valid_labels_sample, _ = tokenize_text(sample_df['sentence'], sample_df['president'])\n\n# Dimensionality reduction using t-SNE for the Tokenized data\ntokenized_tsne_sample = tsne.fit_transform(x_pad_sample)\n\n# Visualise the results for the Tokenized sample data\nplt.figure(figsize=(12, 8))\nfor president in sample_df['president'].unique():\n    indices = [i for i, label in enumerate(valid_labels_sample) if label == president]\n    plt.scatter(tokenized_tsne_sample[indices, 0], tokenized_tsne_sample[indices, 1], label=president, alpha=0.7)\n\nplt.title('Visualisation of Sampled Sentences using Tokenization with Padding')\nplt.legend()\nplt.show()"
  },
  {
    "objectID": "code_appendix.html#bag-of-words",
    "href": "code_appendix.html#bag-of-words",
    "title": "Code for Assignment 1",
    "section": "Bag of Words",
    "text": "Bag of Words\n\nx = bow(sentence_data)\ny = sentence_data['president']\n\n# Prepare data\ndata = prepare_data(prepare_dataset, x, y)\nX_train, y_train, X_val, y_val, X_test, y_test, input_dim, num_classes = data['X_train'], data['y_train'], data['X_val'], data['y_val'], data['X_test'], data['y_test'], data['input_dim'], data['num_classes']\n\n# Scaled data for SVM\nscaler = cuml.preprocessing.StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_val_scaled = scaler.transform(X_val)\nX_test_scaled = scaler.transform(X_test)\n\ny_train_int = np.argmax(y_train, axis=1)\ny_val_int = np.argmax(y_val, axis=1)\ny_test_int = np.argmax(y_test, axis=1)\n\n\nNeural network\n\nwith tf.device('/device:GPU:0'):\n nn_bow_tune = hyperparameter_search_nn(seed,\n                                       train_neural_network,\n                                       X_train, y_train,\n                                       X_val, y_val,\n                                       X_test, y_test)\n\n# Saving the model parameters\nsave_model_params(nn_bow_tune, os.path.join(save_directory, 'nn_bow_tune'))\n\n\n# Load the tuned output\nnn_bow_tuned = load_model_params(os.path.join(save_directory, 'nn_bow_tune'))\n\nvisualize_and_verify_results(nn_bow_tuned, X_val, y_val, X_test, y_test, 'nn', class_labels=None)\n\n\nsave_path = save_directory2 + \"/bow_nn_results\"\nsave_results(bow_nn_results, save_path) \nloaded_results = load_results(save_path)\n\n\ndisplay_accuracy_plot(loaded_results)\n\n\ndisplay_table_nn(loaded_results)\n\n\ndisplay_table_others(loaded_results)\n\n\ndisplay_confusion_matrix(loaded_results)\n\n\ndisplay_test_classification_report(loaded_results)\n\n\n\nSupport Vector Machine\n\nsvm_results_bow = train_evaluate_svm(X_train, y_train, X_val, y_val, X_test, y_test, seed)\n\n# Save the model parameters\nsave_model_params(svm_results_bow, os.path.join(save_directory, 'svm_bow_results'))\n\n# Load the saved model parameters\nsvm_results_bow = load_model_params(os.path.join(save_directory, 'svm_bow_results'))\n\nresults = visualize_and_verify_results(svm_results_bow, X_val_scaled, y_val_int, X_test_scaled, y_test_int, 'svm', class_labels=None)\n\n# Save and load the results for later\nsave_path = save_directory2 + \"/bow_svm_results\"\nsave_results(results, save_path)\nloaded_results = load_results(save_path)\n\n\ndisplay_accuracy_plot(loaded_results)\n\n\ndisplay_table_others(loaded_results)\n\n\ndisplay_confusion_matrix(loaded_results)\n\n\ndisplay_test_classification_report(loaded_results)\n\n\n\nNaive Bayes\n\nnb_results_bow = train_evaluate_nb(X_train, y_train, X_val, y_val, X_test, y_test)\n\n# Save the model parameters\nsave_model_params(nb_results_bow, os.path.join(save_directory, 'nb_bow_results'))\n\n# Load the saved model parameters\nnb_results_bow = load_model_params(os.path.join(save_directory, 'nb_bow_results'))\n\nresults = visualize_and_verify_results(nb_results_bow, X_val, y_val, X_test, y_test, 'nb', class_labels=None)\n\n# Save and load the results for later\nsave_path = save_directory2 + \"/bow_nb_results\"\n# save_results(results, save_path)\nloaded_results = load_results(save_path)\n\n\ndisplay_accuracy_plot(loaded_results)\n\n\ndisplay_table_others(loaded_results)\n\n\ndisplay_confusion_matrix(loaded_results)\n\n\ndisplay_test_classification_report(loaded_results)"
  },
  {
    "objectID": "code_appendix.html#tf-idf",
    "href": "code_appendix.html#tf-idf",
    "title": "Code for Assignment 1",
    "section": "TF-IDF",
    "text": "TF-IDF\n\n# import idf data\nx = tf_idf(sentence_data)\ny = sentence_data['president']\n\n# Prepare the data\ndata = prepare_data(prepare_dataset, x, y)\nX_train, y_train, X_val, y_val, X_test, y_test, input_dim, num_classes = data['X_train'], data['y_train'], data['X_val'], data['y_val'], data['X_test'], data['y_test'], data['input_dim'], data['num_classes']\n\n\n# Scaled data for SVM\nscaler = cuml.preprocessing.StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_val_scaled = scaler.transform(X_val)\nX_test_scaled = scaler.transform(X_test)\n\ny_train_int = np.argmax(y_train, axis=1)\ny_val_int = np.argmax(y_val, axis=1)\ny_test_int = np.argmax(y_test, axis=1)\n\n\nNeural network\n\nwith tf.device('/device:GPU:0'):\n  nn_tf_tune = hyperparameter_search_nn(seed,\n                                        train_neural_network,\n                                        X_train, y_train,\n                                        X_val, y_val,\n                                        X_test, y_test)\n\n# Saving the model parameters\nsave_model_params(nn_tf_tune, os.path.join(save_directory, 'nn_tf_results'))\n\n# Load the tuned output\nnn_tf_tune = load_model_params(os.path.join(save_directory, 'nn_tf_results'))\n\nresults = visualize_and_verify_results(nn_tf_tune, X_val, y_val, X_test, y_test, 'nn', class_labels=None)\n\n# Save and load the results for later\nsave_path = save_directory2 + \"/tfidf_nn_results\"\nsave_results(results, save_path)\nloaded_results = load_results(save_path)\n\n\ndisplay_accuracy_plot(loaded_results)\n\n\ndisplay_table_nn(loaded_results)\n\n\ndisplay_table_others(loaded_results)\n\n\ndisplay_confusion_matrix(loaded_results)\n\n\ndisplay_test_classification_report(loaded_results)\n\n\n\nSupport Vector Machine\n\nsvm_results_tf = train_evaluate_svm(X_train, y_train, X_val, y_val, X_test, y_test, seed)\n\nsave_model_params(svm_results_tf, os.path.join(save_directory, 'svm_tf_results'))\n\nsvm_results_tf = load_model_params(os.path.join(save_directory, 'svm_tf_results'))\n\nresults = visualize_and_verify_results(svm_results_tf, X_val_scaled, y_val_int, X_test_scaled, y_test_int, 'svm', class_labels=None)\n\n# Save and load the results for later\nsave_path = save_directory2 + \"/tfidf_svm_results\"\nsave_results(results, save_path)\nloaded_results = load_results(save_path)\n\n\ndisplay_accuracy_plot(loaded_results)\n\n\ndisplay_table_others(loaded_results)\n\n\ndisplay_confusion_matrix(loaded_results)\n\n\ndisplay_test_classification_report(loaded_results)\n\n\n\nNaive Bayes\n\nnb_results_tf = train_evaluate_nb(X_train, y_train, X_val, y_val, X_test, y_test)\n\nsave_model_params(nb_results_tf, os.path.join(save_directory, 'nb_tf_results'))\n\nnb_results_tf = load_model_params(os.path.join(save_directory, 'nb_tf_results'))\n\nresults = visualize_and_verify_results(nb_results_tf, X_val, y_val, X_test, y_test, 'nb', class_labels=None)\n\n# Save and load the results for later\nsave_path = save_directory2 + \"/tfidf_nb_results\"\nsave_results(results, save_path)\nloaded_results = load_results(save_path)\n\n\ndisplay_accuracy_plot(loaded_results)\n\n\ndisplay_table_others(loaded_results)\n\n\ndisplay_confusion_matrix(loaded_results)\n\n\ndisplay_test_classification_report(loaded_results)"
  },
  {
    "objectID": "code_appendix.html#token-embeddings",
    "href": "code_appendix.html#token-embeddings",
    "title": "Code for Assignment 1",
    "section": "Token Embeddings",
    "text": "Token Embeddings\n\nx, y, _ = tokenize_text(sentence_data['sentence'], sentence_data['president'])\nx_df = pd.DataFrame(x)\n\ndata = prepare_data(prepare_dataset, x_df, y)\nX_train, y_train, X_val, y_val, X_test, y_test, input_dim, num_classes = data['X_train'], data['y_train'], data['X_val'], data['y_val'], data['X_test'], data['y_test'], data['input_dim'], data['num_classes']\n\n# Scaled data for SVM\nscaler = cuml.preprocessing.StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_val_scaled = scaler.transform(X_val)\nX_test_scaled = scaler.transform(X_test)\n\ny_train_int = np.argmax(y_train, axis=1)\ny_val_int = np.argmax(y_val, axis=1)\ny_test_int = np.argmax(y_test, axis=1)\n\n\nNeural network\n\nwith tf.device('/device:GPU:0'):\n  nn_emb_tune = hyperparameter_search_nn(seed,\n                                        train_neural_network,\n                                        X_train, y_train,\n                                        X_val, y_val,\n                                        X_test, y_test)\n\n# Saving the model parameters\nsave_model_params(nn_emb_tune, os.path.join(save_directory, 'nn_emb_results'))\n\n# Load the saved model parameters\nnn_emb_tune = load_model_params(os.path.join(save_directory, 'nn_emb_results'))\n\nresults = visualize_and_verify_results(nn_emb_tune, X_val, y_val, X_test, y_test, 'nn', class_labels=None)\n\n# Save and load the results for later\nsave_path = save_directory2 + \"/emb_nn_results\"\nsave_results(results, save_path)\nloaded_results = load_results(save_path)\n\n\ndisplay_accuracy_plot(loaded_results)\n\n\ndisplay_table_nn(loaded_results)\n\n\ndisplay_table_others(loaded_results)\n\n\ndisplay_confusion_matrix(loaded_results)\n\n\ndisplay_test_classification_report(loaded_results)\n\n\n\nSupport Vector Machine\n\nsvm_results_emb = train_evaluate_svm(X_train, y_train, X_val, y_val, X_test, y_test, seed)\n\nsave_model_params(svm_results_emb, os.path.join(save_directory, 'svm_emb_results'))\n\nsvm_results_emb = load_model_params(os.path.join(save_directory, 'svm_emb_results'))\n\nresults = visualize_and_verify_results(svm_results_emb, X_val_scaled, y_val_int, X_test_scaled, y_test_int, 'svm', class_labels=None)\n\n# Save and load the results for later\nsave_path = save_directory2 + \"/emb_svm_results\"\nsave_results(results, save_path)\nloaded_results = load_results(save_path)\n\n\ndisplay_accuracy_plot(loaded_results)\n\n\ndisplay_table_others(loaded_results)\n\n\ndisplay_confusion_matrix(loaded_results)\n\n\ndisplay_test_classification_report(loaded_results)\n\n\n\nNaive Bayes\n\nnb_results_emb = train_evaluate_nb(X_train, y_train, X_val, y_val, X_test, y_test)\n\nsave_model_params(nb_results_emb, os.path.join(save_directory, 'nb_emb_results'))\n\nnb_results_emb = load_model_params(os.path.join(save_directory, 'nb_emb_results'))\n\nresults = visualize_and_verify_results(nb_results_emb, X_val, y_val, X_test, y_test, 'nb', class_labels=None)\n\n# Save and load the results for later\nsave_path = save_directory2 + \"/emb_nb_results\"\nsave_results(results, save_path)\nloaded_results = load_results(save_path)\n\n\ndisplay_accuracy_plot(loaded_results)\n\n\ndisplay_table_others(loaded_results)\n\n\ndisplay_confusion_matrix(loaded_results)\n\n\ndisplay_test_classification_report(loaded_results)"
  },
  {
    "objectID": "code_appendix.html#bert-embeddings-with-pre-trained-classifier",
    "href": "code_appendix.html#bert-embeddings-with-pre-trained-classifier",
    "title": "Code for Assignment 1",
    "section": "BERT Embeddings with pre-trained classifier",
    "text": "BERT Embeddings with pre-trained classifier\nThe code for this section was adapted from the following source: Google Tensorflow BERT tutorial\n\ny = sentence_data['president'].values\n\n# Split dataset into train and a temporary set (70% - 30% split)\ntrain_df, temp_df = train_test_split(sentence_data, test_size=0.3, stratify=y, random_state=seed)\n\n# Split the temporary set into validation and test sets (50% - 50% split of the 30%)\nval_df, test_df = train_test_split(temp_df, test_size=0.5, stratify=temp_df['president'], random_state=seed)\n\n# Assuming you have a list of unique presidents\nunique_presidents = sentence_data['president'].unique()\n\n# Create a mapping of president names to integer labels\nlabel_map = {name: idx for idx, name in enumerate(unique_presidents)}\n\n# Integer-encode the labels in the dataframes\ntrain_df['president'] = train_df['president'].map(label_map)\nval_df['president'] = val_df['president'].map(label_map)\ntest_df['president'] = test_df['president'].map(label_map)\n\n# Convert pandas DataFrames to TensorFlow datasets\ndef df_to_tfdata(df, shuffle=True, batch_size=32):\n    ds = tf.data.Dataset.from_tensor_slices((df[\"sentence\"].values, df[\"president\"].values))\n    if shuffle:\n        ds = ds.shuffle(buffer_size=len(df))\n    ds = ds.batch(batch_size)\n    ds = ds.cache().prefetch(buffer_size=tf.data.AUTOTUNE)\n    return ds\n\ntrain_ds = df_to_tfdata(train_df)\nval_ds = df_to_tfdata(val_df, shuffle=False)\ntest_ds = df_to_tfdata(test_df, shuffle=False)\n\n\n# Load the saved model\nimport tensorflow_text as text\nsaved_model_path = \"saved_files/jared_sentences_bert\"\nbert_model = tf.saved_model.load(saved_model_path)\n\n# Define a function to evaluate the model\ndef evaluate_model(model, dataset):\n    predictions = []\n    true_labels = []\n\n    for inputs, labels in dataset:\n        logits = model(inputs, training=False)  # Forward pass\n        predicted_labels = np.argmax(logits, axis=1)\n        true_labels.extend(labels.numpy())\n        predictions.extend(predicted_labels)\n\n    return true_labels, predictions\n\n\ndef save_output(train_true, train_pred, val_true, val_pred, test_true, test_pred, filename):\n    with open(filename, 'wb') as f:\n        pickle.dump((train_true, train_pred, val_true, val_pred, test_true, test_pred), f)\n\ndef load_output(filename):\n    with open(filename, 'rb') as f:\n        train_true, train_pred, val_true, val_pred, test_true, test_pred = pickle.load(f)\n    return train_true, train_pred, val_true, val_pred, test_true, test_pred\n\n\n# Evaluate the model on the training, validation, and test datasets\ntrain_true, train_pred = evaluate_model(bert_model, train_ds)\nval_true, val_pred = evaluate_model(bert_model, val_ds)\ntest_true, test_pred = evaluate_model(bert_model, test_ds)\n\n# Save the outputs\nsave_output(train_true, train_pred, val_true, val_pred, test_true, test_pred, 'bert_evaluation.pkl')\n\n# Load the saved values\ntrain_true, train_pred, val_true, val_pred, test_true, test_pred = load_output('bert_evaluation.pkl')\n\n\n# Display the saved image\ndisplay(Image(filename='saved_files/train_val_curves_bert.png'))\n\n\ndef extract_metrics(y_true, y_pred):\n    accuracy = np.mean(np.array(y_true) == np.array(y_pred))\n    \n    precision = precision_score(y_true, y_pred, average='macro', zero_division=1)\n    recall = recall_score(y_true, y_pred, average='macro', zero_division=1)\n    f1 = f1_score(y_true, y_pred, average='macro', zero_division=1)\n    \n    metrics = {\n        'accuracy': accuracy,\n        'precision': precision,\n        'recall': recall,\n        'f1_score': f1\n    }\n    return metrics\n\ntrain_metrics = extract_metrics(train_true, train_pred)\nval_metrics = extract_metrics(val_true, val_pred)\ntest_metrics = extract_metrics(test_true, test_pred)\n\n# Create a DataFrame for a neat table\ndf = pd.DataFrame([train_metrics, val_metrics, test_metrics], \n                  index=['Training', 'Validation', 'Test'])\n\n# Round the values to 3 decimal places for better presentation\ndf = df.round(3)\n\n# Display the table\ndisplay(df)\n\n\n# Function to plot confusion matrix\ndef plot_confusion_matrix(true, pred, title):\n    matrix = confusion_matrix(true, pred)\n    plt.figure(figsize=(10, 7))\n    sns.heatmap(matrix, annot=True, fmt=\"d\", cmap=\"Blues\",\n                xticklabels=set(true), yticklabels=set(true))\n    plt.title(title)\n    plt.ylabel('True Label')\n    plt.xlabel('Predicted Label')\n    plt.show()\n\n# Plot confusion matrices\nplot_confusion_matrix(test_true, test_pred, title=\"Test Data Confusion Matrix\")"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site"
  },
  {
    "objectID": "intro_lit_review.html",
    "href": "intro_lit_review.html",
    "title": "Introduction and Literature Review",
    "section": "",
    "text": "In the socio-political landscape, the manner and content of how leaders communicate provides critical insight into their governance style, priorities, and ideology. The State of the Nation Address (SONA) serves as an essential touchstone in South Africa’s political calendar, where the sitting president not only provides an annual report on the nation’s status but also sets the tone for policy directions and government intent for the subsequent year. Delivered at the commencement of a joint sitting of Parliament, particularly in election years, this address receives heightened scrutiny, given that it occurs twice: pre- and post-election (African 2023).\nNatural Language Processing (NLP) has been increasingly leveraged in the domain of political science to uncover patterns, biases, and ideologies in the speeches and writings of political leaders. Recent advancements in machine learning and NLP tools have enabled more refined text analysis, going beyond mere word frequency to semantic content and stylistic nuances. Researchers such as Katre (2019) and Glavas, Nanni and Ponzetto (2019) have demonstrated the efficacy of using NLP to categorise and analyse political speeches. This raises the question: Can we discern, based purely on textual analysis, which South African president might have uttered a particular sentence during their SONA speech? In other words - can we predict the author of a sentence based on the content and style of the sentence?\nHowever, while there is an abundance of literature on NLP applications in sentiment analysis and topic modelling, its application to discern between specific authors or speakers, especially in the South African political sphere, remains largely unexplored (although there has been some development with regards to creating text resources for South African languages (Eiselen and Puttkammer 2014)). This gap is particularly noticeable when considering the unique linguistic, cultural, and political landscape of South Africa. The challenges lie not just in the variety of linguistic styles but also in the depth and breadth of topics covered, as well as the personal idiosyncrasies of each president (within a single speech as well as over time).\nGiven the above context, this paper aims to predict which of the South African presidents between 1994 and 2022 might have said a specific sentence during their SONA address. It leverages various text transformation techniques, such as Bag of Words (BoW), Term Frequency-Inverse Document Frequency (tf-idf), and text embeddings (a very simple embedding as well as BERT). Subsequent application of machine learning models, including a feed-forward neural net, Support Vector Machine (SVM), Naive Bayes, and a BERT classification model, offers a comparative lens to evaluate the efficacy of each approach.\n\n\n\n\nReferences\n\nAfrican, The South. 2023. “SONA 2023: Everything You Need to Know about State of the Nation Address.” 2023. https://www.thesouthafrican.com/news/sona-2023-everything-you-need-know-about-state-ofnation-address-sona-2023-breaking-news-9-february-2023/.\n\n\nEiselen, Roald, and Martin J. Puttkammer. 2014. “Developing Text Resources for Ten South African Languages.” In Proceedings of the Ninth International Conference on Language Resources and Evaluation (LREC’14), edited by Nicoletta Calzolari, Khalid Choukri, Thierry Declerck, Hrafn Loftsson, Bente Maegaard, Joseph Mariani, Asuncion Moreno, Jan Odijk, and Stelios Piperidis. Reykjavik, Iceland: European Language Resources Association (ELRA).\n\n\nGlavaš, Goran, Federico Nanni, and Simone Paolo Ponzetto. 2019. “Computational Analysis of Political Texts: Bridging Research Efforts Across Communities.” In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics: Tutorial Abstracts, 18–23. Florence, Italy: Association for Computational Linguistics. https://doi.org/10.18653/v1/P19-4004.\n\n\nKatre, Paritosh. 2019. “NLP Based Text Analytics and Visualization of Political Speeches.” International Journal of Recent Technology and Engineering 8 (September): 8574–79. https://doi.org/10.35940/ijrte.C6503.098319."
  },
  {
    "objectID": "eda.html",
    "href": "eda.html",
    "title": "Exploratory Data Analysis",
    "section": "",
    "text": "The dataset (Republic of South Africa 2023) was comprised of a series of text files of the State of the Nation Addresses (SONA) from 1994 through 2022. Each speech’s content was subsequently ingested, omitting the initial lines. These speeches were then collated into a structured format for more convenient access and manipulation.\nSubsequently, essential metadata, including the year of the address and the name of the delivering president, were gleaned. Ater that, the removal of URLs, HTML character codes, and newline characters was performed. Additionally, the date of each address was extracted and appropriately formatted.\nTo achieve the project’s objectives, each speech was dissected into its individual sentences. This granular breakdown facilitated the mapping of each sentence to its originating president. The finalised structured dataset comprises individual sentences paired with their respective presidents. This dataset was also saved as a csv file for future use.\nFor the model building, the data was prepared by create a 70-15-15 train-validation-test split, with the same seed being used for each method to ensure fair comparisons.\n\n\n\n\n\n\n\nThe bar plot above illustrates the total number of speeches given by each president. Mbeki and Zuma had most speeches in the dataset, with 10 each. This means that there’s a substantial amount of data available for them, which could be advantageous when discerning their linguistic patterns, given that there is not a significant overlap in the sentences of the two presidents. Motlanthe and de Klerk only had one speech each, which may be an issue, due to an imbalance in the data, which may bias the model output later. To explore this further, the number of sentences per president is examined.\n\n\n\n\n\n\n\n\nThe plot above gives a breakdown of the number of sentences spoken by each president. Zuma stands out with the most sentences, further underscoring his prominence in the dataset. Notably, while Mbeki gave three more speeches than Ramaphosa, their sentence count is nearly the same, implying that Ramaphosa’s speeches might be more verbose or detailed. This data provides a deeper understanding of the granularity of each president’s contribution and reaffirms the potential data imbalance to be addressed in model development, especially when considering the fact that de Klerk and Motlanthe have less than 300 sentences each, while the others have well over 1500.\n\n\n\n\n\n\n\n\nThis plot unveils the average sentence length, in words, for each president. A striking observation is that Zuma, despite having the most sentences and speeches, has a relatively concise average sentence length. Conversely, Mbeki and Motlanthe have longer average sentence lengths, with Mbeki being the only president that had over 30 words per sentence, on average. This metric offers insights into the verbosity and style of each president, which can be a useful feature when discerning speech patterns in model building.\n\n\n\n\n\n\n\n\nThe word clouds above offer a visually compelling representation of the most frequently used words by each president. The size of each word in the cloud corresponds to its frequency in the speeches. All the presidents had “will” as their most prominent word and referred to the country many times while speaking (highlighted by the use of the words “south” and “africa”/“african”). Motlanthe seemed to focus more on the economy and public image with the use of words such as “national”, “public” and “government”, whereas Mandela seemed to focus more on the people with the use of words such as “people” and “us”. de Klerk focused more on the constitution and forming alliances during a transitional period, and Zuma focused more on work and the development. These word clouds provide a snapshot of the focal points and themes of each president’s speeches. Distinctive words or terms can be potential features when building predictive models. The words from the wordclouds can also be seen in the bar plots below.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nInstead of only looking at single word frequency, bigrams can also be used to find the most common two-word phrases. The bigrams above elucidate the distinctive linguistic patterns and thematic foci of each president, presenting opportunities for differentiation. For instance, President Mandela’s frequently used bigrams, such as “South Africans” and “national unity,” reflect his emphasis on nation-building and reconciliation during his tenure. In contrast, President Zuma’s bigrams like “economic growth” suggest a policy-driven discourse concentrated on economic dynamics. However, there are potential pitfalls. Overlapping or common bigrams across presidents, such as generic terms or phrases prevalent in political discourse, could introduce ambiguity, potentially hindering the model’s precision. Additionally, while President Ramaphosa’s bigrams like “South Africa” are distinctly frequent, they are not uniquely attributable to him, as such phrases are likely universal across South African presidencies.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExpanding on the analysis of linguistic markers, trigrams offer insights into the most recurrent three-word sequences employed by each president. The trigram outputs above further refine our understanding of the unique verbal choices and thematic concerns of each leader. For instance, President Mandela’s recurrent trigrams, such as “trade union movement”, underscore his consistent focus on the working class of South Africa. Meanwhile, President Zuma’s trigrams, such as “expaned public works” indicate a focus on the public sector as a whole. Conversely, the presence of generic or universally applicable trigrams, such as “state nation address”, might pose challenges. These broadly-used trigrams, inherent to political addresses across presidencies, might dilute the distinctive features of individual presidents, complicating the model’s task. Moreover, trigrams like “south africa will” from President Ramaphosa, although salient, are emblematic of speeches common to all presidents, making them less distinguishing. Thus, while trigrams can accentuate the nuances of each president’s discourse, the model would benefit from discerning the balance between distinctiveness and generic trigram usage.\n\n\n\n\n\n2023-10-18 12:01:40.925090: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\nTo enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n\n\n\n\n\n\n\n\n\n\nThe Bag-of-Words (BoW) visualisation above reveals a pronounced central cluster with substantial overlap across presidential sentences, indicating pervasive shared linguistic elements. This convergence towards common terms suggests that the BoW representation predominantly captures universal themes and terminologies characteristic of political discourse. Such patterns, while illuminating shared linguistic tendencies, underscore potential challenges in predictive modeling, with the BoW approach possibly lacking the granularity to detect distinctive linguistic markers for each president.\n\n\n\n\n\n\n\n\nUsing the TF-IDF representation, the visualization depicts a dominant central cluster, reaffirming the presence of overlapping linguistic constructs across presidential discourses. Unlike the BoW representation, the TF-IDF visualization lacks discernible smaller clusters, and data points appear more dispersed. This dispersion underscores the varied thematic undertones each president might have explored, but the pronounced overlap in the central region suggests that these thematic variations are not sufficiently distinct in the TF-IDF space to provide clear demarcations. The observed patterns emphasize the challenges inherent in solely relying on TF-IDF for capturing the unique linguistic nuances of each president.\n\n\n\n\n\n\n\n\nUtilising tokenization with padding, the resultant visualization presents multiple clusters, indicating the method’s ability to recognize shared linguistic constructs or thematic groupings within the dataset. Notably, the significant intermingling of presidents within these clusters underscores the shared nature of discourse patterns across different presidencies. The absence of a dominant central cluster, a divergence from the BoW and TF-IDF representations, alludes to a more nuanced and diverse sentence representation in the embedding space, potentially attributed to the emphasis on sentence structure inherent in the tokenization method."
  },
  {
    "objectID": "eda.html#number-of-speeches-per-president",
    "href": "eda.html#number-of-speeches-per-president",
    "title": "Exploratory Data Analysis",
    "section": "",
    "text": "The bar plot above illustrates the total number of speeches given by each president. Mbeki and Zuma had most speeches in the dataset, with 10 each. This means that there’s a substantial amount of data available for them, which could be advantageous when discerning their linguistic patterns, given that there is not a significant overlap in the sentences of the two presidents. Motlanthe and de Klerk only had one speech each, which may be an issue, due to an imbalance in the data, which may bias the model output later. To explore this further, the number of sentences per president is examined."
  },
  {
    "objectID": "eda.html#number-of-sentences-per-president",
    "href": "eda.html#number-of-sentences-per-president",
    "title": "Exploratory Data Analysis",
    "section": "",
    "text": "The plot above gives a breakdown of the number of sentences spoken by each president. Zuma stands out with the most sentences, further underscoring his prominence in the dataset. Notably, while Mbeki gave three more speeches than Ramaphosa, their sentence count is nearly the same, implying that Ramaphosa’s speeches might be more verbose or detailed. This data provides a deeper understanding of the granularity of each president’s contribution and reaffirms the potential data imbalance to be addressed in model development, especially when considering the fact that de Klerk and Motlanthe have less than 300 sentences each, while the others have well over 1500."
  },
  {
    "objectID": "eda.html#average-sentence-length-per-president",
    "href": "eda.html#average-sentence-length-per-president",
    "title": "Exploratory Data Analysis",
    "section": "",
    "text": "This plot unveils the average sentence length, in words, for each president. A striking observation is that Zuma, despite having the most sentences and speeches, has a relatively concise average sentence length. Conversely, Mbeki and Motlanthe have longer average sentence lengths, with Mbeki being the only president that had over 30 words per sentence, on average. This metric offers insights into the verbosity and style of each president, which can be a useful feature when discerning speech patterns in model building."
  },
  {
    "objectID": "eda.html#word-clouds-for-each-president",
    "href": "eda.html#word-clouds-for-each-president",
    "title": "Exploratory Data Analysis",
    "section": "",
    "text": "The word clouds above offer a visually compelling representation of the most frequently used words by each president. The size of each word in the cloud corresponds to its frequency in the speeches. All the presidents had “will” as their most prominent word and referred to the country many times while speaking (highlighted by the use of the words “south” and “africa”/“african”). Motlanthe seemed to focus more on the economy and public image with the use of words such as “national”, “public” and “government”, whereas Mandela seemed to focus more on the people with the use of words such as “people” and “us”. de Klerk focused more on the constitution and forming alliances during a transitional period, and Zuma focused more on work and the development. These word clouds provide a snapshot of the focal points and themes of each president’s speeches. Distinctive words or terms can be potential features when building predictive models. The words from the wordclouds can also be seen in the bar plots below."
  },
  {
    "objectID": "eda.html#n-gram-frequency-distributions-for-each-president",
    "href": "eda.html#n-gram-frequency-distributions-for-each-president",
    "title": "Exploratory Data Analysis",
    "section": "",
    "text": "Instead of only looking at single word frequency, bigrams can also be used to find the most common two-word phrases. The bigrams above elucidate the distinctive linguistic patterns and thematic foci of each president, presenting opportunities for differentiation. For instance, President Mandela’s frequently used bigrams, such as “South Africans” and “national unity,” reflect his emphasis on nation-building and reconciliation during his tenure. In contrast, President Zuma’s bigrams like “economic growth” suggest a policy-driven discourse concentrated on economic dynamics. However, there are potential pitfalls. Overlapping or common bigrams across presidents, such as generic terms or phrases prevalent in political discourse, could introduce ambiguity, potentially hindering the model’s precision. Additionally, while President Ramaphosa’s bigrams like “South Africa” are distinctly frequent, they are not uniquely attributable to him, as such phrases are likely universal across South African presidencies."
  },
  {
    "objectID": "eda.html#trigrams",
    "href": "eda.html#trigrams",
    "title": "Exploratory Data Analysis",
    "section": "",
    "text": "Expanding on the analysis of linguistic markers, trigrams offer insights into the most recurrent three-word sequences employed by each president. The trigram outputs above further refine our understanding of the unique verbal choices and thematic concerns of each leader. For instance, President Mandela’s recurrent trigrams, such as “trade union movement”, underscore his consistent focus on the working class of South Africa. Meanwhile, President Zuma’s trigrams, such as “expaned public works” indicate a focus on the public sector as a whole. Conversely, the presence of generic or universally applicable trigrams, such as “state nation address”, might pose challenges. These broadly-used trigrams, inherent to political addresses across presidencies, might dilute the distinctive features of individual presidents, complicating the model’s task. Moreover, trigrams like “south africa will” from President Ramaphosa, although salient, are emblematic of speeches common to all presidents, making them less distinguishing. Thus, while trigrams can accentuate the nuances of each president’s discourse, the model would benefit from discerning the balance between distinctiveness and generic trigram usage."
  },
  {
    "objectID": "eda.html#sentence-similarity-between-presidents",
    "href": "eda.html#sentence-similarity-between-presidents",
    "title": "Exploratory Data Analysis",
    "section": "",
    "text": "2023-10-18 12:01:40.925090: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\nTo enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags."
  },
  {
    "objectID": "eda.html#bag-of-words-bow-representation",
    "href": "eda.html#bag-of-words-bow-representation",
    "title": "Exploratory Data Analysis",
    "section": "",
    "text": "The Bag-of-Words (BoW) visualisation above reveals a pronounced central cluster with substantial overlap across presidential sentences, indicating pervasive shared linguistic elements. This convergence towards common terms suggests that the BoW representation predominantly captures universal themes and terminologies characteristic of political discourse. Such patterns, while illuminating shared linguistic tendencies, underscore potential challenges in predictive modeling, with the BoW approach possibly lacking the granularity to detect distinctive linguistic markers for each president."
  },
  {
    "objectID": "eda.html#tf-idf-representation",
    "href": "eda.html#tf-idf-representation",
    "title": "Exploratory Data Analysis",
    "section": "",
    "text": "Using the TF-IDF representation, the visualization depicts a dominant central cluster, reaffirming the presence of overlapping linguistic constructs across presidential discourses. Unlike the BoW representation, the TF-IDF visualization lacks discernible smaller clusters, and data points appear more dispersed. This dispersion underscores the varied thematic undertones each president might have explored, but the pronounced overlap in the central region suggests that these thematic variations are not sufficiently distinct in the TF-IDF space to provide clear demarcations. The observed patterns emphasize the challenges inherent in solely relying on TF-IDF for capturing the unique linguistic nuances of each president."
  },
  {
    "objectID": "eda.html#tokenization-with-padding-representation",
    "href": "eda.html#tokenization-with-padding-representation",
    "title": "Exploratory Data Analysis",
    "section": "",
    "text": "Utilising tokenization with padding, the resultant visualization presents multiple clusters, indicating the method’s ability to recognize shared linguistic constructs or thematic groupings within the dataset. Notably, the significant intermingling of presidents within these clusters underscores the shared nature of discourse patterns across different presidencies. The absence of a dominant central cluster, a divergence from the BoW and TF-IDF representations, alludes to a more nuanced and diverse sentence representation in the embedding space, potentially attributed to the emphasis on sentence structure inherent in the tokenization method."
  },
  {
    "objectID": "full_write_up.html",
    "href": "full_write_up.html",
    "title": "Assignment 1 Full Writeup",
    "section": "",
    "text": "In the socio-political landscape, the manner and content of how leaders communicate provides critical insight into their governance style, priorities, and ideology. The State of the Nation Address (SONA) serves as an essential touchstone in South Africa’s political calendar, where the sitting president not only provides an annual report on the nation’s status but also sets the tone for policy directions and government intent for the subsequent year. Delivered at the commencement of a joint sitting of Parliament, particularly in election years, this address receives heightened scrutiny, given that it occurs twice: pre- and post-election (African 2023).\nNatural Language Processing (NLP) has been increasingly leveraged in the domain of political science to uncover patterns, biases, and ideologies in the speeches and writings of political leaders. Recent advancements in machine learning and NLP tools have enabled more refined text analysis, going beyond mere word frequency to semantic content and stylistic nuances. Researchers such as Katre (2019) and Glavas, Nanni and Ponzetto (2019) have demonstrated the efficacy of using NLP to categorise and analyse political speeches. This raises the question: Can we discern, based purely on textual analysis, which South African president might have uttered a particular sentence during their SONA speech? In other words - can we predict the author of a sentence based on the content and style of the sentence?\nHowever, while there is an abundance of literature on NLP applications in sentiment analysis and topic modelling, its application to discern between specific authors or speakers, especially in the South African political sphere, remains largely unexplored (although there has been some development with regards to creating text resources for South African languages (Eiselen and Puttkammer 2014)). This gap is particularly noticeable when considering the unique linguistic, cultural, and political landscape of South Africa. The challenges lie not just in the variety of linguistic styles but also in the depth and breadth of topics covered, as well as the personal idiosyncrasies of each president (within a single speech as well as over time).\nGiven the above context, this paper aims to predict which of the South African presidents between 1994 and 2022 might have said a specific sentence during their SONA address. It leverages various text transformation techniques, such as Bag of Words (BoW), Term Frequency-Inverse Document Frequency (tf-idf), and text embeddings (a very simple embedding as well as BERT). Subsequent application of machine learning models, including a feed-forward neural net, Support Vector Machine (SVM), Naive Bayes, and a BERT classification model, offers a comparative lens to evaluate the efficacy of each approach."
  },
  {
    "objectID": "full_write_up.html#number-of-speeches-per-president",
    "href": "full_write_up.html#number-of-speeches-per-president",
    "title": "Assignment 1 Full Writeup",
    "section": "Number of speeches per president",
    "text": "Number of speeches per president\n\n\n\n\n\nThe bar plot above illustrates the total number of speeches given by each president. Mbeki and Zuma had most speeches in the dataset, with 10 each. This means that there’s a substantial amount of data available for them, which could be advantageous when discerning their linguistic patterns, given that there is not a significant overlap in the sentences of the two presidents. Motlanthe and de Klerk only had one speech each, which may be an issue, due to an imbalance in the data, which may bias the model output later. To explore this further, the number of sentences per president is examined."
  },
  {
    "objectID": "full_write_up.html#number-of-sentences-per-president",
    "href": "full_write_up.html#number-of-sentences-per-president",
    "title": "Assignment 1 Full Writeup",
    "section": "Number of sentences per president",
    "text": "Number of sentences per president\n\n\n\n\n\nThe plot above gives a breakdown of the number of sentences spoken by each president. Zuma stands out with the most sentences, further underscoring his prominence in the dataset. Notably, while Mbeki gave three more speeches than Ramaphosa, their sentence count is nearly the same, implying that Ramaphosa’s speeches might be more verbose or detailed. This data provides a deeper understanding of the granularity of each president’s contribution and reaffirms the potential data imbalance to be addressed in model development, especially when considering the fact that de Klerk and Motlanthe have less than 300 sentences each, while the others have well over 1500."
  },
  {
    "objectID": "full_write_up.html#average-sentence-length-per-president",
    "href": "full_write_up.html#average-sentence-length-per-president",
    "title": "Assignment 1 Full Writeup",
    "section": "Average sentence length per president",
    "text": "Average sentence length per president\n\n\n\n\n\nThis plot unveils the average sentence length, in words, for each president. A striking observation is that Zuma, despite having the most sentences and speeches, has a relatively concise average sentence length. Conversely, Mbeki and Motlanthe have longer average sentence lengths, with Mbeki being the only president that had over 30 words per sentence, on average. This metric offers insights into the verbosity and style of each president, which can be a useful feature when discerning speech patterns in model building."
  },
  {
    "objectID": "full_write_up.html#word-clouds-for-each-president",
    "href": "full_write_up.html#word-clouds-for-each-president",
    "title": "Assignment 1 Full Writeup",
    "section": "Word clouds for each president",
    "text": "Word clouds for each president\n\n\n\n\n\nThe word clouds above offer a visually compelling representation of the most frequently used words by each president. The size of each word in the cloud corresponds to its frequency in the speeches. All the presidents had “will” as their most prominent word and referred to the country many times while speaking (highlighted by the use of the words “south” and “africa”/“african”). Motlanthe seemed to focus more on the economy and public image with the use of words such as “national”, “public” and “government”, whereas Mandela seemed to focus more on the people with the use of words such as “people” and “us”. de Klerk focused more on the constitution and forming alliances during a transitional period, and Zuma focused more on work and the development. These word clouds provide a snapshot of the focal points and themes of each president’s speeches. Distinctive words or terms can be potential features when building predictive models. The words from the wordclouds can also be seen in the bar plots below."
  },
  {
    "objectID": "full_write_up.html#word-frequency-distribution-for-each-president",
    "href": "full_write_up.html#word-frequency-distribution-for-each-president",
    "title": "Assignment 1 Full Writeup",
    "section": "Word frequency distribution for each president",
    "text": "Word frequency distribution for each president"
  },
  {
    "objectID": "full_write_up.html#n-gram-frequency-distributions-for-each-president",
    "href": "full_write_up.html#n-gram-frequency-distributions-for-each-president",
    "title": "Assignment 1 Full Writeup",
    "section": "N-gram frequency distributions for each president",
    "text": "N-gram frequency distributions for each president\n\nBigrams\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nInstead of only looking at single word frequency, bigrams can also be used to find the most common two-word phrases. The bigrams above elucidate the distinctive linguistic patterns and thematic foci of each president, presenting opportunities for differentiation. For instance, President Mandela’s frequently used bigrams, such as “South Africans” and “national unity,” reflect his emphasis on nation-building and reconciliation during his tenure. In contrast, President Zuma’s bigrams like “economic growth” suggest a policy-driven discourse concentrated on economic dynamics. However, there are potential pitfalls. Overlapping or common bigrams across presidents, such as generic terms or phrases prevalent in political discourse, could introduce ambiguity, potentially hindering the model’s precision. Additionally, while President Ramaphosa’s bigrams like “South Africa” are distinctly frequent, they are not uniquely attributable to him, as such phrases are likely universal across South African presidencies."
  },
  {
    "objectID": "full_write_up.html#trigrams",
    "href": "full_write_up.html#trigrams",
    "title": "Assignment 1 Full Writeup",
    "section": "Trigrams",
    "text": "Trigrams\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExpanding on the analysis of linguistic markers, trigrams offer insights into the most recurrent three-word sequences employed by each president. The trigram outputs above further refine our understanding of the unique verbal choices and thematic concerns of each leader. For instance, President Mandela’s recurrent trigrams, such as “trade union movement”, underscore his consistent focus on the working class of South Africa. Meanwhile, President Zuma’s trigrams, such as “expaned public works” indicate a focus on the public sector as a whole. Conversely, the presence of generic or universally applicable trigrams, such as “state nation address”, might pose challenges. These broadly-used trigrams, inherent to political addresses across presidencies, might dilute the distinctive features of individual presidents, complicating the model’s task. Moreover, trigrams like “south africa will” from President Ramaphosa, although salient, are emblematic of speeches common to all presidents, making them less distinguishing. Thus, while trigrams can accentuate the nuances of each president’s discourse, the model would benefit from discerning the balance between distinctiveness and generic trigram usage."
  },
  {
    "objectID": "full_write_up.html#sentence-similarity-between-presidents",
    "href": "full_write_up.html#sentence-similarity-between-presidents",
    "title": "Assignment 1 Full Writeup",
    "section": "Sentence similarity between presidents",
    "text": "Sentence similarity between presidents\n\n\n2023-10-18 11:59:48.783489: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\nTo enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags."
  },
  {
    "objectID": "full_write_up.html#bag-of-words-bow-representation",
    "href": "full_write_up.html#bag-of-words-bow-representation",
    "title": "Assignment 1 Full Writeup",
    "section": "Bag of Words (BOW) representation",
    "text": "Bag of Words (BOW) representation\n\n\n\n\n\nThe Bag-of-Words (BoW) visualisation above reveals a pronounced central cluster with substantial overlap across presidential sentences, indicating pervasive shared linguistic elements. This convergence towards common terms suggests that the BoW representation predominantly captures universal themes and terminologies characteristic of political discourse. Such patterns, while illuminating shared linguistic tendencies, underscore potential challenges in predictive modeling, with the BoW approach possibly lacking the granularity to detect distinctive linguistic markers for each president."
  },
  {
    "objectID": "full_write_up.html#tf-idf-representation",
    "href": "full_write_up.html#tf-idf-representation",
    "title": "Assignment 1 Full Writeup",
    "section": "TF-IDF representation",
    "text": "TF-IDF representation\n\n\n\n\n\nUsing the TF-IDF representation, the visualization depicts a dominant central cluster, reaffirming the presence of overlapping linguistic constructs across presidential discourses. Unlike the BoW representation, the TF-IDF visualization lacks discernible smaller clusters, and data points appear more dispersed. This dispersion underscores the varied thematic undertones each president might have explored, but the pronounced overlap in the central region suggests that these thematic variations are not sufficiently distinct in the TF-IDF space to provide clear demarcations. The observed patterns emphasize the challenges inherent in solely relying on TF-IDF for capturing the unique linguistic nuances of each president."
  },
  {
    "objectID": "full_write_up.html#tokenization-with-padding-representation",
    "href": "full_write_up.html#tokenization-with-padding-representation",
    "title": "Assignment 1 Full Writeup",
    "section": "Tokenization with Padding representation",
    "text": "Tokenization with Padding representation\n\n\n\n\n\nUtilising tokenization with padding, the resultant visualization presents multiple clusters, indicating the method’s ability to recognize shared linguistic constructs or thematic groupings within the dataset. Notably, the significant intermingling of presidents within these clusters underscores the shared nature of discourse patterns across different presidencies. The absence of a dominant central cluster, a divergence from the BoW and TF-IDF representations, alludes to a more nuanced and diverse sentence representation in the embedding space, potentially attributed to the emphasis on sentence structure inherent in the tokenization method."
  },
  {
    "objectID": "full_write_up.html#text-representation-techniques",
    "href": "full_write_up.html#text-representation-techniques",
    "title": "Assignment 1 Full Writeup",
    "section": "1. Text Representation Techniques",
    "text": "1. Text Representation Techniques\n\na. Bag-of-Words (BoW)\nThe Bag-of-Words (BoW) representation is a simplistic yet effective method for text data representation. It hinges on representing text by its constituent words, disregarding their order. Here, each word operates as a feature, with the text being represented by a vector that denotes the frequency of each word (V M and Kumar R 2019).\nFormally, given a vocabulary \\(V\\) comprising \\(N\\) unique words, each document \\(d\\) can be depicted as a vector \\(\\mathbf{v}_d\\) in \\(\\mathbb{R}^N\\) , where the i-th element \\(v_{d,i}\\) denotes the frequency of the i-th word in the document:\n\\[\n\\mathbf{v}_d = [v_{d,1}, v_{d,2}, \\ldots, v_{d,N}]\n\\]\nThe dataset was transformed into a BoW representation with each row corresponding to a sentence, and each column reflecting the frequency of a word in that sentence. The CountVectorizer class from the sklearn.feature_extraction.text module was employed for this task, with English stop words being excluded to filter out prevalent words that lack significant meaning, such as “and”, “the”, and “is” (Pedregosa et al. 2011).\n\n\nb. Term Frequency-Inverse Document Frequency (TF-IDF)\nContrastingly, the TF-IDF representation scales the frequency of words based on their occurrence across all documents, ensuring that words appearing too frequently across documents (potentially bearing lesser discriminative importance) are assigned lower weights “Understanding TF-IDF: A Simple Introduction” (n.d.).\nThe term frequency (TF) of a word in a document is the raw count of that word in the document. The inverse document frequency (IDF) of a word is defined as:\n\\[\n\\text{IDF}(w) = \\log \\left( \\frac{N}{1 + \\text{count}(w)} \\right)\n\\]\nwhere \\(N\\) signifies the total number of documents and \\(\\text{count}(w)\\) represents the number of documents containing the word \\(w\\). The TF-IDF value for a word in a document is then the product of its TF and IDF values “Understanding TF-IDF: A Simple Introduction” (n.d.).\nThe TfidfVectorizer class from the sklearn.feature_extraction.text module was employed to transform our dataset into this representation (Pedregosa et al. 2011).\n\n\nc. Text Embedding\nFor processing by deep learning models like neural networks, textual data was tokenized and converted into sequences of numbers. The Tokenizer class from the keras.preprocessing.text module was utilized for this purpose. Subsequently, sentences were padded with zeros using pad_sequences from the keras.preprocessing.sequence module to ensure uniform length (Chollet et al. 2015)."
  },
  {
    "objectID": "full_write_up.html#model-architectures-and-training",
    "href": "full_write_up.html#model-architectures-and-training",
    "title": "Assignment 1 Full Writeup",
    "section": "2. Model Architectures and Training",
    "text": "2. Model Architectures and Training\n\na. Feed-Forward Neural Network\nFeed-forward neural networks (FFNNs) are a subset of artificial neural networks characterized by acyclic connections between nodes. They encompass multiple layers: an input layer, several hidden layers, and an output layer (Rumelhart, Hinton, and Williams 1986).\nThe architecture of the neural network employed in this study is delineated as follows:\n\nInput Layer: This layer harbors neurons equal to the number of features in the dataset (word counts for BoW and TF-IDF, sequence length for text embeddings). The Rectified Linear Unit (ReLU) activation function was utilized owing to its efficiency and capability to mitigate the vanishing gradient issue:\n\n\\[\nf(x) = \\max(0, x)\n\\]\n\nHidden Layers: Several hidden layers were introduced, each utilizing He initialization, which is proficient for layers with ReLU activation. A dropout layer succeeded each hidden layer to curb overfitting by randomly nullifying a fraction of input units during each training update.\nOutput Layer: This layer contains neurons equal to the number of classes (presidents, in our scenario). The softmax function was employed as the activation function, generating a probability distribution over the classes:\n\n\\[\n\\sigma(\\mathbf{z})_i = \\frac{e^{z_i}}{\\sum_{j=1}^{K} e^{z_j}}\n\\]\nfor \\(i = 1, \\ldots, K\\) and \\(\\mathbf{z}\\) is the input vector to the softmax function.\nTraining was conducted using the Adam optimization algorithm with a learning rate of 0.001. Adam is adept at training deep neural networks via computing adaptive learning rates for each parameter, leveraging moving averages of the parameter gradients and squared gradients.\nThe EarlyStopping and ReduceLROnPlateau callbacks were also enlisted. The former halts the training process if validation loss ceases to improve for a stipulated number of epochs, while the latter diminishes the learning rate if the validation loss reaches a plateau (Chollet et al. 2015).\n\n\nb. Support Vector Machine (SVM)\nThe Support Vector Machine (SVM) is a supervised learning algorithm suitable for both classification and regression tasks. It operates by identifying the optimal hyperplane that segregates a dataset into distinct classes. Provided a set of training examples, each labeled as belonging to one of two categories, the SVM training algorithm constructs a model that categorizes new examples into one of the two categories (Cortes and Vapnik 1995).\nMathematically, given labeled training data \\((x_1, y_1), \\ldots, (x_N, y_N)\\) where \\(x_i\\) belongs to \\(\\mathbb{R}^D\\) and \\(y_i\\) is either 1 or -1 (indicating the class the input \\(x_i\\) belongs to), SVM seeks the hyperplane defined by \\(w\\) and \\(b\\) that optimally separates the data points of the two classes (Cortes and Vapnik 1995):\n\\[\ny_i(w \\cdot x_i + b) \\geq 1\n\\]\nThe objective of SVM is to maximize the margin, which is the distance between the hyperplane and the nearest point from either class. The decision function is then given by:\n\\[\nf(x) = \\text{sign}(w \\cdot x + b)\n\\]\n\n\nc. Naive Bayes Classifier\nNaive Bayes is a probabilistic classifier predicated on Bayes’ theorem with strong (naive) independence assumptions among features (Raschka 2014). Given a set of features \\(X = x_1, \\ldots, x_n\\) and a class variable \\(C\\), Bayes’ theorem states:\n\\[\nP(C|X) = \\frac{P(X|C) \\times P(C)}{P(X)}\n\\]\nThe Naive Bayes classifier posits that the effect of a particular feature in a class is independent of other features. This simplification expedites computation, hence the term ‘naive’ (Raschka 2014).\nIn our problem, the Naive Bayes classifier estimates the probability of a sentence belonging to each president’s class based on the features (word frequencies for BoW or TF-IDF values). The sentence is then classified to the class (president) with the highest probability."
  },
  {
    "objectID": "full_write_up.html#model-evaluation",
    "href": "full_write_up.html#model-evaluation",
    "title": "Assignment 1 Full Writeup",
    "section": "3. Model Evaluation",
    "text": "3. Model Evaluation\nEvaluating the performance of machine learning models is paramount as it unveils the efficacy of the model and areas of potential improvement. Our evaluation paradigm leverages standard metrics including accuracy, precision, recall, and F1 score to quantify various facets of the model’s predictions in a multi-class classification setting such as ours, where predictions could be true or false for multiple classes (presidents, in this case).\n\na. Accuracy\nAccuracy furnishes a broad overview of the model’s performance and is calculated as the ratio of correct predictions to the total predictions:\n\\[\n\\text{Accuracy} = \\frac{\\text{Number of Correct Predictions}}{\\text{Total Number of Predictions}}\n\\]\nNonetheless, in imbalanced datasets, accuracy could be misleading.\n\n\nb. Precision\nPrecision scrutinizes the model’s positive predictions. Specifically, it computes the frequency at which the model correctly predicted a specific president out of all predictions for that president:\n\\[\n\\text{Precision (for a given president)} = \\frac{\\text{True Positives}}{\\text{True Positives} + \\text{False Positives}}\n\\]\nWhere:\n\nTrue Positives (TP): The number of sentences correctly identified as belonging to that president.\nFalse Positives (FP): The number of sentences erroneously identified as belonging to that president, while they belong to a different one.\n\nPrecision is particularly crucial in scenarios where the cost of a false positive is high.\n\n\nc. Recall (or Sensitivity)\nRecall evaluates how effectively the model identifies sentences from a specific president. It calculates the proportion of actual sentences from a president that the model correctly identified:\n\\[\n\\text{Recall (for a given president)} = \\frac{\\text{True Positives}}{\\text{True Positives} + \\text{False Negatives}}\n\\]\nWhere:\n\nFalse Negatives (FN): The number of sentences that genuinely belong to a president but were misclassified as belonging to another.\n\nRecall is vital in contexts where missing a true instance is significant.\n\n\nd. F1 Score\nThe F1 score is the harmonic mean of precision and recall, providing a balance between them. It achieves its best value at 1 (perfect precision and recall) and its worst at 0:\n\\[\n\\text{F1 Score} = 2 \\times \\frac{\\text{Precision} \\times \\text{Recall}}{\\text{Precision} + \\text{Recall}}\n\\]\nThe F1 score is particularly useful when there is an uneven data distribution among classes.\nThese metrics were computed for each president in our dataset and then averaged (weighted by the number of true instances for each president) to derive a single value representing the overall model’s performance. This approach ensures that the model’s aptitude to predict less frequent classes (presidents with fewer sentences) is considered, rendering the evaluation more robust and representative of the model’s true capabilities in a multi-class setting.\nMoreover, the models were also assessed on separate training and test datasets. The training dataset is the learning corpus for the model, while the test dataset presents a fresh, unseen set of data points to gauge the model’s generalization to new data. This separation is pivotal to ensure that the model doesn’t merely memorize the training data (overfitting), but discerns the underlying patterns determining which president uttered a given sentence."
  },
  {
    "objectID": "full_write_up.html#bag-of-words",
    "href": "full_write_up.html#bag-of-words",
    "title": "Assignment 1 Full Writeup",
    "section": "Bag of Words",
    "text": "Bag of Words\nIn the exploration of various models on a training set employing a bag of words representation, distinct performance disparities were observed. Firstly, utilising a feed-forward neural network, an impressive training accuracy of 0.99 was attained. However, its validation accuracy registered at 0.6, hinting at potential overfitting. An analysis of the test set predictions, as depicted by the confusion matrix, demonstrated that the correct classes were predominantly predicted for the corresponding sentences. Notably, the test set metrics were: precision at 0.595, recall at 0.569, and the f1 score also at 0.569.\nThe employment of support vector machines (SVM) with the bag of words approach, post-tuning, yielded training and validation accuracies of 0.989 and 0.533 respectively, with the test accuracy being 0.55. The precision, recall, and f1 scores for this model stood at 0.551, 0.55, and 0.547 respectively.\nLastly, when Naive Bayes was paired with the bag of words method, post-optimisation, it achieved training and validation accuracies of 0.89 and 0.592 respectively. This model appeared less prone to overfitting compared to its counterparts. For the test set, precision was measured at 0.624, recall at 0.615, and interestingly, the test accuracy was noted to be 0.616, slightly higher than the recall.\n\nNeural network\n\n\n\n\n\nAn accuracy plot for the neural network model trained on the Bag of Words representation.\n\n\n\n\n\n\n\n\n\nA table of accuracies for the neural network model trained on the Bag of Words representation. \n\n\nTraining Accuracy\nValidation Accuracy\n\n\n\n\n0.994\n0.601\n\n\n\n\n\n\n\n\n\n\nA confusion matrix for the neural network model trained on the Bag of Words representation.\n\n\n\n\n\n\n\n\n\nA table of test classification results for the neural network model trained on the Bag of Words representation. \n\n\nMetric\nValue\n\n\n\n\nPrecision\n0.595\n\n\nRecall\n0.575\n\n\nF1 Score\n0.569\n\n\n\n\n\n\n\nSupport Vector Machine\n\n\n\n\n\nAn accuracy plot for the SVM model trained on the Bag of Words representation.\n\n\n\n\n\n\n\n\n\n\n\nHyperparameter Value\nTraining Accuracy\nValidation Accuracy\nTest Accuracy\n\n\n\n\n10.000\n0.989\n0.533\n0.550\n\n\n\n\n\n\n\n\n\n\nC\nTraining Accuracy\nValidation Accuracy\nTest Accuracy\n\n\n\n\n0.001\n0.295\n0.294\n0.294\n\n\n0.010\n0.304\n0.309\n0.306\n\n\n0.100\n0.391\n0.322\n0.325\n\n\n0.500\n0.748\n0.425\n0.445\n\n\n1.000\n0.920\n0.527\n0.566\n\n\n10.000\n0.989\n0.533\n0.550\n\n\n100.000\n0.995\n0.498\n0.517\n\n\n\n\nA table of hyperparameter tuning results for the SVM model trained on the Bag of Words representation, with the best hyperparameters highlighted above.\n\n\n\n\n\n\nA confusion matrix for the SVM model trained on the Bag of Words representation.\n\n\n\n\n\n\n\n\n\nA table of test classification results for the SVM model trained on the Bag of Words representation. \n\n\nMetric\nValue\n\n\n\n\nPrecision\n0.551\n\n\nRecall\n0.55\n\n\nF1 Score\n0.547\n\n\n\n\n\n\n\nNaive Bayes\n\n\n\n\n\nAn accuracy plot for the NB model trained on the Bag of Words representation.\n\n\n\n\n\n\n\n\n\n\n\nHyperparameter Value\nTraining Accuracy\nValidation Accuracy\nTest Accuracy\n\n\n\n\n0.100\n0.890\n0.592\n0.615\n\n\n\n\n\n\n\n\n\n\nalpha\nTraining Accuracy\nValidation Accuracy\nTest Accuracy\n\n\n\n\n0.001\n0.906\n0.572\n0.597\n\n\n0.010\n0.904\n0.581\n0.608\n\n\n0.100\n0.890\n0.592\n0.615\n\n\n1.000\n0.838\n0.586\n0.611\n\n\n10.000\n0.637\n0.494\n0.523\n\n\n\n\nA table of hyperparameter tuning results for the NB model trained on the Bag of Words representation, with the best hyperparameters highlighted above.\n\n\n\n\n\n\nA confusion matrix for the NB model trained on the Bag of Words representation.\n\n\n\n\n\n\n\n\n\nA table of test classification results for the NB model trained on the Bag of Words representation. \n\n\nMetric\nValue\n\n\n\n\nPrecision\n0.624\n\n\nRecall\n0.615\n\n\nF1 Score\n0.616"
  },
  {
    "objectID": "full_write_up.html#tf-idf",
    "href": "full_write_up.html#tf-idf",
    "title": "Assignment 1 Full Writeup",
    "section": "TF-IDF",
    "text": "TF-IDF\nIn a subsequent analysis utilising the term frequency-inverse document frequency (tf-idf) representation with various models, certain resemblances to the bag of words (bow) results were discerned. Firstly, with the feed-forward neural network, the accuracy plot bore a striking similarity to its bow counterpart. This model achieved a training accuracy of 0.99 and a validation accuracy of 0.588. The confusion matrix for test set predictions indicated that the majority of sentences were assigned their correct classes. The test set metrics recorded were: precision at 0.598, recall at 0.597, and the f1 score at 0.595.\nWhen the support vector machines (SVM) were employed in tandem with the tf-idf representation, the accuracy plot was found to mirror that of the bow version. After tuning, the training accuracy registered at 0.968, with validation and test accuracies being 0.542 and 0.574, respectively. The precision, recall, and f1 scores for this model were 0.58, 0.574, and 0.573 in that order.\nLastly, the Naive Bayes model with the tf-idf approach displayed accuracy plots bearing a resemblance to the bow version. Post-optimisation, it yielded training, validation, and test accuracies of 0.915, 0.594, and 0.611 respectively. The precision and recall both stood at 0.611, whilst the test accuracy was slightly lower at 0.609.\n\nNeural network\n\n\n\n\n\nAn accuracy plot for the neural network model trained on the TF-IDF representation.\n\n\n\n\n\n\n\n\n\nA table of accuracies for the neural network model trained on the TF-IDF representation. \n\n\nTraining Accuracy\nValidation Accuracy\n\n\n\n\n0.994\n0.588\n\n\n\n\n\n\n\n\n\n\nA confusion matrix for the neural network model trained on the TF-IDF representation.\n\n\n\n\n\n\n\n\n\nA table of test classification results for the neural network model trained on the TF-IDF representation. \n\n\nMetric\nValue\n\n\n\n\nPrecision\n0.598\n\n\nRecall\n0.597\n\n\nF1 Score\n0.595\n\n\n\n\n\n\n\nSupport Vector Machine\n\n\n\n\n\nAn accuracy plot for the SVM model trained on the TF-IDF representation.\n\n\n\n\n\n\n\n\n\n\n\nHyperparameter Value\nTraining Accuracy\nValidation Accuracy\nTest Accuracy\n\n\n\n\n1.000\n0.968\n0.542\n0.574\n\n\n\n\n\n\n\n\n\n\nC\nTraining Accuracy\nValidation Accuracy\nTest Accuracy\n\n\n\n\n0.001\n0.295\n0.294\n0.294\n\n\n0.010\n0.300\n0.296\n0.288\n\n\n0.100\n0.364\n0.306\n0.301\n\n\n0.500\n0.832\n0.435\n0.439\n\n\n1.000\n0.968\n0.542\n0.574\n\n\n10.000\n0.995\n0.531\n0.550\n\n\n100.000\n0.996\n0.528\n0.544\n\n\n\n\nA table of hyperparameter tuning results for the SVM model trained on the TF-IDF representation, with the best hyperparameters highlighted above.\n\n\n\n\n\n\nA confusion matrix for the SVM model trained on the TF-IDF representation.\n\n\n\n\n\n\n\n\n\nA table of test classification results for the SVM model trained on the TF-IDF representation. \n\n\nMetric\nValue\n\n\n\n\nPrecision\n0.58\n\n\nRecall\n0.574\n\n\nF1 Score\n0.573\n\n\n\n\n\n\n\nNaive Bayes\n\n\n\n\n\nAn accuracy plot for the NB model trained on the TF-IDF representation.\n\n\n\n\n\n\n\n\n\n\n\nHyperparameter Value\nTraining Accuracy\nValidation Accuracy\nTest Accuracy\n\n\n\n\n0.100\n0.915\n0.594\n0.611\n\n\n\n\n\n\n\n\n\n\nalpha\nTraining Accuracy\nValidation Accuracy\nTest Accuracy\n\n\n\n\n0.001\n0.931\n0.575\n0.594\n\n\n0.010\n0.929\n0.584\n0.605\n\n\n0.100\n0.915\n0.594\n0.611\n\n\n1.000\n0.812\n0.575\n0.590\n\n\n10.000\n0.622\n0.500\n0.519\n\n\n\n\nA table of hyperparameter tuning results for the NB model trained on the TF-IDF representation, with the best hyperparameters highlighted above.\n\n\n\n\n\n\nA confusion matrix for the NB model trained on the TF-IDF representation.\n\n\n\n\n\n\n\n\n\nA table of test classification results for the NB model trained on the TF-IDF representation. \n\n\nMetric\nValue\n\n\n\n\nPrecision\n0.611\n\n\nRecall\n0.611\n\n\nF1 Score\n0.609"
  },
  {
    "objectID": "full_write_up.html#token-embeddings",
    "href": "full_write_up.html#token-embeddings",
    "title": "Assignment 1 Full Writeup",
    "section": "Token Embeddings",
    "text": "Token Embeddings\nUpon utilising text embedding as a representation technique alongside various models, a marked degradation in performance was observed compared to other preprocessing methods. With the feed-forward neural network, the training accuracy was a mere 0.409, while the validation accuracy dropped further to 0.369. The confusion matrix for test set predictions was quite telling: for a majority of sentences, the correct classes were not discerned. Intriguingly, the class “Zuma” was predominantly predicted. The test set showcased a precision of 0.367, recall of 0.368, and a notably lower f1 score of 0.328.\nWhen paired with the support vector machines (SVM), post-tuning, the training accuracy stood at 0.406, with validation and test accuracies of 0.361 and 0.347, respectively. The precision was 0.342, the recall was 0.347, and the f1 score was slightly lower at 0.319.\nIncorporating the Naive Bayes model with text embedding, post-optimisation, the training, validation, and test accuracies were 0.359, 0.359, and 0.338 in that order. This model’s precision and recall registered at 0.335 and 0.338 respectively, with the test accuracy being considerably reduced to 0.291. This underlines the challenge posed by text embeddings in this specific context, as the results were notably inferior to other data preparation methods.\n\nNeural network\n\n\n\n\n\nAn accuracy plot for the neural network model trained on the text embeddings representation.\n\n\n\n\n\n\n\n\n\nA table of accuracies for the neural network model trained on the text embeddings representation. \n\n\nTraining Accuracy\nValidation Accuracy\n\n\n\n\n0.409\n0.369\n\n\n\n\n\n\n\n\n\n\nA confusion matrix for the neural network model trained on the text embeddings representation.\n\n\n\n\n\n\n\n\n\nA table of test classification results for the neural network model trained on the text embeddings representation. \n\n\nMetric\nValue\n\n\n\n\nPrecision\n0.367\n\n\nRecall\n0.368\n\n\nF1 Score\n0.328\n\n\n\n\n\n\n\nSupport Vector Machine\n\n\n\n\n\nAn accuracy plot for the SVM model trained on the text embedding representation.\n\n\n\n\n\n\n\n\n\n\n\nHyperparameter Value\nTraining Accuracy\nValidation Accuracy\nTest Accuracy\n\n\n\n\n0.500\n0.406\n0.361\n0.347\n\n\n\n\n\n\n\n\n\n\nC\nTraining Accuracy\nValidation Accuracy\nTest Accuracy\n\n\n\n\n0.001\n0.257\n0.257\n0.257\n\n\n0.010\n0.300\n0.300\n0.292\n\n\n0.100\n0.368\n0.340\n0.335\n\n\n0.500\n0.406\n0.361\n0.347\n\n\n1.000\n0.451\n0.353\n0.341\n\n\n10.000\n0.574\n0.324\n0.338\n\n\n100.000\n0.687\n0.334\n0.327\n\n\n\n\nA table of hyperparameter tuning results for the SVM model trained on the text embedding representation, with the best hyperparameters highlighted below.\n\n\n\n\n\n\nA confusion matrix for the SVM model trained on the text embedding representation.\n\n\n\n\n\n\n\n\n\nA table of test classification results for the SVM model trained on the text embedding representation. \n\n\nMetric\nValue\n\n\n\n\nPrecision\n0.342\n\n\nRecall\n0.347\n\n\nF1 Score\n0.319\n\n\n\n\n\n\n\nNaive Bayes\n\n\n\n\n\nAn accuracy plot for the NB model trained on the text embedding representation.\n\n\n\n\n\n\n\n\n\n\n\nHyperparameter Value\nTraining Accuracy\nValidation Accuracy\nTest Accuracy\n\n\n\n\n0.001\n0.359\n0.359\n0.338\n\n\n0.001\n0.359\n0.359\n0.338\n\n\n0.001\n0.359\n0.359\n0.338\n\n\n0.001\n0.359\n0.359\n0.338\n\n\n0.001\n0.359\n0.359\n0.338\n\n\n\n\n\n\n\n\n\n\nalpha\nTraining Accuracy\nValidation Accuracy\nTest Accuracy\n\n\n\n\n0.001\n0.359\n0.359\n0.338\n\n\n0.010\n0.359\n0.359\n0.338\n\n\n0.100\n0.359\n0.359\n0.338\n\n\n1.000\n0.359\n0.359\n0.338\n\n\n10.000\n0.359\n0.359\n0.338\n\n\n\n\nA table of hyperparameter tuning results for the NB model trained on the text embedding representation, with the best hyperparameters highlighted above.\n\n\n\n\n\n\nA confusion matrix for the NB model trained on the text embedding representation.\n\n\n\n\n\n\n\n\n\nA table of test classification results for the NB model trained on the text embedding representation. \n\n\nMetric\nValue\n\n\n\n\nPrecision\n0.335\n\n\nRecall\n0.338\n\n\nF1 Score\n0.291"
  },
  {
    "objectID": "full_write_up.html#bert-embeddings-with-pre-trained-classifier",
    "href": "full_write_up.html#bert-embeddings-with-pre-trained-classifier",
    "title": "Assignment 1 Full Writeup",
    "section": "BERT Embeddings with pre-trained classifier",
    "text": "BERT Embeddings with pre-trained classifier\nThe code for this section was adapted from the following source: Google Tensorflow BERT tutorial\nUtilising the BERT embedding in tandem with a pre-trained model, a strategy known as transfer learning, distinctive patterns in performance were observed. Throughout the training epochs, the training accuracy showcased a consistent uptick. However, the validation accuracy plateaued rather swiftly, exhibiting minimal fluctuations thereafter. At the culmination of the training, the accuracy metrics stood as follows: training accuracy at 0.759, validation accuracy at 0.684, and a slightly higher test accuracy of 0.712. Further delving into the test set metrics, the precision was 0.71, recall was 0.707, and the f1 score was close behind at 0.708. An examination of the confusion matrix for the test set underscored these findings. The model predominantly made accurate predictions for the respective presidents, mirroring the positive metrics mentioned earlier. This highlights the efficacy of the BERT embeddings and transfer learning in this particular context, as the results were substantially more favourable than some other methods previously explored.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\naccuracy\nprecision\nrecall\nf1_score\n\n\n\n\nTraining\n0.759\n0.507\n0.837\n0.503\n\n\nValidation\n0.684\n0.548\n0.744\n0.543\n\n\nTest\n0.712\n0.710\n0.707\n0.708"
  },
  {
    "objectID": "discussion_conclusion.html",
    "href": "discussion_conclusion.html",
    "title": "Discussion and Conclusion",
    "section": "",
    "text": "This study aimed to figure out which of the South African presidents, from 1994 to 2022, might have said certain sentences during their State of the Nation Address (SONA). Different ways of processing the text, such as Bag of Words (BoW), Term Frequency-Inverse Document Frequency (tf-idf), and text embeddings, were used. These methods were then paired with machine learning models to see which combination worked best.\nWith the Bag of Words (BoW) method, the feed-forward neural network did well in training but not as well in validation, suggesting it might not do well with new, unseen data. The SVM and Naive Bayes models had similar outcomes. The tf-idf method gave results close to BoW for the neural net and SVM, but Naive Bayes seemed a bit more stable. However, simple text embeddings didn’t work as well across the board. This could be because these embeddings might be too basic to capture the unique way presidents speak in their SONA addresses.\nOn the other hand, using BERT embeddings with a pre-trained model gave us some hope. The model kept getting better during training, and its test results were the best among all the models. This suggests that using advanced methods like BERT might be the way forward for such tasks."
  },
  {
    "objectID": "discussion_conclusion.html#discussion",
    "href": "discussion_conclusion.html#discussion",
    "title": "Discussion and Conclusion",
    "section": "",
    "text": "This study aimed to figure out which of the South African presidents, from 1994 to 2022, might have said certain sentences during their State of the Nation Address (SONA). Different ways of processing the text, such as Bag of Words (BoW), Term Frequency-Inverse Document Frequency (tf-idf), and text embeddings, were used. These methods were then paired with machine learning models to see which combination worked best.\nWith the Bag of Words (BoW) method, the feed-forward neural network did well in training but not as well in validation, suggesting it might not do well with new, unseen data. The SVM and Naive Bayes models had similar outcomes. The tf-idf method gave results close to BoW for the neural net and SVM, but Naive Bayes seemed a bit more stable. However, simple text embeddings didn’t work as well across the board. This could be because these embeddings might be too basic to capture the unique way presidents speak in their SONA addresses.\nOn the other hand, using BERT embeddings with a pre-trained model gave us some hope. The model kept getting better during training, and its test results were the best among all the models. This suggests that using advanced methods like BERT might be the way forward for such tasks."
  },
  {
    "objectID": "discussion_conclusion.html#conclusion",
    "href": "discussion_conclusion.html#conclusion",
    "title": "Discussion and Conclusion",
    "section": "Conclusion:",
    "text": "Conclusion:\nThis study shows how important it is to pick the right method to process text and the right model to analyse it. While methods like BoW and tf-idf gave decent results, simple text embeddings didn’t do as well. But, the combination of BERT embeddings and a pre-trained model stood out.\nThis has two main takeaways. First, for researchers looking into political speeches, these models can help in figuring out who might have said an unattributed speech. Second, for those into machine learning, it highlights the growing role of advanced methods like BERT.\nLooking ahead, it might be worth exploring even better text processing methods or fine-tuning models like BERT for even more accurate results. Overall, this study shows the exciting possibilities when combining tech with the study of political speeches."
  },
  {
    "objectID": "results.html",
    "href": "results.html",
    "title": "Results",
    "section": "",
    "text": "In the exploration of various models on a training set employing a bag of words representation, distinct performance disparities were observed. Firstly, utilising a feed-forward neural network, an impressive training accuracy of 0.99 was attained. However, its validation accuracy registered at 0.6, hinting at potential overfitting. An analysis of the test set predictions, as depicted by the confusion matrix, demonstrated that the correct classes were predominantly predicted for the corresponding sentences. Notably, the test set metrics were: precision at 0.595, recall at 0.569, and the f1 score also at 0.569.\nThe employment of support vector machines (SVM) with the bag of words approach, post-tuning, yielded training and validation accuracies of 0.989 and 0.533 respectively, with the test accuracy being 0.55. The precision, recall, and f1 scores for this model stood at 0.551, 0.55, and 0.547 respectively.\nLastly, when Naive Bayes was paired with the bag of words method, post-optimisation, it achieved training and validation accuracies of 0.89 and 0.592 respectively. This model appeared less prone to overfitting compared to its counterparts. For the test set, precision was measured at 0.624, recall at 0.615, and interestingly, the test accuracy was noted to be 0.616, slightly higher than the recall.\n\n\n\n\n\n\n\nAn accuracy plot for the neural network model trained on the Bag of Words representation.\n\n\n\n\n\n\n\n\n\nA table of accuracies for the neural network model trained on the Bag of Words representation. \n\n\nTraining Accuracy\nValidation Accuracy\n\n\n\n\n0.994\n0.601\n\n\n\n\n\n\n\n\n\n\nA confusion matrix for the neural network model trained on the Bag of Words representation.\n\n\n\n\n\n\n\n\n\nA table of test classification results for the neural network model trained on the Bag of Words representation. \n\n\nMetric\nValue\n\n\n\n\nPrecision\n0.595\n\n\nRecall\n0.575\n\n\nF1 Score\n0.569\n\n\n\n\n\n\n\n\n\n\n\n\n\nAn accuracy plot for the SVM model trained on the Bag of Words representation.\n\n\n\n\n\n\n\n\n\n\n\nHyperparameter Value\nTraining Accuracy\nValidation Accuracy\nTest Accuracy\n\n\n\n\n10.000\n0.989\n0.533\n0.550\n\n\n\n\n\n\n\n\n\n\nC\nTraining Accuracy\nValidation Accuracy\nTest Accuracy\n\n\n\n\n0.001\n0.295\n0.294\n0.294\n\n\n0.010\n0.304\n0.309\n0.306\n\n\n0.100\n0.391\n0.322\n0.325\n\n\n0.500\n0.748\n0.425\n0.445\n\n\n1.000\n0.920\n0.527\n0.566\n\n\n10.000\n0.989\n0.533\n0.550\n\n\n100.000\n0.995\n0.498\n0.517\n\n\n\n\nA table of hyperparameter tuning results for the SVM model trained on the Bag of Words representation, with the best hyperparameters highlighted above.\n\n\n\n\n\n\nA confusion matrix for the SVM model trained on the Bag of Words representation.\n\n\n\n\n\n\n\n\n\nA table of test classification results for the SVM model trained on the Bag of Words representation. \n\n\nMetric\nValue\n\n\n\n\nPrecision\n0.551\n\n\nRecall\n0.55\n\n\nF1 Score\n0.547\n\n\n\n\n\n\n\n\n\n\n\n\n\nAn accuracy plot for the NB model trained on the Bag of Words representation.\n\n\n\n\n\n\n\n\n\n\n\nHyperparameter Value\nTraining Accuracy\nValidation Accuracy\nTest Accuracy\n\n\n\n\n0.100\n0.890\n0.592\n0.615\n\n\n\n\n\n\n\n\n\n\nalpha\nTraining Accuracy\nValidation Accuracy\nTest Accuracy\n\n\n\n\n0.001\n0.906\n0.572\n0.597\n\n\n0.010\n0.904\n0.581\n0.608\n\n\n0.100\n0.890\n0.592\n0.615\n\n\n1.000\n0.838\n0.586\n0.611\n\n\n10.000\n0.637\n0.494\n0.523\n\n\n\n\nA table of hyperparameter tuning results for the NB model trained on the Bag of Words representation, with the best hyperparameters highlighted above.\n\n\n\n\n\n\nA confusion matrix for the NB model trained on the Bag of Words representation.\n\n\n\n\n\n\n\n\n\nA table of test classification results for the NB model trained on the Bag of Words representation. \n\n\nMetric\nValue\n\n\n\n\nPrecision\n0.624\n\n\nRecall\n0.615\n\n\nF1 Score\n0.616"
  },
  {
    "objectID": "results.html#bag-of-words",
    "href": "results.html#bag-of-words",
    "title": "Results",
    "section": "",
    "text": "In the exploration of various models on a training set employing a bag of words representation, distinct performance disparities were observed. Firstly, utilising a feed-forward neural network, an impressive training accuracy of 0.99 was attained. However, its validation accuracy registered at 0.6, hinting at potential overfitting. An analysis of the test set predictions, as depicted by the confusion matrix, demonstrated that the correct classes were predominantly predicted for the corresponding sentences. Notably, the test set metrics were: precision at 0.595, recall at 0.569, and the f1 score also at 0.569.\nThe employment of support vector machines (SVM) with the bag of words approach, post-tuning, yielded training and validation accuracies of 0.989 and 0.533 respectively, with the test accuracy being 0.55. The precision, recall, and f1 scores for this model stood at 0.551, 0.55, and 0.547 respectively.\nLastly, when Naive Bayes was paired with the bag of words method, post-optimisation, it achieved training and validation accuracies of 0.89 and 0.592 respectively. This model appeared less prone to overfitting compared to its counterparts. For the test set, precision was measured at 0.624, recall at 0.615, and interestingly, the test accuracy was noted to be 0.616, slightly higher than the recall.\n\n\n\n\n\n\n\nAn accuracy plot for the neural network model trained on the Bag of Words representation.\n\n\n\n\n\n\n\n\n\nA table of accuracies for the neural network model trained on the Bag of Words representation. \n\n\nTraining Accuracy\nValidation Accuracy\n\n\n\n\n0.994\n0.601\n\n\n\n\n\n\n\n\n\n\nA confusion matrix for the neural network model trained on the Bag of Words representation.\n\n\n\n\n\n\n\n\n\nA table of test classification results for the neural network model trained on the Bag of Words representation. \n\n\nMetric\nValue\n\n\n\n\nPrecision\n0.595\n\n\nRecall\n0.575\n\n\nF1 Score\n0.569\n\n\n\n\n\n\n\n\n\n\n\n\n\nAn accuracy plot for the SVM model trained on the Bag of Words representation.\n\n\n\n\n\n\n\n\n\n\n\nHyperparameter Value\nTraining Accuracy\nValidation Accuracy\nTest Accuracy\n\n\n\n\n10.000\n0.989\n0.533\n0.550\n\n\n\n\n\n\n\n\n\n\nC\nTraining Accuracy\nValidation Accuracy\nTest Accuracy\n\n\n\n\n0.001\n0.295\n0.294\n0.294\n\n\n0.010\n0.304\n0.309\n0.306\n\n\n0.100\n0.391\n0.322\n0.325\n\n\n0.500\n0.748\n0.425\n0.445\n\n\n1.000\n0.920\n0.527\n0.566\n\n\n10.000\n0.989\n0.533\n0.550\n\n\n100.000\n0.995\n0.498\n0.517\n\n\n\n\nA table of hyperparameter tuning results for the SVM model trained on the Bag of Words representation, with the best hyperparameters highlighted above.\n\n\n\n\n\n\nA confusion matrix for the SVM model trained on the Bag of Words representation.\n\n\n\n\n\n\n\n\n\nA table of test classification results for the SVM model trained on the Bag of Words representation. \n\n\nMetric\nValue\n\n\n\n\nPrecision\n0.551\n\n\nRecall\n0.55\n\n\nF1 Score\n0.547\n\n\n\n\n\n\n\n\n\n\n\n\n\nAn accuracy plot for the NB model trained on the Bag of Words representation.\n\n\n\n\n\n\n\n\n\n\n\nHyperparameter Value\nTraining Accuracy\nValidation Accuracy\nTest Accuracy\n\n\n\n\n0.100\n0.890\n0.592\n0.615\n\n\n\n\n\n\n\n\n\n\nalpha\nTraining Accuracy\nValidation Accuracy\nTest Accuracy\n\n\n\n\n0.001\n0.906\n0.572\n0.597\n\n\n0.010\n0.904\n0.581\n0.608\n\n\n0.100\n0.890\n0.592\n0.615\n\n\n1.000\n0.838\n0.586\n0.611\n\n\n10.000\n0.637\n0.494\n0.523\n\n\n\n\nA table of hyperparameter tuning results for the NB model trained on the Bag of Words representation, with the best hyperparameters highlighted above.\n\n\n\n\n\n\nA confusion matrix for the NB model trained on the Bag of Words representation.\n\n\n\n\n\n\n\n\n\nA table of test classification results for the NB model trained on the Bag of Words representation. \n\n\nMetric\nValue\n\n\n\n\nPrecision\n0.624\n\n\nRecall\n0.615\n\n\nF1 Score\n0.616"
  },
  {
    "objectID": "results.html#tf-idf",
    "href": "results.html#tf-idf",
    "title": "Results",
    "section": "TF-IDF",
    "text": "TF-IDF\nIn a subsequent analysis utilising the term frequency-inverse document frequency (tf-idf) representation with various models, certain resemblances to the bag of words (bow) results were discerned. Firstly, with the feed-forward neural network, the accuracy plot bore a striking similarity to its bow counterpart. This model achieved a training accuracy of 0.99 and a validation accuracy of 0.588. The confusion matrix for test set predictions indicated that the majority of sentences were assigned their correct classes. The test set metrics recorded were: precision at 0.598, recall at 0.597, and the f1 score at 0.595.\nWhen the support vector machines (SVM) were employed in tandem with the tf-idf representation, the accuracy plot was found to mirror that of the bow version. After tuning, the training accuracy registered at 0.968, with validation and test accuracies being 0.542 and 0.574, respectively. The precision, recall, and f1 scores for this model were 0.58, 0.574, and 0.573 in that order.\nLastly, the Naive Bayes model with the tf-idf approach displayed accuracy plots bearing a resemblance to the bow version. Post-optimisation, it yielded training, validation, and test accuracies of 0.915, 0.594, and 0.611 respectively. The precision and recall both stood at 0.611, whilst the test accuracy was slightly lower at 0.609.\n\nNeural network\n\n\n\n\n\nAn accuracy plot for the neural network model trained on the TF-IDF representation.\n\n\n\n\n\n\n\n\n\nA table of accuracies for the neural network model trained on the TF-IDF representation. \n\n\nTraining Accuracy\nValidation Accuracy\n\n\n\n\n0.994\n0.588\n\n\n\n\n\n\n\n\n\n\nA confusion matrix for the neural network model trained on the TF-IDF representation.\n\n\n\n\n\n\n\n\n\nA table of test classification results for the neural network model trained on the TF-IDF representation. \n\n\nMetric\nValue\n\n\n\n\nPrecision\n0.598\n\n\nRecall\n0.597\n\n\nF1 Score\n0.595\n\n\n\n\n\n\n\nSupport Vector Machine\n\n\n\n\n\nAn accuracy plot for the SVM model trained on the TF-IDF representation.\n\n\n\n\n\n\n\n\n\n\n\nHyperparameter Value\nTraining Accuracy\nValidation Accuracy\nTest Accuracy\n\n\n\n\n1.000\n0.968\n0.542\n0.574\n\n\n\n\n\n\n\n\n\n\nC\nTraining Accuracy\nValidation Accuracy\nTest Accuracy\n\n\n\n\n0.001\n0.295\n0.294\n0.294\n\n\n0.010\n0.300\n0.296\n0.288\n\n\n0.100\n0.364\n0.306\n0.301\n\n\n0.500\n0.832\n0.435\n0.439\n\n\n1.000\n0.968\n0.542\n0.574\n\n\n10.000\n0.995\n0.531\n0.550\n\n\n100.000\n0.996\n0.528\n0.544\n\n\n\n\nA table of hyperparameter tuning results for the SVM model trained on the TF-IDF representation, with the best hyperparameters highlighted above.\n\n\n\n\n\n\nA confusion matrix for the SVM model trained on the TF-IDF representation.\n\n\n\n\n\n\n\n\n\nA table of test classification results for the SVM model trained on the TF-IDF representation. \n\n\nMetric\nValue\n\n\n\n\nPrecision\n0.58\n\n\nRecall\n0.574\n\n\nF1 Score\n0.573\n\n\n\n\n\n\n\nNaive Bayes\n\n\n\n\n\nAn accuracy plot for the NB model trained on the TF-IDF representation.\n\n\n\n\n\n\n\n\n\n\n\nHyperparameter Value\nTraining Accuracy\nValidation Accuracy\nTest Accuracy\n\n\n\n\n0.100\n0.915\n0.594\n0.611\n\n\n\n\n\n\n\n\n\n\nalpha\nTraining Accuracy\nValidation Accuracy\nTest Accuracy\n\n\n\n\n0.001\n0.931\n0.575\n0.594\n\n\n0.010\n0.929\n0.584\n0.605\n\n\n0.100\n0.915\n0.594\n0.611\n\n\n1.000\n0.812\n0.575\n0.590\n\n\n10.000\n0.622\n0.500\n0.519\n\n\n\n\nA table of hyperparameter tuning results for the NB model trained on the TF-IDF representation, with the best hyperparameters highlighted above.\n\n\n\n\n\n\nA confusion matrix for the NB model trained on the TF-IDF representation.\n\n\n\n\n\n\n\n\n\nA table of test classification results for the NB model trained on the TF-IDF representation. \n\n\nMetric\nValue\n\n\n\n\nPrecision\n0.611\n\n\nRecall\n0.611\n\n\nF1 Score\n0.609"
  },
  {
    "objectID": "results.html#token-embeddings",
    "href": "results.html#token-embeddings",
    "title": "Results",
    "section": "Token Embeddings",
    "text": "Token Embeddings\nUpon utilising text embedding as a representation technique alongside various models, a marked degradation in performance was observed compared to other preprocessing methods. With the feed-forward neural network, the training accuracy was a mere 0.409, while the validation accuracy dropped further to 0.369. The confusion matrix for test set predictions was quite telling: for a majority of sentences, the correct classes were not discerned. Intriguingly, the class “Zuma” was predominantly predicted. The test set showcased a precision of 0.367, recall of 0.368, and a notably lower f1 score of 0.328.\nWhen paired with the support vector machines (SVM), post-tuning, the training accuracy stood at 0.406, with validation and test accuracies of 0.361 and 0.347, respectively. The precision was 0.342, the recall was 0.347, and the f1 score was slightly lower at 0.319.\nIncorporating the Naive Bayes model with text embedding, post-optimisation, the training, validation, and test accuracies were 0.359, 0.359, and 0.338 in that order. This model’s precision and recall registered at 0.335 and 0.338 respectively, with the test accuracy being considerably reduced to 0.291. This underlines the challenge posed by text embeddings in this specific context, as the results were notably inferior to other data preparation methods.\n\nNeural network\n\n\n\n\n\nAn accuracy plot for the neural network model trained on the text embeddings representation.\n\n\n\n\n\n\n\n\n\nA table of accuracies for the neural network model trained on the text embeddings representation. \n\n\nTraining Accuracy\nValidation Accuracy\n\n\n\n\n0.409\n0.369\n\n\n\n\n\n\n\n\n\n\nA confusion matrix for the neural network model trained on the text embeddings representation.\n\n\n\n\n\n\n\n\n\nA table of test classification results for the neural network model trained on the text embeddings representation. \n\n\nMetric\nValue\n\n\n\n\nPrecision\n0.367\n\n\nRecall\n0.368\n\n\nF1 Score\n0.328\n\n\n\n\n\n\n\nSupport Vector Machine\n\n\n\n\n\nAn accuracy plot for the SVM model trained on the text embedding representation.\n\n\n\n\n\n\n\n\n\n\n\nHyperparameter Value\nTraining Accuracy\nValidation Accuracy\nTest Accuracy\n\n\n\n\n0.500\n0.406\n0.361\n0.347\n\n\n\n\n\n\n\n\n\n\nC\nTraining Accuracy\nValidation Accuracy\nTest Accuracy\n\n\n\n\n0.001\n0.257\n0.257\n0.257\n\n\n0.010\n0.300\n0.300\n0.292\n\n\n0.100\n0.368\n0.340\n0.335\n\n\n0.500\n0.406\n0.361\n0.347\n\n\n1.000\n0.451\n0.353\n0.341\n\n\n10.000\n0.574\n0.324\n0.338\n\n\n100.000\n0.687\n0.334\n0.327\n\n\n\n\nA table of hyperparameter tuning results for the SVM model trained on the text embedding representation, with the best hyperparameters highlighted below.\n\n\n\n\n\n\nA confusion matrix for the SVM model trained on the text embedding representation.\n\n\n\n\n\n\n\n\n\nA table of test classification results for the SVM model trained on the text embedding representation. \n\n\nMetric\nValue\n\n\n\n\nPrecision\n0.342\n\n\nRecall\n0.347\n\n\nF1 Score\n0.319\n\n\n\n\n\n\n\nNaive Bayes\n\n\n\n\n\nAn accuracy plot for the NB model trained on the text embedding representation.\n\n\n\n\n\n\n\n\n\n\n\nHyperparameter Value\nTraining Accuracy\nValidation Accuracy\nTest Accuracy\n\n\n\n\n0.001\n0.359\n0.359\n0.338\n\n\n0.001\n0.359\n0.359\n0.338\n\n\n0.001\n0.359\n0.359\n0.338\n\n\n0.001\n0.359\n0.359\n0.338\n\n\n0.001\n0.359\n0.359\n0.338\n\n\n\n\n\n\n\n\n\n\nalpha\nTraining Accuracy\nValidation Accuracy\nTest Accuracy\n\n\n\n\n0.001\n0.359\n0.359\n0.338\n\n\n0.010\n0.359\n0.359\n0.338\n\n\n0.100\n0.359\n0.359\n0.338\n\n\n1.000\n0.359\n0.359\n0.338\n\n\n10.000\n0.359\n0.359\n0.338\n\n\n\n\nA table of hyperparameter tuning results for the NB model trained on the text embedding representation, with the best hyperparameters highlighted above.\n\n\n\n\n\n\nA confusion matrix for the NB model trained on the text embedding representation.\n\n\n\n\n\n\n\n\n\nA table of test classification results for the NB model trained on the text embedding representation. \n\n\nMetric\nValue\n\n\n\n\nPrecision\n0.335\n\n\nRecall\n0.338\n\n\nF1 Score\n0.291"
  },
  {
    "objectID": "results.html#bert-embeddings-with-pre-trained-classifier",
    "href": "results.html#bert-embeddings-with-pre-trained-classifier",
    "title": "Results",
    "section": "BERT Embeddings with pre-trained classifier",
    "text": "BERT Embeddings with pre-trained classifier\nThe code for this section was adapted from the following source: Google Tensorflow BERT tutorial\nUtilising the BERT embedding in tandem with a pre-trained model, a strategy known as transfer learning, distinctive patterns in performance were observed. Throughout the training epochs, the training accuracy showcased a consistent uptick. However, the validation accuracy plateaued rather swiftly, exhibiting minimal fluctuations thereafter. At the culmination of the training, the accuracy metrics stood as follows: training accuracy at 0.759, validation accuracy at 0.684, and a slightly higher test accuracy of 0.712. Further delving into the test set metrics, the precision was 0.71, recall was 0.707, and the f1 score was close behind at 0.708. An examination of the confusion matrix for the test set underscored these findings. The model predominantly made accurate predictions for the respective presidents, mirroring the positive metrics mentioned earlier. This highlights the efficacy of the BERT embeddings and transfer learning in this particular context, as the results were substantially more favourable than some other methods previously explored.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\naccuracy\nprecision\nrecall\nf1_score\n\n\n\n\nTraining\n0.759\n0.507\n0.837\n0.503\n\n\nValidation\n0.684\n0.548\n0.744\n0.543\n\n\nTest\n0.712\n0.710\n0.707\n0.708"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Assignment 2",
    "section": "",
    "text": "This is the website for Assignment 1 of the course “Data Science for Industry” at the University of Cape Town."
  },
  {
    "objectID": "index.html#links-to-the-repos",
    "href": "index.html#links-to-the-repos",
    "title": "Assignment 2",
    "section": "Links to the repos:",
    "text": "Links to the repos:\n\nWebsite files\nLink to the website files\n\n\nColab notebooks\nColab BERT model training file"
  },
  {
    "objectID": "methods.html",
    "href": "methods.html",
    "title": "Methods",
    "section": "",
    "text": "The Bag-of-Words (BoW) representation is a simplistic yet effective method for text data representation. It hinges on representing text by its constituent words, disregarding their order. Here, each word operates as a feature, with the text being represented by a vector that denotes the frequency of each word (V M and Kumar R 2019).\nFormally, given a vocabulary \\(V\\) comprising \\(N\\) unique words, each document \\(d\\) can be depicted as a vector \\(\\mathbf{v}_d\\) in \\(\\mathbb{R}^N\\) , where the i-th element \\(v_{d,i}\\) denotes the frequency of the i-th word in the document:\n\\[\n\\mathbf{v}_d = [v_{d,1}, v_{d,2}, \\ldots, v_{d,N}]\n\\]\nThe dataset was transformed into a BoW representation with each row corresponding to a sentence, and each column reflecting the frequency of a word in that sentence. The CountVectorizer class from the sklearn.feature_extraction.text module was employed for this task, with English stop words being excluded to filter out prevalent words that lack significant meaning, such as “and”, “the”, and “is” (Pedregosa et al. 2011).\n\n\n\nContrastingly, the TF-IDF representation scales the frequency of words based on their occurrence across all documents, ensuring that words appearing too frequently across documents (potentially bearing lesser discriminative importance) are assigned lower weights “Understanding TF-IDF: A Simple Introduction” (n.d.).\nThe term frequency (TF) of a word in a document is the raw count of that word in the document. The inverse document frequency (IDF) of a word is defined as:\n\\[\n\\text{IDF}(w) = \\log \\left( \\frac{N}{1 + \\text{count}(w)} \\right)\n\\]\nwhere \\(N\\) signifies the total number of documents and \\(\\text{count}(w)\\) represents the number of documents containing the word \\(w\\). The TF-IDF value for a word in a document is then the product of its TF and IDF values “Understanding TF-IDF: A Simple Introduction” (n.d.).\nThe TfidfVectorizer class from the sklearn.feature_extraction.text module was employed to transform our dataset into this representation (Pedregosa et al. 2011).\n\n\n\nFor processing by deep learning models like neural networks, textual data was tokenized and converted into sequences of numbers. The Tokenizer class from the keras.preprocessing.text module was utilized for this purpose. Subsequently, sentences were padded with zeros using pad_sequences from the keras.preprocessing.sequence module to ensure uniform length (Chollet et al. 2015)."
  },
  {
    "objectID": "methods.html#text-representation-techniques",
    "href": "methods.html#text-representation-techniques",
    "title": "Methods",
    "section": "",
    "text": "The Bag-of-Words (BoW) representation is a simplistic yet effective method for text data representation. It hinges on representing text by its constituent words, disregarding their order. Here, each word operates as a feature, with the text being represented by a vector that denotes the frequency of each word (V M and Kumar R 2019).\nFormally, given a vocabulary \\(V\\) comprising \\(N\\) unique words, each document \\(d\\) can be depicted as a vector \\(\\mathbf{v}_d\\) in \\(\\mathbb{R}^N\\) , where the i-th element \\(v_{d,i}\\) denotes the frequency of the i-th word in the document:\n\\[\n\\mathbf{v}_d = [v_{d,1}, v_{d,2}, \\ldots, v_{d,N}]\n\\]\nThe dataset was transformed into a BoW representation with each row corresponding to a sentence, and each column reflecting the frequency of a word in that sentence. The CountVectorizer class from the sklearn.feature_extraction.text module was employed for this task, with English stop words being excluded to filter out prevalent words that lack significant meaning, such as “and”, “the”, and “is” (Pedregosa et al. 2011).\n\n\n\nContrastingly, the TF-IDF representation scales the frequency of words based on their occurrence across all documents, ensuring that words appearing too frequently across documents (potentially bearing lesser discriminative importance) are assigned lower weights “Understanding TF-IDF: A Simple Introduction” (n.d.).\nThe term frequency (TF) of a word in a document is the raw count of that word in the document. The inverse document frequency (IDF) of a word is defined as:\n\\[\n\\text{IDF}(w) = \\log \\left( \\frac{N}{1 + \\text{count}(w)} \\right)\n\\]\nwhere \\(N\\) signifies the total number of documents and \\(\\text{count}(w)\\) represents the number of documents containing the word \\(w\\). The TF-IDF value for a word in a document is then the product of its TF and IDF values “Understanding TF-IDF: A Simple Introduction” (n.d.).\nThe TfidfVectorizer class from the sklearn.feature_extraction.text module was employed to transform our dataset into this representation (Pedregosa et al. 2011).\n\n\n\nFor processing by deep learning models like neural networks, textual data was tokenized and converted into sequences of numbers. The Tokenizer class from the keras.preprocessing.text module was utilized for this purpose. Subsequently, sentences were padded with zeros using pad_sequences from the keras.preprocessing.sequence module to ensure uniform length (Chollet et al. 2015)."
  },
  {
    "objectID": "methods.html#model-architectures-and-training",
    "href": "methods.html#model-architectures-and-training",
    "title": "Methods",
    "section": "2. Model Architectures and Training",
    "text": "2. Model Architectures and Training\n\na. Feed-Forward Neural Network\nFeed-forward neural networks (FFNNs) are a subset of artificial neural networks characterized by acyclic connections between nodes. They encompass multiple layers: an input layer, several hidden layers, and an output layer (Rumelhart, Hinton, and Williams 1986).\nThe architecture of the neural network employed in this study is delineated as follows:\n\nInput Layer: This layer harbors neurons equal to the number of features in the dataset (word counts for BoW and TF-IDF, sequence length for text embeddings). The Rectified Linear Unit (ReLU) activation function was utilized owing to its efficiency and capability to mitigate the vanishing gradient issue:\n\n\\[\nf(x) = \\max(0, x)\n\\]\n\nHidden Layers: Several hidden layers were introduced, each utilizing He initialization, which is proficient for layers with ReLU activation. A dropout layer succeeded each hidden layer to curb overfitting by randomly nullifying a fraction of input units during each training update.\nOutput Layer: This layer contains neurons equal to the number of classes (presidents, in our scenario). The softmax function was employed as the activation function, generating a probability distribution over the classes:\n\n\\[\n\\sigma(\\mathbf{z})_i = \\frac{e^{z_i}}{\\sum_{j=1}^{K} e^{z_j}}\n\\]\nfor \\(i = 1, \\ldots, K\\) and \\(\\mathbf{z}\\) is the input vector to the softmax function.\nTraining was conducted using the Adam optimization algorithm with a learning rate of 0.001. Adam is adept at training deep neural networks via computing adaptive learning rates for each parameter, leveraging moving averages of the parameter gradients and squared gradients.\nThe EarlyStopping and ReduceLROnPlateau callbacks were also enlisted. The former halts the training process if validation loss ceases to improve for a stipulated number of epochs, while the latter diminishes the learning rate if the validation loss reaches a plateau (Chollet et al. 2015).\n\n\nb. Support Vector Machine (SVM)\nThe Support Vector Machine (SVM) is a supervised learning algorithm suitable for both classification and regression tasks. It operates by identifying the optimal hyperplane that segregates a dataset into distinct classes. Provided a set of training examples, each labeled as belonging to one of two categories, the SVM training algorithm constructs a model that categorizes new examples into one of the two categories (Cortes and Vapnik 1995).\nMathematically, given labeled training data \\((x_1, y_1), \\ldots, (x_N, y_N)\\) where \\(x_i\\) belongs to \\(\\mathbb{R}^D\\) and \\(y_i\\) is either 1 or -1 (indicating the class the input \\(x_i\\) belongs to), SVM seeks the hyperplane defined by \\(w\\) and \\(b\\) that optimally separates the data points of the two classes (Cortes and Vapnik 1995):\n\\[\ny_i(w \\cdot x_i + b) \\geq 1\n\\]\nThe objective of SVM is to maximize the margin, which is the distance between the hyperplane and the nearest point from either class. The decision function is then given by:\n\\[\nf(x) = \\text{sign}(w \\cdot x + b)\n\\]\n\n\nc. Naive Bayes Classifier\nNaive Bayes is a probabilistic classifier predicated on Bayes’ theorem with strong (naive) independence assumptions among features (Raschka 2014). Given a set of features \\(X = x_1, \\ldots, x_n\\) and a class variable \\(C\\), Bayes’ theorem states:\n\\[\nP(C|X) = \\frac{P(X|C) \\times P(C)}{P(X)}\n\\]\nThe Naive Bayes classifier posits that the effect of a particular feature in a class is independent of other features. This simplification expedites computation, hence the term ‘naive’ (Raschka 2014).\nIn our problem, the Naive Bayes classifier estimates the probability of a sentence belonging to each president’s class based on the features (word frequencies for BoW or TF-IDF values). The sentence is then classified to the class (president) with the highest probability."
  },
  {
    "objectID": "methods.html#model-evaluation",
    "href": "methods.html#model-evaluation",
    "title": "Methods",
    "section": "3. Model Evaluation",
    "text": "3. Model Evaluation\nEvaluating the performance of machine learning models is paramount as it unveils the efficacy of the model and areas of potential improvement. Our evaluation paradigm leverages standard metrics including accuracy, precision, recall, and F1 score to quantify various facets of the model’s predictions in a multi-class classification setting such as ours, where predictions could be true or false for multiple classes (presidents, in this case).\n\na. Accuracy\nAccuracy furnishes a broad overview of the model’s performance and is calculated as the ratio of correct predictions to the total predictions:\n\\[\n\\text{Accuracy} = \\frac{\\text{Number of Correct Predictions}}{\\text{Total Number of Predictions}}\n\\]\nNonetheless, in imbalanced datasets, accuracy could be misleading.\n\n\nb. Precision\nPrecision scrutinizes the model’s positive predictions. Specifically, it computes the frequency at which the model correctly predicted a specific president out of all predictions for that president:\n\\[\n\\text{Precision (for a given president)} = \\frac{\\text{True Positives}}{\\text{True Positives} + \\text{False Positives}}\n\\]\nWhere:\n\nTrue Positives (TP): The number of sentences correctly identified as belonging to that president.\nFalse Positives (FP): The number of sentences erroneously identified as belonging to that president, while they belong to a different one.\n\nPrecision is particularly crucial in scenarios where the cost of a false positive is high.\n\n\nc. Recall (or Sensitivity)\nRecall evaluates how effectively the model identifies sentences from a specific president. It calculates the proportion of actual sentences from a president that the model correctly identified:\n\\[\n\\text{Recall (for a given president)} = \\frac{\\text{True Positives}}{\\text{True Positives} + \\text{False Negatives}}\n\\]\nWhere:\n\nFalse Negatives (FN): The number of sentences that genuinely belong to a president but were misclassified as belonging to another.\n\nRecall is vital in contexts where missing a true instance is significant.\n\n\nd. F1 Score\nThe F1 score is the harmonic mean of precision and recall, providing a balance between them. It achieves its best value at 1 (perfect precision and recall) and its worst at 0:\n\\[\n\\text{F1 Score} = 2 \\times \\frac{\\text{Precision} \\times \\text{Recall}}{\\text{Precision} + \\text{Recall}}\n\\]\nThe F1 score is particularly useful when there is an uneven data distribution among classes.\nThese metrics were computed for each president in our dataset and then averaged (weighted by the number of true instances for each president) to derive a single value representing the overall model’s performance. This approach ensures that the model’s aptitude to predict less frequent classes (presidents with fewer sentences) is considered, rendering the evaluation more robust and representative of the model’s true capabilities in a multi-class setting.\nMoreover, the models were also assessed on separate training and test datasets. The training dataset is the learning corpus for the model, while the test dataset presents a fresh, unseen set of data points to gauge the model’s generalization to new data. This separation is pivotal to ensure that the model doesn’t merely memorize the training data (overfitting), but discerns the underlying patterns determining which president uttered a given sentence."
  },
  {
    "objectID": "ds4i_assignment2.html",
    "href": "ds4i_assignment2.html",
    "title": "Happy Legendary Title",
    "section": "",
    "text": "Hello there\nThis is a happy document. It has a title, and some content. It also has a list:"
  },
  {
    "objectID": "ds4i_assignment2.html#topic-modelling",
    "href": "ds4i_assignment2.html#topic-modelling",
    "title": "Assignment 2",
    "section": "Topic modelling",
    "text": "Topic modelling\n\nLatent Semantic Analysis (LSA)\n\n\n\nLSA [@Deerwester1990] is a non-probabilistic, non-generative model where a form of matrix factorization is utilized to uncover few latent topics, capturing meaningful relationships among documents/tokens. As depicted in Figure, in the first step, a document-term matrix DTM is generated from the raw text data by tokenizing d documents into w words (or sentences), forming the columns and rows respectively. Each row-column entry is either valued via the BoW or tf-idf approach. This DTM-matrix, which is often sparse and high-dimensional, is then decomposed via a dimensionality-reduction-technique, namely truncated Singular Value Decomposition (SVD). Consequently, in the second step the DTM-matrix becomes the product of three matrices: the topic-word matrix At* (for the tokens), the topic-prevalence matrix Bt* (for the latent semantic factors), and the transposed document-topic matrix CTt* (for the document). Here, t*, the optimal number of topics, is a hyperparameter which is refined at a value (either via the Silhouette-Coefficient or the coherence-measure approach) that retains the most significant dimensions in the transformed space. In the final step, the text data is then encoded using this top-topic number. \n\nGiven LSA only implicates a DTM-matrix, the implementation thereof is generally efficient. Though, with the involvement of truncated SVD, some computational intensity and a lack of quick updates with new, incoming text-data can arise. Additional LSA drawbacks include: the lack of interpretability, the underlying linear-model framework (which results in poor performance on text-data with non-linear dependencies), and the underlying Gaussian assumption for tokens in documents (which may not be an appropriate distribution). \n\n### Probabilistic Latent Semantic Analysis (pLSA)\n\n\n\n\n```{=html}\n\n\n\nInstead of implementing truncated SVD, pLSA (Hofmann 1999) rather utilizes a generative, probabilistic model. Within this framework, a document d is first selected with probability P(d). Then given this, a latent topic t is present in this selected document d and so chosen with probability of P(t|d). Finally, given this chosen topic t, a word w (or sentence) is generated from it with probability P(w|t), as shown in Figure. It is noted that the values of P(d) is determined directly from the corpus D which is defined in terms of a DTM matrix. In contrast, the probabilities P(t|d) and P(w|t) are parameters modelled as multinomial distributions and iteratively updated via the Expectation-Maximization (EM) algorithm. Direct parallelism between LSA and pLSA can be drawn via the methods’ parameterization, as conveyed via matching colours of the topic-word matrix and P(w|t), the document-topic matrix and P(d|t) as well as the topic-prevalence matrix and P(t) displayed in Figure and Figure, respectively.\nDespite pLSA implicitly addressing LSA-related disadvantages, this method still involves two main drawbacks. There is no probability model for the document-topic probabilities P(t|d), resulting in the inability to assign topic mixtures to new, unseen documents not trained on. Model parameters also then increase linearly with the number of documents added, making this method more susceptible to overfitting.\n\n\nLatent Dirichlet Allocation\n\nLDA is another generative, probabilistic model which can be deemed as a hierarchical Bayesian version of pLSA. Via explicitly defining a generative model for the document-topic probabilities, both the above-mentioned pitfalls of pLSA are improved upon. The number of parameters to estimate drastically decrease and the ability to apply and generalize to new, unseen documents is attainable. As presented in Figure, the initial steps first involve randomly sampling a document-topic probability distribution (\\(\\theta\\)) from a Dirichlet (Dir) distribution (\\(\\eta\\)), followed by randomly sampling a topic-word probability distribution (\\(\\phi\\)) from another Dirichlet distribution (\\(\\tau\\)). From the \\(\\theta\\) distribution, a topic t is selected by drawing from a multinomial (Mult) distribution (third step) and from the \\(\\phi\\) distribution given said topic t, a word w (or sentences) is sampled from another multinomial distribution (fourth step). The associated LDA-parameters are then estimated via a variational expectation maximization algorithm or collapsed Gibbs sampling.\n\n\nCorrelated Topic Model (CTM)\nFollowing closely to LDA, the CTM (Lafferty and Blei 2005) additionally allows for the ability to model the presence of any correlated topics. Such topic correlations are introduced via the inclusion of the multivariate normal (MultNorm) distribution with t length-vector of means (\\(\\mu\\)) and t \\(\\times\\) t covariance matrix (\\(\\Sigma\\)) where the resulting values are then mapped into probabilities by passing through a logistic (log) transformation. Comparing Figure and Figure, the nuance between LDA and CTM is highlighted in light-blue, where the discrepancy in the models come about from replacing the Dirichlet distribution (which involves the implicit assumption of independence across topics) with the logit-normal distribution (which now explicitly enables for topic dependency via a covariance structure) for generating document-topic probabilities. The other generative processes previously outlined for LDA is retained and repeated for CTM. Given this additional model complexity, the more convoluted mean-field variational inference algorithm is employed for CTM-parameter estimation which necessitate many iterations for optimization purposes. CTM is consequently computationally more expensive than LDA. Though, this snag is far outweighed by the procurement of richer topics with overt relationships acknowledged between these."
  },
  {
    "objectID": "ds4i_assignment2.html#lsa",
    "href": "ds4i_assignment2.html#lsa",
    "title": "Assignment 2",
    "section": "LSA",
    "text": "LSA\n\n\n(0, '0.267*\"year\" + 0.242*\"government\" + 0.198*\"work\" + 0.195*\"south\" + 0.188*\"people\" + 0.163*\"country\" + 0.145*\"development\" + 0.142*\"national\" + 0.140*\"programme\" + 0.134*\"african\"')\n(1, '-0.169*\"government\" + 0.146*\"south\" + -0.142*\"regard\" + 0.135*\"year\" + -0.134*\"people\" + 0.115*\"energy\" + 0.114*\"000\" + -0.113*\"shall\" + -0.112*\"ensure\" + -0.102*\"question\"')\n(2, '-0.140*\"honourable\" + -0.131*\"programme\" + 0.125*\"pandemic\" + -0.123*\"continue\" + 0.115*\"new\" + -0.110*\"development\" + -0.109*\"rand\" + 0.107*\"great\" + -0.106*\"compatriot\" + 0.102*\"investment\"')\n(3, '-0.337*\"alliance\" + -0.240*\"transitional\" + -0.204*\"party\" + -0.204*\"constitution\" + -0.156*\"zulu\" + -0.155*\"constitutional\" + -0.131*\"south\" + -0.126*\"concern\" + -0.125*\"election\" + -0.122*\"freedom\"')\n(4, '0.219*\"shall\" + -0.204*\"people\" + 0.148*\"year\" + -0.144*\"alliance\" + 0.130*\"start\" + -0.101*\"government\" + -0.097*\"address\" + -0.093*\"transitional\" + 0.088*\"community\" + 0.088*\"citizen\"')"
  },
  {
    "objectID": "ds4i_assignment2.html#plsa-probabilistic-latent-semantic-analysis",
    "href": "ds4i_assignment2.html#plsa-probabilistic-latent-semantic-analysis",
    "title": "Assignment 2",
    "section": "pLSA (Probabilistic Latent Semantic Analysis)",
    "text": "pLSA (Probabilistic Latent Semantic Analysis)\n\n\n[(0,\n  '0.001*\"year\" + 0.000*\"country\" + 0.000*\"south\" + 0.000*\"work\" + 0.000*\"programme\" + 0.000*\"government\" + 0.000*\"national\" + 0.000*\"development\" + 0.000*\"people\" + 0.000*\"continue\"'),\n (1,\n  '0.001*\"year\" + 0.000*\"government\" + 0.000*\"south\" + 0.000*\"african\" + 0.000*\"people\" + 0.000*\"work\" + 0.000*\"country\" + 0.000*\"africa\" + 0.000*\"development\" + 0.000*\"programme\"'),\n (2,\n  '0.001*\"government\" + 0.001*\"south\" + 0.001*\"year\" + 0.001*\"work\" + 0.001*\"people\" + 0.001*\"country\" + 0.001*\"development\" + 0.001*\"national\" + 0.000*\"programme\" + 0.000*\"public\"'),\n (3,\n  '0.001*\"government\" + 0.001*\"work\" + 0.001*\"people\" + 0.000*\"year\" + 0.000*\"south\" + 0.000*\"ensure\" + 0.000*\"country\" + 0.000*\"programme\" + 0.000*\"national\" + 0.000*\"development\"'),\n (4,\n  '0.001*\"year\" + 0.001*\"government\" + 0.001*\"people\" + 0.001*\"work\" + 0.001*\"south\" + 0.000*\"african\" + 0.000*\"country\" + 0.000*\"national\" + 0.000*\"development\" + 0.000*\"programme\"')]"
  },
  {
    "objectID": "ds4i_assignment2.html#lda-latent-dirichlet-allocation",
    "href": "ds4i_assignment2.html#lda-latent-dirichlet-allocation",
    "title": "Assignment 2",
    "section": "LDA (Latent Dirichlet Allocation)",
    "text": "LDA (Latent Dirichlet Allocation)\n\n\n[(0,\n  '0.001*\"year\" + 0.001*\"south\" + 0.001*\"government\" + 0.001*\"work\" + 0.000*\"african\" + 0.000*\"country\" + 0.000*\"programme\" + 0.000*\"people\" + 0.000*\"national\" + 0.000*\"new\"'),\n (1,\n  '0.000*\"year\" + 0.000*\"work\" + 0.000*\"development\" + 0.000*\"government\" + 0.000*\"country\" + 0.000*\"national\" + 0.000*\"south\" + 0.000*\"programme\" + 0.000*\"continue\" + 0.000*\"energy\"'),\n (2,\n  '0.000*\"government\" + 0.000*\"year\" + 0.000*\"work\" + 0.000*\"people\" + 0.000*\"south\" + 0.000*\"make\" + 0.000*\"development\" + 0.000*\"new\" + 0.000*\"country\" + 0.000*\"african\"'),\n (3,\n  '0.001*\"year\" + 0.001*\"people\" + 0.001*\"government\" + 0.001*\"south\" + 0.000*\"work\" + 0.000*\"national\" + 0.000*\"african\" + 0.000*\"country\" + 0.000*\"public\" + 0.000*\"development\"'),\n (4,\n  '0.001*\"government\" + 0.001*\"year\" + 0.001*\"people\" + 0.001*\"south\" + 0.001*\"work\" + 0.001*\"country\" + 0.001*\"development\" + 0.001*\"national\" + 0.001*\"programme\" + 0.001*\"ensure\"')]"
  },
  {
    "objectID": "ds4i_assignment2.html#ctm-correlated-topic-model",
    "href": "ds4i_assignment2.html#ctm-correlated-topic-model",
    "title": "Assignment 2",
    "section": "CTM (Correlated Topic Model)",
    "text": "CTM (Correlated Topic Model)\n\n\nIteration: 0    Log-likelihood: -6.799771183041395\nIteration: 1    Log-likelihood: -6.513216470635273\nIteration: 2    Log-likelihood: -6.355882543279467\nIteration: 3    Log-likelihood: -6.2607084358537355\nIteration: 4    Log-likelihood: -6.197715604875132\nIteration: 5    Log-likelihood: -6.1645253555687605\nIteration: 6    Log-likelihood: -6.1122808732623835\nIteration: 7    Log-likelihood: -6.085954824218551\nIteration: 8    Log-likelihood: -6.059680097514997\nIteration: 9    Log-likelihood: -6.042303789103667\nIteration: 10   Log-likelihood: -6.034703158302843\nIteration: 11   Log-likelihood: -6.026411959046902\nIteration: 12   Log-likelihood: -6.012074849000191\nIteration: 13   Log-likelihood: -6.006581333479062\nIteration: 14   Log-likelihood: -5.981535018899553\nIteration: 15   Log-likelihood: -5.97984242965751\nIteration: 16   Log-likelihood: -5.970574501383571\nIteration: 17   Log-likelihood: -5.960764167081597\nIteration: 18   Log-likelihood: -5.963370076835946\nIteration: 19   Log-likelihood: -5.955796617616424\nIteration: 20   Log-likelihood: -5.943864613207089\nIteration: 21   Log-likelihood: -5.924873732106614\nIteration: 22   Log-likelihood: -5.925764058521468\nIteration: 23   Log-likelihood: -5.917318439500449\nIteration: 24   Log-likelihood: -5.923300406003926\nIteration: 25   Log-likelihood: -5.936856154077835\nIteration: 26   Log-likelihood: -5.927367016062913\nIteration: 27   Log-likelihood: -5.932287106411244\nIteration: 28   Log-likelihood: -5.925052711701778\nIteration: 29   Log-likelihood: -5.922297659435844\nIteration: 30   Log-likelihood: -5.9148964825677375\nIteration: 31   Log-likelihood: -5.920110826752398\nIteration: 32   Log-likelihood: -5.925217357349694\nIteration: 33   Log-likelihood: -5.926121272255882\nIteration: 34   Log-likelihood: -5.932963827134607\nIteration: 35   Log-likelihood: -5.936853457568642\nIteration: 36   Log-likelihood: -5.935402018157176\nIteration: 37   Log-likelihood: -5.921002639547479\nIteration: 38   Log-likelihood: -5.9315975701046835\nIteration: 39   Log-likelihood: -5.931833293586385\nIteration: 40   Log-likelihood: -5.915176835980096\nIteration: 41   Log-likelihood: -5.925503612210657\nIteration: 42   Log-likelihood: -5.920214925846161\nIteration: 43   Log-likelihood: -5.915519618559458\nIteration: 44   Log-likelihood: -5.91078095566771\nIteration: 45   Log-likelihood: -5.915563020301967\nIteration: 46   Log-likelihood: -5.90297347913047\nIteration: 47   Log-likelihood: -5.9146779799117875\nIteration: 48   Log-likelihood: -5.9052572820671605\nIteration: 49   Log-likelihood: -5.896244958880168\nIteration: 50   Log-likelihood: -5.897956814473697\nIteration: 51   Log-likelihood: -5.891216379529858\nIteration: 52   Log-likelihood: -5.882996521804093\nIteration: 53   Log-likelihood: -5.885107223648392\nIteration: 54   Log-likelihood: -5.8861502927951665\nIteration: 55   Log-likelihood: -5.884340942568278\nIteration: 56   Log-likelihood: -5.886141222155755\nIteration: 57   Log-likelihood: -5.879304754542334\nIteration: 58   Log-likelihood: -5.882409576832495\nIteration: 59   Log-likelihood: -5.879441705527177\nIteration: 60   Log-likelihood: -5.887988123391837\nIteration: 61   Log-likelihood: -5.882250356617471\nIteration: 62   Log-likelihood: -5.88277408570063\nIteration: 63   Log-likelihood: -5.880745324709502\nIteration: 64   Log-likelihood: -5.879870967814149\nIteration: 65   Log-likelihood: -5.874075120717159\nIteration: 66   Log-likelihood: -5.8840868342399135\nIteration: 67   Log-likelihood: -5.882302154805391\nIteration: 68   Log-likelihood: -5.87726827885877\nIteration: 69   Log-likelihood: -5.8772000665354165\nIteration: 70   Log-likelihood: -5.877679227560991\nIteration: 71   Log-likelihood: -5.8778453467191305\nIteration: 72   Log-likelihood: -5.8853993995654905\nIteration: 73   Log-likelihood: -5.879228626938545\nIteration: 74   Log-likelihood: -5.890349691145356\nIteration: 75   Log-likelihood: -5.888726733271527\nIteration: 76   Log-likelihood: -5.892956967823788\nIteration: 77   Log-likelihood: -5.889057054373432\nIteration: 78   Log-likelihood: -5.887914756529604\nIteration: 79   Log-likelihood: -5.894897742022889\nIteration: 80   Log-likelihood: -5.897905378786642\nIteration: 81   Log-likelihood: -5.90678878872963\nIteration: 82   Log-likelihood: -5.908479201250067\nIteration: 83   Log-likelihood: -5.900765073215898\nIteration: 84   Log-likelihood: -5.911757269362371\nIteration: 85   Log-likelihood: -5.906457945621606\nIteration: 86   Log-likelihood: -5.909260595351656\nIteration: 87   Log-likelihood: -5.910211183825542\nIteration: 88   Log-likelihood: -5.909865931081799\nIteration: 89   Log-likelihood: -5.9124118383851885\nIteration: 90   Log-likelihood: -5.90486904203658\nIteration: 91   Log-likelihood: -5.898546547332345\nIteration: 92   Log-likelihood: -5.908060579631446\nIteration: 93   Log-likelihood: -5.904451751982099\nIteration: 94   Log-likelihood: -5.902535769434896\nIteration: 95   Log-likelihood: -5.897933960910536\nIteration: 96   Log-likelihood: -5.908713180182573\nIteration: 97   Log-likelihood: -5.906865499118049\nIteration: 98   Log-likelihood: -5.899870508465152\nIteration: 99   Log-likelihood: -5.897041106380576\nTopic #0: [('work', 0.08041610568761826), ('economic', 0.03734271973371506), ('address', 0.026227004826068878), ('provide', 0.02475070022046566), ('make', 0.02440333366394043), ('create', 0.023013869300484657), ('opportunity', 0.022840186953544617), ('increase', 0.02119019627571106), ('implement', 0.016848120838403702), ('build', 0.016066547483205795)]\nTopic #1: [('ensure', 0.04742421582341194), ('include', 0.034458935260772705), ('business', 0.030891306698322296), ('nation', 0.030369214713573456), ('progress', 0.019492298364639282), ('land', 0.018535131588578224), ('000', 0.017490947619080544), ('focus', 0.016446763649582863), ('policy', 0.015576610341668129), ('national', 0.014967503026127815)]\nTopic #2: [('south', 0.06261109560728073), ('african', 0.05913274735212326), ('continue', 0.04129508510231972), ('social', 0.03326813504099846), ('measure', 0.016857484355568886), ('plan', 0.0160547886043787), ('start', 0.015965601429343224), ('effort', 0.013468327932059765), ('especially', 0.011506184935569763), ('small', 0.01123862061649561)]\nTopic #3: [('development', 0.0596558041870594), ('u', 0.047933220863342285), ('society', 0.0271799024194479), ('million', 0.024748550727963448), ('infrastructure', 0.02466171607375145), ('south', 0.018496504053473473), ('water', 0.01771499775350094), ('education', 0.01762816309928894), ('resource', 0.017020326107740402), ('matter', 0.01510997861623764)]\nTopic #4: [('africa', 0.04934827238321304), ('service', 0.04361424595117569), ('regard', 0.02693343162536621), ('challenge', 0.02276322804391384), ('investment', 0.020330609753727913), ('implementation', 0.01711607724428177), ('parliament', 0.015117854811251163), ('like', 0.01477033831179142), ('critical', 0.013988425023853779), ('speaker', 0.0139015456661582)]\nTopic #5: [('government', 0.08499310165643692), ('country', 0.06861845403909683), ('state', 0.03162387013435364), ('far', 0.026512207463383675), ('time', 0.026165654882788658), ('crime', 0.02252684347331524), ('security', 0.02079407311975956), ('high', 0.01810828410089016), ('say', 0.01724190078675747), ('capacity', 0.016722070053219795)]\nTopic #6: [('programme', 0.05151817947626114), ('sector', 0.02975277788937092), ('support', 0.02738315798342228), ('life', 0.026066701859235764), ('make', 0.02439919114112854), ('project', 0.021941805258393288), ('place', 0.01843125745654106), ('important', 0.01781691052019596), ('government', 0.01658821851015091), ('change', 0.015271763317286968)]\nTopic #7: [('people', 0.08195344358682632), ('national', 0.04559854790568352), ('improve', 0.03688393533229828), ('economy', 0.03582761809229851), ('need', 0.03547551482915878), ('great', 0.017694182693958282), ('energy', 0.01663786545395851), ('poverty', 0.015845628455281258), ('area', 0.013556942343711853), ('number', 0.013292863965034485)]\nTopic #8: [('new', 0.04433754086494446), ('public', 0.043900296092033386), ('growth', 0.030870389193296432), ('community', 0.028334366157650948), ('job', 0.026323039084672928), ('process', 0.025623446330428123), ('honourable', 0.021163543686270714), ('child', 0.018102826550602913), ('president', 0.017753031104803085), ('billion', 0.017140887677669525)]\nTopic #9: [('year', 0.11361329257488251), ('local', 0.01975954882800579), ('action', 0.01681307889521122), ('achieve', 0.0167264174669981), ('right', 0.015513165853917599), ('member', 0.01525318343192339), ('act', 0.014299913309514523), ('department', 0.013953269459307194), ('democratic', 0.01360662654042244), ('police', 0.013173321262001991)]"
  },
  {
    "objectID": "ds4i_assignment2.html#atm-author-topic-model",
    "href": "ds4i_assignment2.html#atm-author-topic-model",
    "title": "Assignment 2",
    "section": "ATM (Author-Topic Model)",
    "text": "ATM (Author-Topic Model)\n\n\n[(0,\n  '0.009*\"government\" + 0.009*\"year\" + 0.007*\"work\" + 0.007*\"people\" + 0.007*\"south\" + 0.007*\"country\" + 0.006*\"development\" + 0.005*\"african\" + 0.005*\"africa\" + 0.005*\"ensure\"'),\n (1,\n  '0.010*\"government\" + 0.009*\"people\" + 0.009*\"year\" + 0.007*\"work\" + 0.007*\"south\" + 0.007*\"country\" + 0.007*\"africa\" + 0.007*\"african\" + 0.006*\"national\" + 0.005*\"make\"'),\n (2,\n  '0.011*\"year\" + 0.010*\"government\" + 0.009*\"work\" + 0.008*\"south\" + 0.007*\"people\" + 0.007*\"country\" + 0.005*\"new\" + 0.005*\"national\" + 0.005*\"african\" + 0.005*\"sector\"'),\n (3,\n  '0.008*\"year\" + 0.007*\"government\" + 0.007*\"work\" + 0.007*\"south\" + 0.006*\"people\" + 0.006*\"country\" + 0.005*\"development\" + 0.004*\"ensure\" + 0.004*\"national\" + 0.004*\"african\"'),\n (4,\n  '0.014*\"year\" + 0.010*\"government\" + 0.009*\"south\" + 0.007*\"development\" + 0.007*\"people\" + 0.007*\"programme\" + 0.007*\"work\" + 0.007*\"national\" + 0.006*\"country\" + 0.006*\"african\"')]"
  }
]